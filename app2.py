# app.py
import os
import ast
import random
import numpy as np
import pandas as pd
from pathlib import Path
import platform
from sklearn.metrics import mean_squared_error
import streamlit as st
import plotly.express as px
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.base import clone
import re

#XGBê°€ ì„¤ì¹˜ë¼ ìˆìœ¼ë©´ ì“°ë„ë¡ ì•ˆì „í•˜ê²Œ ì¶”ê°€
try:
    from xgboost import XGBRegressor
    XGB_AVAILABLE = True
except Exception:
    XGB_AVAILABLE = False

def rmse(y_true, y_pred):
    # squared=False ë¯¸ì§€ì› í™˜ê²½ì—ì„œë„ ë™ì‘
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))
    
# ===== í˜ì´ì§€ ì„¤ì • =====
st.set_page_config(page_title="K-ë“œë¼ë§ˆ ë¶„ì„/ì˜ˆì¸¡", page_icon="ğŸ¬", layout="wide")

# ===== ì „ì—­ ì‹œë“œ ê³ ì • =====
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED)

# ===== í•œê¸€ í°íŠ¸ ë¶€íŠ¸ìŠ¤íŠ¸ë© =====
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

if st.session_state.get("font_cache_cleared") is not True:
    import shutil
    shutil.rmtree(matplotlib.get_cachedir(), ignore_errors=True)
    st.session_state["font_cache_cleared"] = True

def ensure_korean_font():
    matplotlib.rcParams['axes.unicode_minus'] = False
    base = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()
    candidates = [
        base / "fonts" / "NanumGothic-Regular.ttf",
        base / "fonts" / "NanumGothic.ttf",
    ]
    if platform.system() == "Windows":
        candidates += [Path(r"C:\Windows\Fonts\malgun.ttf"), Path(r"C:\Windows\Fonts\malgunbd.ttf")]
    wanted = ("nanum","malgun","applegothic","notosanscjk","sourcehan","gulim","dotum","batang","pretendard","gowun","spoqa")
    for f in fm.findSystemFonts(fontext="ttf"):
        if any(k in os.path.basename(f).lower() for k in wanted):
            candidates.append(Path(f))
    for p in candidates:
        try:
            if p.exists():
                fm.fontManager.addfont(str(p))
                family = fm.FontProperties(fname=str(p)).get_name()
                matplotlib.rcParams['font.family'] = family
                st.session_state["kfont_path"] = str(p)  # WordCloudìš©
                return family
        except Exception:
            continue
    st.session_state["kfont_path"] = None
    return None

_ = ensure_korean_font()

def age_to_age_group(age: int) -> str:
    # ë°ì´í„°ì…‹ì— ìˆëŠ” ì—°ë ¹ëŒ€ ë¼ë²¨ë“¤
    s = raw_df.get('ì—°ë ¹ëŒ€')
    if s is None or s.dropna().empty:
        # í´ë°±: ê¸°ë³¸ êµ¬ê°„
        if age < 20: return "10ëŒ€"
        if age < 30: return "20ëŒ€"
        if age < 40: return "30ëŒ€"
        if age < 50: return "40ëŒ€"
        return "50ëŒ€ ì´ìƒ"

    series = s.dropna().astype(str)
    vocab = series.unique().tolist()
    counts = series.value_counts()

    decade = (int(age)//10)*10  # 27â†’20, 41â†’40 ...

    # 1) '20ëŒ€'ì²˜ëŸ¼ ì •í™•í•œ íŒ¨í„´ ìš°ì„ 
    exact = [g for g in vocab if re.search(rf"{decade}\s*ëŒ€", g)]
    if exact:
        return counts[exact].idxmax()  # ê°€ì¥ í”í•œ ë¼ë²¨

    # 2) ìˆ«ìë§Œ í¬í•¨ë¼ë„ í—ˆìš© (ì˜ˆ: '20ëŒ€ í›„ë°˜')
    loose = [g for g in vocab if str(decade) in g]
    if loose:
        return counts[loose].idxmax()

    # 3) 50ëŒ€ ì´ìƒ í´ë°±
    if decade >= 50:
        over = [g for g in vocab if ('50' in g) or ('ì´ìƒ' in g)]
        if over:
            return counts[over].idxmax()

    # 4) ê°€ì¥ ê°€ê¹Œìš´ ì‹­ëŒ€ ë¼ë²¨ë¡œ ë§¤ì¹­
    with_num = []
    for g in vocab:
        m = re.search(r'(\d+)', g)
        if m:
            with_num.append((g, int(m.group(1))))
    if with_num:
        nearest_num = min(with_num, key=lambda t: abs(t[1]-decade))[1]
        candidates = [g for g,n in with_num if n==nearest_num]
        return counts[candidates].idxmax()

    # 5) ìµœë¹ˆê°’
    return counts.idxmax()

# ===== ì „ì²˜ë¦¬ ìœ í‹¸ =====
from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

def clean_cell_colab(x):
    if isinstance(x, list):
        return [str(i).strip() for i in x if pd.notna(i)]
    if pd.isna(x):
        return []
    if isinstance(x, str):
        try:
            parsed = ast.literal_eval(x)
            if isinstance(parsed, list):
                return [str(i).strip() for i in parsed if pd.notna(i)]
            return [x.strip()]
        except Exception:
            return [x.strip()]
    return [str(x).strip()]

def colab_multilabel_fit_transform(df: pd.DataFrame, cols=('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)
        mlb = MultiLabelBinarizer()
        arr = mlb.fit_transform(out[col])
        new_cols = [f"{col}_{c.strip().upper()}" for c in mlb.classes_]
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)
         # ì„¸ì…˜ì— 'í•™ìŠµëœ' mlbì™€ í´ë˜ìŠ¤ ë‘˜ ë‹¤ ì €ì¥
        st.session_state[f"mlb_{col}"] = mlb
        st.session_state[f"mlb_classes_{col}"] = mlb.classes_.tolist()
    return out

def colab_multilabel_transform(df: pd.DataFrame, cols=('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)

        # 1) ì„¸ì…˜ì— 'í•™ìŠµëœ' mlbê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        mlb = st.session_state.get(f"mlb_{col}", None)

        # 2) ì—†ìœ¼ë©´ classesë¡œë¶€í„° ë³µêµ¬ (classes_ ì§ì ‘ ì„¸íŒ…)
        if mlb is None:
            classes = st.session_state.get(f"mlb_classes_{col}", [])
            mlb = MultiLabelBinarizer()
            if classes:
                mlb.classes_ = np.array(classes)  # â† transformì´ ë°”ë¡œ ê°€ëŠ¥
            else:
                # 3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ df_mlb ì»¬ëŸ¼ì—ì„œ ìœ ì¶” (prefix ì œê±°)
                try:
                    prefix = f"{col}_"
                    labels = [c[len(prefix):] for c in df_mlb.columns if c.startswith(prefix)]
                    if labels:
                        mlb.classes_ = np.array(labels)
                    else:
                        # ë§ˆì§€ë§‰ í´ë°±: í˜„ì¬ ì…ë ¥ìœ¼ë¡œ fit (ì´ ê²½ìš° í›ˆë ¨ ìŠ¤í‚¤ë§ˆì™€ ì–´ê¸‹ë‚  ìˆ˜ ìˆìŒ)
                        mlb.fit(out[col])
                except Exception:
                    mlb.fit(out[col])

        arr = mlb.transform(out[col])  # ì´ì œ NotFittedError ì•ˆ ë‚¨
        new_cols = [f"{col}_{c}" for c in mlb.classes_]  # classes_ ì´ë¯¸ UPPER ì²˜ë¦¬ë¨
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)

    return out

# (ì„ íƒ) ì‚¬ìš©ì ì„ íƒ ê¸°ë°˜ X ìƒì„± ìœ í‹¸ â€” EDA/ì˜ˆì¸¡ íƒ­ì—ì„œ ì‚¬ìš©
def expand_feature_cols_for_training(base: pd.DataFrame, selected: list):
    cols = []
    for c in selected:
        if c in ('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼'):
            prefix = f"{c}_"
            cols += [k for k in base.columns if k.startswith(prefix)]
        else:
            cols.append(c)
    return cols

def build_X_from_selected(base: pd.DataFrame, selected: list) -> pd.DataFrame:
    X = pd.DataFrame(index=base.index)
    use_cols = expand_feature_cols_for_training(base, selected)
    if use_cols:
        X = pd.concat([X, base[use_cols]], axis=1)
    singles = [c for c in selected if c not in ('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼')]
    for c in singles:
        if c in base.columns and base[c].dtype == 'object':
            X = pd.concat([X, pd.get_dummies(base[c], prefix=c)], axis=1)
        elif c in base.columns:
            X[c] = base[c]
    return X

# ===== ë°ì´í„° ë¡œë“œ =====
@st.cache_data
def load_data():
    raw = pd.read_json('drama_data.json')
    return pd.DataFrame({c: pd.Series(v) for c,v in raw.items()})

raw_df = load_data()

# ===== Colabê³¼ ë™ì¼ ë©€í‹°ë¼ë²¨ ì¸ì½”ë”© ê²°ê³¼ ìƒì„± =====
df_mlb = colab_multilabel_fit_transform(raw_df, cols=('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼'))
df_mlb['ì ìˆ˜'] = pd.to_numeric(df_mlb['ì ìˆ˜'], errors='coerce')

# ===== Colab ìŠ¤íƒ€ì¼ X/y, ì „ì²˜ë¦¬ ì •ì˜ =====
drop_cols = [c for c in ['ë°°ìš°ëª…','ë“œë¼ë§ˆëª…','ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼','ì ìˆ˜','ë°©ì˜ë…„ë„'] if c in df_mlb.columns]
X_colab_base = df_mlb.drop(columns=drop_cols)
y_all = df_mlb['ì ìˆ˜']
categorical_features = [c for c in ['ì—­í• ','ì„±ë³„','ë°©ì˜ë¶„ê¸°','ê²°í˜¼ì—¬ë¶€','ì—°ë ¹ëŒ€'] if c in X_colab_base.columns]
preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)],
    remainder='passthrough'
)

# ===== EDAìš© ë¦¬ìŠ¤íŠ¸ =====
genre_list = [g for sub in raw_df['ì¥ë¥´'].dropna().apply(clean_cell_colab) for g in sub]
broadcaster_list = [b for sub in raw_df['í”Œë«í¼'].dropna().apply(clean_cell_colab) for b in sub]
week_list = [w for sub in raw_df['ë°©ì˜ìš”ì¼'].dropna().apply(clean_cell_colab) for w in sub]
unique_genres = sorted(set(genre_list))

# ===== ì‚¬ì´ë“œë°” =====
with st.sidebar:
    st.header("ğŸ¤– ëª¨ë¸ ì„¤ì •")
    # ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•˜ê²Œ ê¸°ë³¸ê°’ 0.3ìœ¼ë¡œ ì„¤ì •
    test_size = st.slider('í…ŒìŠ¤íŠ¸ì…‹ ë¹„ìœ¨', 0.1, 0.5, 0.3, 0.05)
    feature_cols = st.multiselect(
        'íŠ¹ì„± ì„ íƒ(ì˜ˆì¸¡ íƒ­ìš©)',
        ['ë‚˜ì´','ë°©ì˜ë…„ë„','ì„±ë³„','ì¥ë¥´','ë°°ìš°ëª…','í”Œë«í¼','ê²°í˜¼ì—¬ë¶€'],
        default=['ë‚˜ì´','ë°©ì˜ë…„ë„','ì¥ë¥´']
    )

# ===== íƒ­ êµ¬ì„± =====
tabs = st.tabs(["ğŸ—‚ê°œìš”","ğŸ“Šê¸°ì´ˆí†µê³„","ğŸ“ˆë¶„í¬/êµì°¨","ğŸ’¬ì›Œë“œí´ë¼ìš°ë“œ","âš™ï¸í•„í„°","ğŸ”ì „ì²´ë³´ê¸°","ğŸ”§íŠœë‹","ğŸ¤–MLëª¨ë¸","ğŸ¯ì˜ˆì¸¡"])

# --- 4.1 ë°ì´í„° ê°œìš” ---
with tabs[0]:
    st.header("ë°ì´í„° ê°œìš”")
    c1,c2,c3 = st.columns(3)
    c1.metric("ìƒ˜í”Œ ìˆ˜", raw_df.shape[0])
    c2.metric("ì»¬ëŸ¼ ìˆ˜", raw_df.shape[1])
    c3.metric("ê³ ìœ  ì¥ë¥´", len(unique_genres))
    st.subheader("ê²°ì¸¡ì¹˜ ë¹„ìœ¨")
    st.dataframe(raw_df.isnull().mean())
    st.subheader("ì›ë³¸ ìƒ˜í”Œ")
    st.dataframe(raw_df.head(), use_container_width=True)

# --- 4.2 ê¸°ì´ˆí†µê³„ ---
with tabs[1]:
    st.header("ê¸°ì´ˆ í†µê³„: ì ìˆ˜")
    st.write(raw_df['ì ìˆ˜'].astype(float).describe())
    fig,ax=plt.subplots(figsize=(6,3))
    ax.hist(raw_df['ì ìˆ˜'].astype(float), bins=20)
    ax.set_title("ì „ì²´ í‰ì  ë¶„í¬")
    st.pyplot(fig)

# --- 4.3 ë¶„í¬/êµì°¨ë¶„ì„ ---
with tabs[2]:
    st.header("ë¶„í¬ ë° êµì°¨ë¶„ì„")
    st.subheader("ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    ct = (
        pd.DataFrame({'ë°©ì˜ë…„ë„': raw_df['ë°©ì˜ë…„ë„'], 'í”Œë«í¼': raw_df['í”Œë«í¼'].apply(clean_cell_colab)})
        .explode('í”Œë«í¼').groupby(['ë°©ì˜ë…„ë„','í”Œë«í¼']).size().reset_index(name='count')
    )
    ct['í”Œë«í¼_up'] = ct['í”Œë«í¼'].str.upper()
    focus = ['KBS','MBC','TVN','NETFLIX','SBS']
    fig3 = px.line(ct[ct['í”Œë«í¼_up'].isin(focus)], x='ë°©ì˜ë…„ë„', y='count', color='í”Œë«í¼',
                   log_y=True, title="ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    st.plotly_chart(fig3, use_container_width=True)

    # ---------- ìë™ ì¸ì‚¬ì´íŠ¸ ----------
    import numpy as np
    
    # ì—°-í”Œë«í¼ í”¼ë²—
    p = (ct.pivot_table(index='ë°©ì˜ë…„ë„', columns='í”Œë«í¼_up', values='count', aggfunc='sum')
           .fillna(0).astype(int))
    years = sorted(p.index)
    
    insights = []
    
    # 1) Netflix ê¸‰ì„±ì¥
    if 'NETFLIX' in p.columns:
        s = p['NETFLIX']
        nz = s[s > 0]
        if not nz.empty:
            first_year = int(nz.index.min())
            max_year, max_val = int(s.idxmax()), int(s.max())
            txt = f"- **ë„·í”Œë¦­ìŠ¤(OTT)ì˜ ê¸‰ì„±ì¥**: {first_year}ë…„ ì´í›„ ë¹ ë¥´ê²Œ ì¦ê°€, **{max_year}ë…„ {max_val}í¸**ìœ¼ë¡œ ìµœê³ ì¹˜."
            # 2020ë…„ ë¹„êµ
            if 2020 in p.index:
                comps = ", ".join([f"{b} {int(p.loc[2020,b])}í¸"
                                   for b in ['KBS','MBC','SBS'] if b in p.columns])
                txt += f" 2020ë…„ì—ëŠ” ë„·í”Œë¦­ìŠ¤ {int(p.loc[2020,'NETFLIX'])}í¸, ì§€ìƒíŒŒ({comps})ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€."
            insights.append(txt)
    
    # 2) ì§€ìƒíŒŒ ê°ì†Œ ì¶”ì„¸
    down_ter = []
    for b in ['KBS','MBC','SBS']:
        if b in p.columns and len(years) >= 2:
            slope = np.polyfit(years, p[b].reindex(years, fill_value=0), 1)[0]
            if slope < 0:
                down_ter.append(b)
    if down_ter:
        insights.append(f"- **ì§€ìƒíŒŒì˜ ì§€ì†ì  ê°ì†Œ**: {' / '.join(down_ter)} ë“± ì „í†µ 3ì‚¬ì˜ ì‘í’ˆ ìˆ˜ê°€ ì „ë°˜ì ìœ¼ë¡œ í•˜ë½ ì¶”ì„¸.")
    
    # 3) tvN ì„±ì¥ê³¼ ì •ì²´
    if 'TVN' in p.columns:
        tvn = p['TVN']
        peak_year, peak_val = int(tvn.idxmax()), int(tvn.max())
        tail = []
        for y in [y for y in [2020, 2021, 2022] if y in tvn.index]:
            tail.append(f"{y}ë…„ {int(tvn.loc[y])}í¸")
        insights.append(f"- **tvNì˜ ì„±ì¥ê³¼ ì •ì²´**: ìµœê³  {peak_year}ë…„ {peak_val}í¸. ìµœê·¼ ìˆ˜ë…„({', '.join(tail)})ì€ ì •ì²´/ì†Œí­ ê°ì†Œ ê²½í–¥.")
    
    # 4) 2022ë…„ ì „ë…„ ëŒ€ë¹„ ê°ì†Œ
    if 2021 in p.index and 2022 in p.index:
        downs = [c for c in p.columns if p.loc[2022, c] < p.loc[2021, c]]
        if downs:
            insights.append(f"- **2022ë…„ ì „ë…„ ëŒ€ë¹„ ê°ì†Œ**: {', '.join(downs)} ë“± ì—¬ëŸ¬ í”Œë«í¼ì´ 2021ë…„ë³´ë‹¤ ì¤„ì–´ë“¦.")
    
    # ì¶œë ¥
    st.markdown("**ì¸ì‚¬ì´íŠ¸**\n" + "\n".join(insights) +
                "\n\n*í•´ì„ ë©”ëª¨: OTT-ë°©ì†¡ì‚¬ ë™ì‹œë°©ì˜, ì œì‘í™˜ê²½(ì˜ˆì‚°/ì‹œì²­ë¥ ), ì½”ë¡œë‚˜19 ë“± ì™¸ë¶€ ìš”ì¸ì´ ì˜í–¥ì„ ì¤€ ê²ƒìœ¼ë¡œ í•´ì„ ê°€ëŠ¥.*")

    # --- ì¥ë¥´ 'ê°œìˆ˜'ë³„ ë°°ìš° í‰ê·  í‰ì  (1~2 / 3~4 / 5~6 / 7+) ---
    st.subheader("ì¥ë¥´ ê°œìˆ˜ë³„ í‰ê·  í‰ì  (ë°°ìš° ë‹¨ìœ„, 1 ~ 2 / 3 ~ 4 / 5 ~ 6 / 7+)")
    
    # 1) ë°°ìš°ë³„ ê³ ìœ  ì¥ë¥´ ê°œìˆ˜
    gdf = (
        pd.DataFrame({
            'ë°°ìš°ëª…': raw_df['ë°°ìš°ëª…'],
            'ì¥ë¥´' :  raw_df['ì¥ë¥´'].apply(clean_cell_colab)
        })
        .explode('ì¥ë¥´')
        .dropna(subset=['ë°°ìš°ëª…','ì¥ë¥´'])
    )
    genre_cnt = gdf.groupby('ë°°ìš°ëª…')['ì¥ë¥´'].nunique().rename('ì¥ë¥´ê°œìˆ˜')
    
    # 2) ë°°ìš°ë³„ í‰ê·  ì ìˆ˜
    actor_mean = (raw_df.groupby('ë°°ìš°ëª…', as_index=False)['ì ìˆ˜']
                  .mean()
                  .rename(columns={'ì ìˆ˜':'ë°°ìš°í‰ê· ì ìˆ˜'}))
    
    # 3) ë³‘í•© + êµ¬ê°„í™”(1~2, 3~4, 5~6, 7+)
    df_actor = actor_mean.merge(genre_cnt.reset_index(), on='ë°°ìš°ëª…', how='left')
    df_actor['ì¥ë¥´ê°œìˆ˜'] = df_actor['ì¥ë¥´ê°œìˆ˜'].fillna(0).astype(int)
    df_actor = df_actor[df_actor['ì¥ë¥´ê°œìˆ˜'] > 0].copy()  # ì¥ë¥´ì •ë³´ ì—†ëŠ” ë°°ìš° ì œì™¸
    
    def bucket(n: int) -> str:
        if n <= 2:  return '1~2ê°œ'
        if n <= 4:  return '3~4ê°œ'
        if n <= 6:  return '5~6ê°œ'
        return '7ê°œ ì´ìƒ'
    
    df_actor['ì¥ë¥´ê°œìˆ˜êµ¬ê°„'] = df_actor['ì¥ë¥´ê°œìˆ˜'].apply(bucket)
    order_bins = ['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ']
    df_actor['ì¥ë¥´ê°œìˆ˜êµ¬ê°„'] = pd.Categorical(df_actor['ì¥ë¥´ê°œìˆ˜êµ¬ê°„'],
                                          categories=order_bins, ordered=True)
    
    # 4) ë°•ìŠ¤í”Œë¡¯
    fig_box = px.box(
        df_actor, x='ì¥ë¥´ê°œìˆ˜êµ¬ê°„', y='ë°°ìš°í‰ê· ì ìˆ˜',
        category_orders={'ì¥ë¥´ê°œìˆ˜êµ¬ê°„': order_bins},
        title="ì¥ë¥´ ê°œìˆ˜ë³„ ë°°ìš° í‰ê·  ì ìˆ˜ ë¶„í¬ (1 ~ 2 / 3 ~ 4 / 5 ~ 6 / 7+)"
    )
    st.plotly_chart(fig_box, use_container_width=True)
    
    # 5) ê·¸ë˜í”„ ì•„ë˜ ì¸ì‚¬ì´íŠ¸ ìë™ ìƒì„±
    stats = (df_actor.groupby('ì¥ë¥´ê°œìˆ˜êµ¬ê°„')['ë°°ìš°í‰ê· ì ìˆ˜']
             .agg(í‰ê· ='mean', ì¤‘ì•™ê°’='median', í‘œë³¸ìˆ˜='count')
             .reindex(order_bins)
             .dropna(how='all')
             .round(3))
    
    if not stats.empty and stats['í‘œë³¸ìˆ˜'].sum() > 0:
        # ìµœê³  ê·¸ë£¹
        best_mean_grp   = stats['í‰ê· '].idxmax()
        best_median_grp = stats['ì¤‘ì•™ê°’'].idxmax()
    
        # ë‹¨ì¡° ê²½í–¥(í‰ê·  ê¸°ì¤€)
        vals = stats['í‰ê· '].dropna().values
        diffs = pd.Series(vals).diff().dropna()
        if (diffs >= 0).all():
            trend = "ì¥ë¥´ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í‰ê·  í‰ì ì´ **ë†’ì•„ì§€ëŠ” ê²½í–¥**"
        elif (diffs <= 0).all():
            trend = "ì¥ë¥´ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í‰ê·  í‰ì ì´ **ë‚®ì•„ì§€ëŠ” ê²½í–¥**"
        else:
            trend = "ì¥ë¥´ ìˆ˜ì™€ í‰ê·  í‰ì  ê°„ **ì¼ê´€ëœ ë‹¨ì¡° ê²½í–¥ì€ ì•½í•¨**"
    
        # 1~2ê°œ vs 7ê°œ ì´ìƒ ë¹„êµ(ë‘˜ ë‹¤ ìˆì„ ë•Œë§Œ)
        comp_txt = ""
        if {'1~2ê°œ','7ê°œ ì´ìƒ'}.issubset(stats.index):
            diff_mean = stats.loc['1~2ê°œ','í‰ê· '] - stats.loc['7ê°œ ì´ìƒ','í‰ê· ']
            diff_med  = stats.loc['1~2ê°œ','ì¤‘ì•™ê°’'] - stats.loc['7ê°œ ì´ìƒ','ì¤‘ì•™ê°’']
            sign = "ë†’ìŒ" if diff_mean >= 0 else "ë‚®ìŒ"
            comp_txt = f"- **1~2ê°œ vs 7ê°œ ì´ìƒ**: í‰ê·  {abs(diff_mean):.3f}p {sign}, ì¤‘ì•™ê°’ ì°¨ì´ {abs(diff_med):.3f}p\n"
    
        st.markdown("**ìš”ì•½ í†µê³„(ë°°ìš° ë‹¨ìœ„)**")
        try:
            st.markdown(stats.to_markdown())
        except Exception:
            st.dataframe(stats.reset_index(), use_container_width=True)
    
        st.markdown(
            f"""
    **ì¸ì‚¬ì´íŠ¸**
    - í‰ê·  ê¸°ì¤€ ìµœê³  ê·¸ë£¹: **{best_mean_grp}** / ì¤‘ì•™ê°’ ê¸°ì¤€ ìµœê³  ê·¸ë£¹: **{best_median_grp}**  
    - {trend}  
    {comp_txt if comp_txt else ""}
    - ì¥ë¥´ ë‹¤ì–‘ì„±â†‘ â†’ í‰ì â†‘ (ë‹¨ì¡° ì¦ê°€)
í‰ê· ì´ 7.774 â†’ 7.802 â†’ 7.861 â†’ 7.911ë¡œ ê³„ë‹¨ì‹ ìƒìŠ¹í•©ë‹ˆë‹¤.
ì¤‘ì•™ê°’ë„ 7.700 â†’ 7.715 â†’ 7.810 â†’ 7.901ë¡œ ë™ì¼í•˜ê²Œ ì¦ê°€.

-> ë‹¤ì¥ë¥´ ê²½í—˜ì´ ë§ì„ìˆ˜ë¡ ì—°ê¸° ì ì‘ë ¥/ì¸ì§€ë„/ìºìŠ¤íŒ… íŒŒì›Œê°€ ë†’ì•„ ì‘í’ˆ ì„ íƒ í’ˆì§ˆì´ ì¢‹ì•„ì¡Œì„ ê°€ëŠ¥ì„±.
            """
        )
    else:
        st.info("ì¥ë¥´ ê°œìˆ˜ êµ¬ê°„ë³„ í†µê³„ë¥¼ ê³„ì‚°í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")


    st.subheader("ì£¼ì—° ë°°ìš° ê²°í˜¼ ìƒíƒœë³„ í‰ê·  ì ìˆ˜ ë¹„êµ")
    main_roles = raw_df[raw_df['ì—­í• ']=='ì£¼ì—°'].copy()
    main_roles['ê²°í˜¼ìƒíƒœ'] = main_roles['ê²°í˜¼ì—¬ë¶€'].apply(lambda x: 'ë¯¸í˜¼' if x=='ë¯¸í˜¼' else 'ë¯¸í˜¼ ì™¸')
    avg_scores_by_marriage = main_roles.groupby('ê²°í˜¼ìƒíƒœ')['ì ìˆ˜'].mean()
    fig, ax = plt.subplots(figsize=(3,3))
    bars = ax.bar(avg_scores_by_marriage.index, avg_scores_by_marriage.values, color=['mediumseagreen','gray'])
    for b in bars:
        yv = b.get_height()
        ax.text(b.get_x()+b.get_width()/2, yv+0.005, f'{yv:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
    ax.set_title('ì£¼ì—° ë°°ìš° ê²°í˜¼ ìƒíƒœë³„ í‰ê·  ì ìˆ˜ ë¹„êµ'); ax.set_ylabel('í‰ê·  ì ìˆ˜'); ax.set_xlabel('ê²°í˜¼ ìƒíƒœ')
    ax.set_ylim(min(avg_scores_by_marriage.values)-0.05, max(avg_scores_by_marriage.values)+0.05)
    ax.grid(axis='y', linestyle='--', alpha=0.5)
    st.pyplot(fig)
    # --- ê·¸ë˜í”„ ì•„ë˜ ì¸ì‚¬ì´íŠ¸ ---
    m_single = avg_scores_by_marriage.get('ë¯¸í˜¼')
    m_else   = avg_scores_by_marriage.get('ë¯¸í˜¼ ì™¸')
    diff_txt = f"(ì°¨ì´ {m_single - m_else:+.3f}p)" if (m_single is not None and m_else is not None) else ""
    
    st.markdown(
        f"""
    **ìš”ì•½**
    - ë¯¸í˜¼ í‰ê· : **{m_single:.3f}**, ë¯¸í˜¼ ì™¸ í‰ê· : **{m_else:.3f}** {diff_txt}
    
    **ì¸ì‚¬ì´íŠ¸**
    - ë¯¸í˜¼ ë°°ìš°ëŠ” ìƒëŒ€ì ìœ¼ë¡œ **ì²­ì¶˜ë¬¼Â·ë¡œë§¨í‹± ì½”ë¯¸ë””Â·ì„±ì¥í˜• ì„œì‚¬**ì— ìì£¼ ë“±ì¥í•˜ë©°, ì´ëŸ° ì¥ë¥´ëŠ” ì‹œì²­ì ì„ í˜¸ë„ì™€ ê°ì • ì´ì…ë¥ ì´ ë†’ì•„ **í‰ì ì´ ìš°í˜¸ì ìœ¼ë¡œ í˜•ì„±**ë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
    - ë°˜ë©´, ê¸°í˜¼ ë°°ìš°ëŠ” **ê°€ì¡±ê·¹Â·ì‚¬íšŒê·¹Â·ì •ì¹˜ë¬¼**ì— ì¶œì—°í•˜ëŠ” ë¹„ì¤‘ì´ ë†’ê³ , ì´ë“¤ ì¥ë¥´ëŠ” ì£¼ì œì˜ ë¬´ê²Œê°/ëª°ì… ì¥ë²½ìœ¼ë¡œ **í‰ê°€ê°€ ê°ˆë¦´ ê°€ëŠ¥ì„±**ì´ í½ë‹ˆë‹¤.
    - ì‹œì²­ì ì¸ì‹ì—ì„œ â€˜**ì‹±ê¸€**â€™ ì´ë¯¸ì§€ëŠ” ë³´ë‹¤ ììœ ë¡­ê³  ë‹¤ì–‘í•œ ìºë¦­í„° ì†Œë¹„ë¡œ ì´ì–´ì§€ê¸° ì‰¬ì›Œ, **ì—°ì•  ì„œì‚¬ ëª°ì…ë„**ë‚˜ **ëŒ€ì¤‘ì  íŒíƒ€ì§€ ìê·¹** ì—­í• ì´ ë¯¸í˜¼ ë°°ìš°ì—ê²Œ ë” ìì£¼ ë¶€ì—¬ë˜ëŠ” í¸ì…ë‹ˆë‹¤.
    """
    )

    st.subheader("ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜")
    dfg = raw_df.copy(); dfg['ì¥ë¥´'] = dfg['ì¥ë¥´'].apply(clean_cell_colab)
    dfg = dfg.explode('ì¥ë¥´').dropna(subset=['ì¥ë¥´','ì ìˆ˜'])
    g_score = dfg.groupby('ì¥ë¥´')['ì ìˆ˜'].mean().round(3)
    g_count = dfg['ì¥ë¥´'].value_counts()
    gdf = (pd.DataFrame({'í‰ê·  ì ìˆ˜': g_score, 'ì‘í’ˆ ìˆ˜': g_count}).reset_index().rename(columns={'index':'ì¥ë¥´'}))
    gdf = gdf.sort_values('ì‘í’ˆ ìˆ˜', ascending=False).reset_index(drop=True)
    fig, ax1 = plt.subplots(figsize=(12,6))
    bars = ax1.bar(range(len(gdf)), gdf['ì‘í’ˆ ìˆ˜'], color='lightgray')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜'); ax1.set_xticks(range(len(gdf))); ax1.set_xticklabels(gdf['ì¥ë¥´'], rotation=45, ha='right')
    for i, r in enumerate(bars):
        h = r.get_height(); ax1.text(i, h+max(2, h*0.01), f'{int(h)}', ha='center', va='bottom', fontsize=10, fontweight='bold', color='#444')
    ax2 = ax1.twinx(); ax2.plot(range(len(gdf)), gdf['í‰ê·  ì ìˆ˜'], marker='o', linewidth=2, color='tab:blue')
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', color='tab:blue'); ax2.tick_params(axis='y', colors='tab:blue')
    ax2.set_ylim(gdf['í‰ê·  ì ìˆ˜'].min()-0.1, gdf['í‰ê·  ì ìˆ˜'].max()+0.1)
    for i, v in enumerate(gdf['í‰ê·  ì ìˆ˜']):
        ax2.text(i, v+0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold', color='tab:blue')
    plt.title('ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜'); ax1.set_xlabel('ì¥ë¥´'); ax1.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout(); st.pyplot(fig)

    # ====== ìš”ì•½(ë°ì´í„°) + ì¸ì‚¬ì´íŠ¸ ì¶œë ¥ ======
    # ìƒ/í•˜ìœ„ ì¥ë¥´ ì •ë¦¬
    top_cnt = gdf.nlargest(3, 'ì‘í’ˆ ìˆ˜')
    low_cnt = gdf.nsmallest(3, 'ì‘í’ˆ ìˆ˜')
    top_score = gdf.nlargest(4, 'í‰ê·  ì ìˆ˜')  # ìƒìœ„ 4ê°œ ì •ë„
    
    def fmt_counts(df):
        return ", ".join([f"{r['ì¥ë¥´']}({int(r['ì‘í’ˆ ìˆ˜']):,}í¸)" for _, r in df.iterrows()])
    
    def fmt_scores(df):
        return ", ".join([f"{r['ì¥ë¥´']}({r['í‰ê·  ì ìˆ˜']:.3f})" for _, r in df.iterrows()])
    
    st.markdown(
        f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ì‘í’ˆ ìˆ˜ ìƒìœ„: **{fmt_counts(top_cnt)}**  
    - ì‘í’ˆ ìˆ˜ í•˜ìœ„: **{fmt_counts(low_cnt)}**  
    - í‰ê·  í‰ì  ìƒìœ„: **{fmt_scores(top_score)}**
    
    **ì¸ì‚¬ì´íŠ¸(ìƒì‚°ëŸ‰)**  
    - **romance / drama / comedy**ëŠ” ë³´í¸ì  ê°ì •ì„ ê³¼ ì¼ìƒ ë°°ê²½ìœ¼ë¡œ **ë¹„ìš© ëŒ€ë¹„ íš¨ìœ¨**ì´ ë†’ê³ , í­ë„“ì€ ì‹œì²­ì¸µì„ í™•ë³´í•˜ê¸° ì¢‹ì•„ **ë°˜ë³µ ì œì‘**ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.  
    - **action / sf / hist_war**ëŠ” CGÂ·ë¬´ìˆ Â·ëŒ€ê·œëª¨ ì„¸íŠ¸Â·ì—­ì‚¬ ê³ ì¦ ë“±ìœ¼ë¡œ **ì œì‘ë¹„Â·ì œì‘ê¸°ê°„ ë¶€ë‹´**ì´ ì»¤ ìƒëŒ€ì ìœ¼ë¡œ **ë¬¼ëŸ‰ì´ ì ì€** í¸ì…ë‹ˆë‹¤.
    
    **ì¸ì‚¬ì´íŠ¸(í‰ì )**  
    - **hist_war, thriller, sf**ëŠ” ë§ˆë‹ˆì•„ì¸µ ì¤‘ì‹¬ìœ¼ë¡œ **ì™„ì„±ë„ì™€ ê°œì„±**ì´ í‰ê°€ í¬ì¸íŠ¸ê°€ ë˜ë©° **í‰ì ì´ ë†’ê²Œ í˜•ì„±**ë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.  
    - ë°˜ë©´ **romance, society**ëŠ” ì‚¼ê°ê´€ê³„Â·ì¶œìƒì˜ ë¹„ë°€ ë“± **ê°ì • ì„œì‚¬ì˜ ë°˜ë³µ**ìœ¼ë¡œ ì¤‘í›„ë°˜ ì „ê°œì— ë”°ë¼ **í˜¸ë¶ˆí˜¸**ê°€ ì»¤ì ¸ í‰ê·  ì ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
    - **action / sf**ëŠ” ì‹œê°ì  ì„íŒ©íŠ¸ê°€ ì»¤ OTT/ì˜¨ë¼ì¸ ì‹œì²­ í™˜ê²½ì—ì„œ **ì´ˆê¸° ë§Œì¡±ë„(ì²«ì¸ìƒ íš¨ê³¼)**ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚˜ í‰ê·  ì ìˆ˜ë¥¼ ëŒì–´ì˜¬ë¦¬ê¸°ë„ í•©ë‹ˆë‹¤.
    """
    )


    st.subheader("ë°©ì˜ ìš”ì¼ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜ (ì›”â†’ì¼)")
    dfe = raw_df.copy(); dfe['ë°©ì˜ìš”ì¼'] = dfe['ë°©ì˜ìš”ì¼'].apply(clean_cell_colab)
    dfe = dfe.explode('ë°©ì˜ìš”ì¼').dropna(subset=['ë°©ì˜ìš”ì¼','ì ìˆ˜']).copy()
    dfe['ë°©ì˜ìš”ì¼'] = dfe['ë°©ì˜ìš”ì¼'].astype(str).str.strip().str.lower()
    ordered = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']
    day_ko = {'monday':'ì›”','tuesday':'í™”','wednesday':'ìˆ˜','thursday':'ëª©','friday':'ê¸ˆ','saturday':'í† ','sunday':'ì¼'}
    mean_by = dfe.groupby('ë°©ì˜ìš”ì¼')['ì ìˆ˜'].mean().reindex(ordered)
    cnt_by = dfe['ë°©ì˜ìš”ì¼'].value_counts().reindex(ordered).fillna(0).astype(int)
    fig, ax1 = plt.subplots(figsize=(12,6))
    bars = ax1.bar(ordered, cnt_by.values, alpha=0.3, color='tab:gray')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜', color='tab:gray'); ax1.tick_params(axis='y', labelcolor='tab:gray')
    for b in bars:
        h = b.get_height(); ax1.text(b.get_x()+b.get_width()/2, h+0.5, f'{int(h)}', ha='center', va='bottom', fontsize=9, color='black')
    ax2 = ax1.twinx(); ax2.plot(ordered, mean_by.values, marker='o', color='tab:blue')
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', color='tab:blue'); ax2.tick_params(axis='y', labelcolor='tab:blue')
    if mean_by.notna().any(): ax2.set_ylim(mean_by.min()-0.05, mean_by.max()+0.05)
    for x, yv in zip(ordered, mean_by.values):
        if pd.notna(yv): ax2.text(x, yv+0.005, f'{yv:.3f}', color='tab:blue', fontsize=9, ha='center')
    ax1.set_xticks(ordered); ax1.set_xticklabels([day_ko[d] for d in ordered])
    plt.title('ë°©ì˜ ìš”ì¼ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜ (ì›”ìš”ì¼ â†’ ì¼ìš”ì¼ ìˆœ)'); plt.tight_layout(); st.pyplot(fig)

    # ====== ìš”ì•½(ë°ì´í„°) + ì¸ì‚¬ì´íŠ¸ ì¶œë ¥ ======
    weekday = ['monday','tuesday','wednesday','thursday']
    weekend = ['friday','saturday','sunday']
    
    wk_avg = mean_by.loc[weekday].mean(skipna=True)
    we_avg = mean_by.loc[weekend].mean(skipna=True)
    
    top_day_en  = mean_by.idxmax() if mean_by.notna().any() else None
    top_day_ko  = day_ko.get(top_day_en, "N/A") if top_day_en else "N/A"
    top_mean    = float(mean_by.max()) if mean_by.notna().any() else float("nan")
    
    wk_cnt = int(cnt_by.loc[weekday].sum())
    we_cnt = int(cnt_by.loc[weekend].sum())
    
    st.markdown(
        f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ì£¼ì¤‘ í‰ê·  í‰ì (ì›”~ëª©): **{wk_avg:.3f}** Â· ì£¼ì¤‘ ì‘í’ˆ ìˆ˜ í•©ê³„: **{wk_cnt}í¸**  
    - ì£¼ë§ í‰ê·  í‰ì (ê¸ˆ~ì¼): **{we_avg:.3f}** Â· ì£¼ë§ ì‘í’ˆ ìˆ˜ í•©ê³„: **{we_cnt}í¸**  
    - í‰ê·  í‰ì  ìµœê³  ìš”ì¼: **{top_day_ko} {top_mean:.3f}ì **
    
    **ì¸ì‚¬ì´íŠ¸(ìš”ì•½)**  
    - **ì£¼ì¤‘(ì›”~ëª©)**: ì¼ìƒ í¸ì„± ë¹„ì¤‘ì´ ë†’ê³ , ë‹¤ì–‘í•œ ì—°ë ¹/ì·¨í–¥ì„ ê²¨ëƒ¥í•œ **ë³´í¸ì  ì½˜í…ì¸ **ê°€ ë§ìŒ.  
      ì œì‘ ì†ë„Â·ì–‘ì‚°ì„±, **ì‹œì²­ë¥  ì§€í–¥** í¸ì„±ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‘ë“œëŸ¬ì§.  
    - **ê¸ˆìš”ì¼**: í•œ ì£¼ í”¼ë¡œì™€ ì™¸ë¶€ ì¼ì •(íšŒì‹Â·ì™¸ì¶œÂ·ì£¼ë§ ì¤€ë¹„)ë¡œ **ì‹¤ì‹œê°„ ì‹œì²­ ì§‘ì¤‘ë„ ë‚®ìŒ** â†’  
      ë°©ì†¡ì‚¬ëŠ” **ì˜ˆëŠ¥Â·ë‰´ìŠ¤Â·ì˜í™” ëŒ€ì²´ í¸ì„±**ì„ íƒí•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ë“œë¼ë§ˆ í¸ì„±/ì„±ê³¼ê°€ ì•½í•¨.  
    - **ì¼ìš”ì¼**: ë‹¤ìŒ ë‚ ì´ ì›”ìš”ì¼ì´ë¼ **ê°€ë²¼ìš´ ì½˜í…ì¸  ì„ í˜¸**. ì „í†µì ìœ¼ë¡œ ì˜ˆëŠ¥ì´ í”„ë¼ì„ì„ ì¥ì•… â†’  
      ë“œë¼ë§ˆ **í¸ì„± ìˆ˜ìš” ìì²´ê°€ ë‚®ì€ êµ¬ì¡°**.  
    - **í† ìš”ì¼**: ì‹œê°„ì  ì—¬ìœ  + ë‹¤ìŒ ë‚  íœ´ì‹ìœ¼ë¡œ **ì‹œì²­ë¥ Â·ëª°ì…Â·ê´‘ê³  íš¨ê³¼ê°€ ìµœëŒ€**.  
      ê³ í’ˆì§ˆ ì‘í’ˆì„ ì§‘ì¤‘ íˆ¬ì…í•˜ë©° **ê²½ìŸì´ ê°€ì¥ ì¹˜ì—´**í•œ ìš”ì¼.  
    
    **í•´ì„ ë©”ëª¨**  
    - ì£¼ì¤‘ì€ í”¼ë¡œÂ·ì‹œê°„ ì œì•½ íƒ“ì— **ì™„ì£¼ìœ¨/ëª°ì…ë„ê°€ ë‚®ì•„** í˜¸ë¶ˆí˜¸ê°€ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚˜ê¸° ì‰¬ì›€.  
    - ì£¼ë§(íŠ¹íˆ í† ìš”ì¼) í¸ì„±ì€ ê¸°íš ë‹¨ê³„ë¶€í„° **íƒ€ê¹ƒÂ·ì˜ˆì‚°Â·ì™„ì„±ë„**ê°€ ë†’ì€ ì „ëµ í¸ì„±ì´ ë§ì•„,  
      **ê°ì • ëª°ì…**ì´ ì˜ ì¼ì–´ë‚˜ê³  ì¢‹ì€ ì‘í’ˆì¼ìˆ˜ë¡ **ìš°í˜¸ì  í‰ê°€**ê°€ í˜•ì„±ë˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.
    """
    )


    # --- ì£¼ì—° ë°°ìš° ì„±ë³„ ì¸ì›ìˆ˜ ë° ë¹„ìœ¨ ---
    st.subheader("ì£¼ì—° ë°°ìš° ì„±ë³„ ì¸ì›ìˆ˜ ë° ë¹„ìœ¨")

    # 'ì£¼ì—°'ë§Œ í•„í„° + ì„±ë³„ ê²°ì¸¡ ì œê±°
    main_roles = raw_df[raw_df['ì—­í• '] == 'ì£¼ì—°'].dropna(subset=['ì„±ë³„']).copy()
    
    # ì„±ë³„ë³„ ì¸ì›ìˆ˜ / í™•ë¥ 
    gender_counts = main_roles['ì„±ë³„'].value_counts()
    total_main_roles = int(gender_counts.sum())
    gender_probs = (gender_counts / total_main_roles).reindex(gender_counts.index)
    
    # ìƒ‰ìƒ(ì„±ë³„ ê°œìˆ˜ì— ë§ì¶° ë°˜ë³µ)
    palette = ['skyblue', 'lightpink', 'lightgreen', 'lightgray', 'orange', 'violet']
    colors = [palette[i % len(palette)] for i in range(len(gender_counts))]
    
    # ê·¸ë˜í”„
    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.bar(gender_counts.index.astype(str), gender_counts.values, color=colors)
    
    # ë¼ë²¨: ì¸ì›ìˆ˜ + í™•ë¥ (%) í‘œê¸°
    for bar, prob in zip(bars, gender_probs.values):
        yval = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width()/2, yval + max(2, yval*0.02),
            f'{int(yval)}ëª…\n({prob*100:.2f}%)',
            ha='center', va='bottom', fontsize=11, fontweight='bold'
        )
    
    ax.set_title('ì£¼ì—° ë°°ìš° ì„±ë³„ ì¸ì›ìˆ˜ ë° ë¹„ìœ¨', fontsize=14)
    ax.set_ylabel('ì¸ì›ìˆ˜'); ax.set_xlabel('ì„±ë³„')
    
    # ì—¬ìœ  ê³µë°±
    ymax = gender_counts.max()
    ax.set_ylim(0, ymax + max(10, int(ymax*0.15)))
    
    ax.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    st.pyplot(fig)

    # --- ë°©ì˜ë…„ë„ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜ ---
    st.subheader("ë°©ì˜ë…„ë„ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜")
    
    # ìˆ«ìí˜• ë³€í™˜ & ê²°ì¸¡ ì œê±°
    dfe = raw_df.copy()
    dfe['ë°©ì˜ë…„ë„'] = pd.to_numeric(dfe['ë°©ì˜ë…„ë„'], errors='coerce')
    dfe['ì ìˆ˜']    = pd.to_numeric(dfe['ì ìˆ˜'], errors='coerce')
    dfe = dfe.dropna(subset=['ë°©ì˜ë…„ë„','ì ìˆ˜']).copy()
    dfe['ë°©ì˜ë…„ë„'] = dfe['ë°©ì˜ë…„ë„'].astype(int)
    
    # ì§‘ê³„
    mean_score_by_year = dfe.groupby('ë°©ì˜ë…„ë„')['ì ìˆ˜'].mean().round(3)
    count_by_year      = dfe['ë°©ì˜ë…„ë„'].value_counts()
    
    # xì¶• ì—°ë„(ë‘˜ì˜ í•©ì§‘í•©, ì˜¤ë¦„ì°¨ìˆœ)
    years = sorted(set(mean_score_by_year.index) | set(count_by_year.index))
    mean_s = mean_score_by_year.reindex(years)
    count_s = count_by_year.reindex(years, fill_value=0)
    
    # ì‹œê°í™”
    fig, ax1 = plt.subplots(figsize=(12, 6))
    
    # ì™¼ìª½ Yì¶•: ì‘í’ˆ ìˆ˜ (ë§‰ëŒ€)
    color_bar = 'tab:gray'
    ax1.set_xlabel('ë°©ì˜ë…„ë„')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜', color=color_bar)
    bars = ax1.bar(years, count_s.values, alpha=0.3, color=color_bar, width=0.6)
    ax1.tick_params(axis='y', labelcolor=color_bar)
    
    # ë§‰ëŒ€ ìœ„ ìˆ˜ì¹˜
    for bar in bars:
        h = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2, h + max(0.5, h*0.02),
                 f'{int(h)}', ha='center', va='bottom', fontsize=9, color='black')
    
    # ì˜¤ë¥¸ìª½ Yì¶•: í‰ê·  ì ìˆ˜ (ì„ )
    ax2 = ax1.twinx()
    color_line = 'tab:blue'
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', color=color_line)
    ax2.plot(years, mean_s.values, marker='o', color=color_line)
    ax2.tick_params(axis='y', labelcolor=color_line)
    if mean_s.notna().any():
        ax2.set_ylim(mean_s.min() - 0.05, mean_s.max() + 0.05)
    
    # ì  ìœ„ ìˆ˜ì¹˜
    for x, y in zip(years, mean_s.values):
        if pd.notna(y):
            ax2.text(x, y + 0.01, f'{y:.3f}', color=color_line, fontsize=9, ha='center')
    
    plt.title('ë°©ì˜ë…„ë„ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜')
    plt.tight_layout()
    st.pyplot(fig)

    # ìœ íš¨ ì—°ë„ë§Œ ì„ íƒ
    valid_mask = mean_s.notna() & count_s.notna()
    yrs   = pd.Index(years)[valid_mask]
    cnt   = count_s[valid_mask].astype(float)
    meanv = mean_s[valid_mask].astype(float)
    
    # (1) ì „ì²´ êµ¬ê°„ ìƒê´€ê³„ìˆ˜
    r_all = float(np.corrcoef(cnt.values, meanv.values)[0, 1]) if len(yrs) >= 2 else np.nan
    
    # (2) 2017ë…„ ì´í›„ ìƒê´€ê³„ìˆ˜ (ë°ì´í„° ìˆìœ¼ë©´)
    mask_2017 = yrs >= 2017
    if mask_2017.any() and mask_2017.sum() >= 2:
        r_2017 = float(np.corrcoef(cnt[mask_2017].values, meanv[mask_2017].values)[0, 1])
    else:
        r_2017 = np.nan
    
    # (3) CAGR(ì‘í’ˆ ìˆ˜) - ì „ì²´ / 2017â†’ë§ˆì§€ë§‰
    def calc_cagr(series: pd.Series, start_year: int, end_year: int):
        if start_year in series.index and end_year in series.index:
            start, end = float(series.loc[start_year]), float(series.loc[end_year])
            n = int(end_year - start_year)
            if start > 0 and n > 0:
                return (end / start) ** (1 / n) - 1
        return np.nan
    
    first_year, last_year = int(yrs.min()), int(yrs.max())
    cagr_all   = calc_cagr(count_s, first_year, last_year)
    cagr_2017  = calc_cagr(count_s, max(2017, first_year), last_year)
    
    # (4) ìš”ì•½ í‘œì‹œ
    st.markdown("**ì¶”ê°€ í†µê³„ ìš”ì•½**")
    st.markdown(
        f"""
    - ì „ì²´ ê¸°ê°„ ìƒê´€ê³„ìˆ˜ r(ì‘í’ˆ ìˆ˜ vs í‰ê·  ì ìˆ˜): **{r_all:.3f}**  
    - 2017ë…„ ì´í›„ ìƒê´€ê³„ìˆ˜ r: **{r_2017:.3f}**  
    - ì‘í’ˆ ìˆ˜ CAGR(ì „ì²´ {first_year}â†’{last_year}): **{(cagr_all*100):.2f}%/ë…„**  
    - ì‘í’ˆ ìˆ˜ CAGR(2017â†’{last_year}): **{(cagr_2017*100):.2f}%/ë…„**
    """
    )
    
    # (ì„ íƒ) í•´ì„ í•œ ì¤„
    if not np.isnan(r_2017):
        trend = "ìŒ(-)ì˜" if r_2017 < 0 else "ì–‘(+)ì˜"
        st.caption(f"ë©”ëª¨: 2017ë…„ ì´í›„ êµ¬ê°„ì—ì„œ ì‘í’ˆ ìˆ˜ì™€ í‰ê·  ì ìˆ˜ëŠ” **{trend} ìƒê´€**ì„ ë³´ì…ë‹ˆë‹¤.")

    # --- ì—°ë ¹ëŒ€ë³„ ì‘í’ˆ ìˆ˜ & ì„±ë³„ í‰ê·  ì ìˆ˜ (ì£¼ì—° ë°°ìš° ê¸°ì¤€) ---
    st.subheader("ì—°ë ¹ëŒ€ë³„ ì‘í’ˆ ìˆ˜ ë° ì„±ë³„ í‰ê·  ì ìˆ˜ (ì£¼ì—° ë°°ìš° ê¸°ì¤€)")
    
    import re
    import numpy as np
    
    # 1) ë°ì´í„° ì¤€ë¹„: ì£¼ì—°ë§Œ, í•„ìš”í•œ ì»¬ëŸ¼ ê²°ì¸¡ ì œê±°
    main_roles = raw_df.copy()
    main_roles = main_roles[main_roles['ì—­í• '] == 'ì£¼ì—°']
    main_roles = main_roles.dropna(subset=['ì—°ë ¹ëŒ€','ì„±ë³„','ì ìˆ˜']).copy()
    main_roles['ì ìˆ˜'] = pd.to_numeric(main_roles['ì ìˆ˜'], errors='coerce')
    main_roles = main_roles.dropna(subset=['ì ìˆ˜'])
    
    # 2) ì—°ë ¹ëŒ€ ì •ë ¬ í‚¤ (ì˜ˆ: '20ëŒ€ í›„ë°˜'ë„ 20ìœ¼ë¡œ ì¸ì‹, '50ëŒ€ ì´ìƒ'ì€ 50)
    def age_key(s: str):
        m = re.search(r'(\d+)', str(s))
        return int(m.group(1)) if m else 999
    
    age_order = sorted(main_roles['ì—°ë ¹ëŒ€'].astype(str).unique(), key=age_key)
    
    # 3) ì—°ë ¹ëŒ€ë³„ ì‘í’ˆ ìˆ˜
    age_counts = (main_roles['ì—°ë ¹ëŒ€']
                  .value_counts()
                  .reindex(age_order)
                  .fillna(0)
                  .astype(int))
    
    # 4) ì„±ë³„+ì—°ë ¹ëŒ€ë³„ í‰ê·  ì ìˆ˜
    ga = (main_roles.groupby(['ì„±ë³„','ì—°ë ¹ëŒ€'])['ì ìˆ˜']
          .mean()
          .round(3)
          .reset_index())
    
    male_vals   = ga[ga['ì„±ë³„']=='ë‚¨ì'].set_index('ì—°ë ¹ëŒ€').reindex(age_order)['ì ìˆ˜']
    female_vals = ga[ga['ì„±ë³„']=='ì—¬ì'].set_index('ì—°ë ¹ëŒ€').reindex(age_order)['ì ìˆ˜']
    
    # 5) ì‹œê°í™”
    fig, ax1 = plt.subplots(figsize=(10, 6))
    
    # ë§‰ëŒ€: ì‘í’ˆ ìˆ˜
    bars = ax1.bar(age_order, age_counts.values, color='lightgray', label='ì‘í’ˆ ìˆ˜')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜', fontsize=12)
    ax1.set_ylim(0, max(age_counts.max()*1.2, age_counts.max()+2))
    
    # ë§‰ëŒ€ ìœ„ ìˆ˜ì¹˜
    for rect in bars:
        h = rect.get_height()
        ax1.text(rect.get_x()+rect.get_width()/2, h + max(2, h*0.02),
                 f'{int(h)}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # ì„ : í‰ê·  ì ìˆ˜(ì´ì¤‘ì¶•)
    ax2 = ax1.twinx()
    line1, = ax2.plot(age_order, male_vals.values, marker='o', linewidth=2, label='ë‚¨ì')
    line2, = ax2.plot(age_order, female_vals.values, marker='o', linewidth=2, label='ì—¬ì')
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', fontsize=12)
    
    # yì¶• ë²”ìœ„(ë°ì´í„° ê¸°ë°˜)
    all_means = pd.concat([male_vals, female_vals]).dropna()
    if not all_means.empty:
        ymin = float(all_means.min()) - 0.05
        ymax = float(all_means.max()) + 0.05
        if ymin == ymax:  # ë™ì¼ê°’ ë³´í˜¸
            ymin, ymax = ymin-0.05, ymax+0.05
        ax2.set_ylim(ymin, ymax)
    
    # ì  ìœ„ ìˆ˜ì¹˜
    for x, y in zip(age_order, male_vals.values):
        if not np.isnan(y):
            ax2.text(x, y + 0.004, f'{y:.3f}', color=line1.get_color(),
                     ha='center', va='bottom', fontsize=10, fontweight='bold')
    for x, y in zip(age_order, female_vals.values):
        if not np.isnan(y):
            ax2.text(x, y + 0.004, f'{y:.3f}', color=line2.get_color(),
                     ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # ì œëª©/ê²©ì/ë²”ë¡€
    plt.title('ì—°ë ¹ëŒ€ë³„ ì‘í’ˆ ìˆ˜ ë° ì„±ë³„ í‰ê·  ì ìˆ˜ (ì£¼ì—° ë°°ìš° ê¸°ì¤€)', fontsize=14)
    ax1.set_xlabel('ì—°ë ¹ëŒ€', fontsize=12)
    ax1.grid(axis='y', linestyle='--', alpha=0.4)
    lines, labels = [bars, line1, line2], ['ì‘í’ˆ ìˆ˜', 'ë‚¨ì', 'ì—¬ì']
    ax1.legend(lines[1:], labels[1:], loc='upper left')  # ì„ ë§Œ ë²”ë¡€ë¡œ í‘œì‹œ
    
    plt.tight_layout()
    st.pyplot(fig)

    # ====== ê·¸ë˜í”„ ì•„ë˜ ìš”ì•½ & ì¸ì‚¬ì´íŠ¸ ======
    # 1) ì‘í’ˆ ìˆ˜ í†±3
    age_top = age_counts.sort_values(ascending=False)
    top_items = [f"{idx} {int(val)}í¸" for idx, val in age_top.head(3).items()]
    top_txt = " Â· ".join(top_items) if len(top_items) else "N/A"
    
    # 2) ì„±ë³„ë³„ ìµœê³ /ìµœì € í‰ê· 
    def safe_max(s):
        s = s.dropna()
        return (s.idxmax(), float(s.max())) if not s.empty else (None, np.nan)
    
    def safe_min(s):
        s = s.dropna()
        return (s.idxmin(), float(s.min())) if not s.empty else (None, np.nan)
    
    m_best_age, m_best = safe_max(male_vals)
    f_best_age, f_best = safe_max(female_vals)
    f_worst_age, f_worst = safe_min(female_vals)
    
    def fmt(v): 
        return f"{v:.3f}" if (v is not None and not np.isnan(v)) else "N/A"
    def nz(s): 
        return s if s is not None else "N/A"
    
    st.markdown(
        f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ì‘í’ˆ ìˆ˜ ìƒìœ„: **{top_txt}**  
    - ë‚¨ì„± ìµœê³  í‰ê· : **{nz(m_best_age)} {fmt(m_best)}**  
    - ì—¬ì„± ìµœê³  í‰ê· : **{nz(f_best_age)} {fmt(f_best)}**  
    - ì—¬ì„± ìµœì € í‰ê· : **{nz(f_worst_age)} {fmt(f_worst)}**
    
    **ì¸ì‚¬ì´íŠ¸(ìš”ì•½)**  
    - **ìºìŠ¤íŒ… ì§‘ì¤‘**: ë¡œë§¨ìŠ¤Â·ì²­ì¶˜Â·íŒíƒ€ì§€Â·ì§ì¥ë¬¼Â·ì„±ì¥ ì„œì‚¬ ë“± ì£¼ë¥˜ ì¥ë¥´ì˜ ì£¼ì¸ê³µ ì„¤ì •ì´ **20â€“30ëŒ€**ì— ë§ì¶°ì ¸, í•´ë‹¹ ì—°ë ¹ëŒ€ì— **ì£¼ì—° ê¸°íšŒê°€ ì§‘ì¤‘**ë©ë‹ˆë‹¤.  
    - **50ëŒ€+ ìˆ˜ëŸ‰ì´ ì ì€ ì´ìœ **: ìºë¦­í„°ê°€ ë¶€ëª¨/ìƒì‚¬/ì¡°ë ¥ìÂ·ì•…ì—­ ë“± **ì¡°ì—° ì¶•**ì— ë°°ì¹˜ë˜ê¸° ì‰¬ì›Œ ì£¼ì—° í¸ìˆ˜ê°€ ì ê³ , ì‹ ì§„ ë°°ìš° ìœ ì…ë„ ì œí•œì ì´ë¼ **ë°°ìš° í’€ ìì²´ê°€ ì‘ìŒ**.  
    - **ì„±ë³„ ê²©ì°¨ íŒ¨í„´**:  
      - ë‚¨ì„±ì€ **40â€“50ëŒ€**ì—ë„ CEO/ê²€ì‚¬/í˜•ì‚¬/ë³€í˜¸ì‚¬ ë“± **ì¤‘ì‹¬ì¶• ì—­í• **ì„ ë§¡ìœ¼ë©° í‰ê·  ì ìˆ˜ê°€ ë¹„êµì  **ì•ˆì •ì **ì…ë‹ˆë‹¤.  
      - ì—¬ì„±ì€ ì—°ë ¹ì´ ë†’ì•„ì§ˆìˆ˜ë¡ **ì¤‘ì‹¬ ì„œì‚¬ì—ì„œ ë¹„ì¤‘ì´ ì¤„ì–´**(ì—„ë§ˆÂ·ì¥ëª¨Â·í• ë¨¸ë‹ˆ ë“± ì£¼ë³€ ì¸ë¬¼), **í‰ê·  ì ìˆ˜ê°€ í•˜ë½í•˜ëŠ” ê²½í–¥**ì´ ê´€ì°°ë©ë‹ˆë‹¤.  
    - **ì‹¤ë¬´ ì‹œì‚¬ì **: ì—°ë ¹Â·ì„±ë³„ í¸ì¤‘ì„ ì™„í™”í•˜ë ¤ë©´ **ì¥ë¥´Â·ìºë¦­í„° ì„¤ê³„ ë‹¨ê³„**ì—ì„œ ë‹¤ì–‘í•œ ì—°ë ¹ëŒ€ì˜ **ì£¼ë„ì  ì—­í• **ì„ ì˜ë„ì ìœ¼ë¡œ ê¸°íší•˜ëŠ” ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """
    )



    # --- 4.4 ì›Œë“œí´ë¼ìš°ë“œ ---
    from wordcloud import WordCloud
    with tabs[3]:
        st.header("ì›Œë“œí´ë¼ìš°ë“œ")
        from collections import Counter
    
        def top_pairs(words, n=5, keyfn=lambda x: str(x).strip().lower()):
            vals = [keyfn(w) for w in words if pd.notna(w) and str(w).strip() != ""]
            return Counter(vals).most_common(n)
    
        def pairs_to_str(pairs, label_map=None):
            it = []
            for k, v in pairs:
                kk = label_map.get(k, k) if label_map else k
                it.append(f"{kk}({v:,})")
            return ", ".join(it) if it else "N/A"
    
        font_path = st.session_state.get("kfont_path")
    
        # 1) ì¥ë¥´ ì›Œë“œí´ë¼ìš°ë“œ + ì¸ì‚¬ì´íŠ¸
        if genre_list:
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                    .generate(' '.join(genre_list))
            fig, ax = plt.subplots()
            ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
            st.pyplot(fig)
    
            top_g = top_pairs(genre_list, n=5)
            st.markdown(
                f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ìƒìœ„ ì¥ë¥´: **{pairs_to_str(top_g)}**
    
    **ì¸ì‚¬ì´íŠ¸**  
    - **romance / comedy / drama** ì¤‘ì‹¬ìœ¼ë¡œ ë¬¼ëŸ‰ì´ ì ë¦¼ â†’ ëŒ€ì¤‘ì„±Â·ì œì‘ íš¨ìœ¨ì´ ë†’ì•„ **ë°˜ë³µ ìƒì‚°**ë˜ëŠ” êµ¬ì¡°.  
    - **thriller / sf / hist_war** ë“±ì€ ìƒëŒ€ì ìœ¼ë¡œ ì ì§€ë§Œ **íŒ¬ë¤ ì¶©ì„±ë„/ì™„ì„±ë„ ìŠ¹ë¶€**ë¡œ í‰ì ì´ ë†’ê²Œ í˜•ì„±ë˜ëŠ” ê²½í–¥.  
    - í˜¼í•© í‘œê¸°(ì˜ˆ: *romance thriller*)ê°€ ìì£¼ ë³´ì„ â†’ **ë©€í‹° ì¥ë¥´ íŠ¸ë Œë“œ**ê°€ í™œë°œ.
    """
            )
    
        # 2) í”Œë«í¼ ì›Œë“œí´ë¼ìš°ë“œ + ì¸ì‚¬ì´íŠ¸
        if broadcaster_list:
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                    .generate(' '.join(broadcaster_list))
            fig, ax = plt.subplots()
            ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
            st.pyplot(fig)
    
            top_p = top_pairs(broadcaster_list, n=6)
            st.markdown(
                f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ìƒìœ„ í”Œë«í¼: **{pairs_to_str(top_p)}**
    
    **ì¸ì‚¬ì´íŠ¸**  
    - **KBSÂ·MBCÂ·SBS** ë“± ì „í†µ ì±„ë„ì´ ì—¬ì „íˆ ì£¼ë¥˜, **TVNÂ·NETFLIX**ê°€ ë’¤ë¥¼ ì¶”ê²© â†’ **ë°©ì†¡ì‚¬ ì£¼ë„ + OTT ê°€ì„¸** êµ¬ë„.  
    - ê¸°íƒ€/ê³µë™ í‘œê¸°(ETC ë“±)ëŠ” **ë™ì‹œ ë°©ì˜Â·ê³µë™ ìœ í†µ**ì´ í”í•˜ë‹¤ëŠ” ì‹ í˜¸.  
    - ì „ëµ: ì „í†µ ì±„ë„ í¸ì„± í™•ë³´ì™€ í•¨ê»˜ **OTT í˜‘ì—…/ë™ì‹œ ê³µê°œ**ë¥¼ ë³‘í–‰í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ìœ í†µì´ ìœ ë¦¬.
    """
            )
    
        # 3) ë°©ì˜ìš”ì¼ ì›Œë“œí´ë¼ìš°ë“œ + ì¸ì‚¬ì´íŠ¸
        if week_list:
            # ì˜ë¬¸ ì†Œë¬¸ì í†µì¼
            wk = [str(w).strip().lower() for w in week_list if pd.notna(w)]
            wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                    .generate(' '.join(wk))
            fig, ax = plt.subplots()
            ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
            st.pyplot(fig)
    
            # ìš”ì¼ í•œê¸€ ë§¤í•‘
            day_ko = {'monday':'ì›”', 'tuesday':'í™”', 'wednesday':'ìˆ˜', 'thursday':'ëª©',
                      'friday':'ê¸ˆ', 'saturday':'í† ', 'sunday':'ì¼'}
            top_w = top_pairs(wk, n=7)
            st.markdown(
                f"""
    **ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
    - ìš”ì¼ ë¹ˆë„: **{pairs_to_str(top_w, label_map=day_ko)}**
    
    **ì¸ì‚¬ì´íŠ¸**  
    - **ì£¼ì¤‘(ì›”~ëª©)** ë¬¼ëŸ‰ì´ í¬ê³ , **í† ìš”ì¼**ì´ ê°•ì„¸ â†’ í† ìš”ì¼ì€ ì‹œì²­ ì—¬ê±´Â·ëª°ì…Â·ê´‘ê³  íš¨ê³¼ê°€ ë†’ì€ **í™©ê¸ˆ ìŠ¬ë¡¯**.  
    - **ê¸ˆÂ·ì¼ìš”ì¼**ì€ ì˜ˆëŠ¥/ê°€ë²¼ìš´ ì½˜í…ì¸ ì™€ì˜ ê²½ìŸìœ¼ë¡œ ë“œë¼ë§ˆ í¸ì„± ë¹„ì¤‘ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ í¸.  
    - ì „ëµ: ì£¼ì¤‘ì€ **ë³´í¸ ì¥ë¥´**ë¡œ ì•ˆì •ì  í¸ì„±, í† ìš”ì¼ì€ **ê³ í’ˆì§ˆÂ·í™”ì œì„±** ìŠ¹ë¶€, ê¸ˆÂ·ì¼ì€ ì¥ë¥´ ì„ íƒê³¼ ë§ˆì¼€íŒ…ì„ ì°¨ë³„í™”.
    """
            )

# --- 4.5 ì‹¤ì‹œê°„ í•„í„° ---
with tabs[4]:
    st.header("ì‹¤ì‹œê°„ í•„í„°")
    smin,smax = float(raw_df['ì ìˆ˜'].min()), float(raw_df['ì ìˆ˜'].max())
    sfilter = st.slider("ìµœì†Œ í‰ì ", smin,smax,smin)
    yfilter = st.slider("ë°©ì˜ë…„ë„ ë²”ìœ„", int(raw_df['ë°©ì˜ë…„ë„'].min()), int(raw_df['ë°©ì˜ë…„ë„'].max()), (2000,2025))
    filt = raw_df[(raw_df['ì ìˆ˜']>=sfilter) & raw_df['ë°©ì˜ë…„ë„'].between(*yfilter)]
    st.dataframe(filt.head(20))

# --- 4.6 ì „ì²´ ë¯¸ë¦¬ë³´ê¸° ---
with tabs[5]:
    st.header("ì›ë³¸ ì „ì²´ë³´ê¸°")
    st.dataframe(raw_df, use_container_width=True)

# --- 4.7 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ (Colab ì„¤ì • ê·¸ëŒ€ë¡œ) ---
# --- 4.8 GridSearch íŠœë‹ (ëª¨ë“  ëª¨ë¸) ---
with tabs[6]:
    st.header("GridSearchCV íŠœë‹")

    # split ë³´ì¥ (íŠœë‹ì„ ë¨¼ì € ë“¤ì–´ì™€ë„ ë™ì‘í•˜ë„ë¡)
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    scoring = st.selectbox("ìŠ¤ì½”ì–´ë§", ["neg_root_mean_squared_error", "r2"], index=0)
    cv = st.number_input("CV í´ë“œ ìˆ˜", 3, 10, 5, 1)

    model_zoo = {
        "KNN": ("nonsparse", KNeighborsRegressor()),
        "Linear Regression (Poly)": ("nonsparse", LinearRegression()),
        "Ridge": ("nonsparse", Ridge()),
        "Lasso": ("nonsparse", Lasso()),
        "ElasticNet": ("nonsparse", ElasticNet(max_iter=10000)),
        "SGDRegressor": ("nonsparse", SGDRegressor(max_iter=10000)),
        "SVR": ("nonsparse", SVR()),
        "Decision Tree": ("tree", DecisionTreeRegressor(random_state=SEED)),
        "Random Forest": ("tree", RandomForestRegressor(random_state=SEED)),
    }
    if 'XGBRegressor' in globals() and XGB_AVAILABLE:
        model_zoo["XGBRegressor"] = ("tree", XGBRegressor(
            random_state=SEED, objective="reg:squarederror",
            n_jobs=-1, tree_method="hist"
        ))

    def make_pipeline(kind, estimator):
        if kind == "tree":
            return Pipeline([('preprocessor', preprocessor), ('model', estimator)])
        else:
            return Pipeline([
                ('preprocessor', preprocessor),
                ('poly', PolynomialFeatures(include_bias=False)),
                ('scaler', StandardScaler(with_mean=False)),
                ('model', estimator)
            ])

    param_grids = {
        "KNN": {"poly__degree":[1,2,3], "model__n_neighbors":[3,4,5,6,7,8,9,10]},
        "Linear Regression (Poly)": {"poly__degree":[1,2,3]},
        "Ridge": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "Lasso": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "ElasticNet": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000], "model__l1_ratio":[0.1,0.5,0.9]},
        "SGDRegressor": {"poly__degree":[1,2,3], "model__learning_rate":["constant","invscaling","adaptive"]},
        "SVR": {"poly__degree":[1,2,3], "model__kernel":["poly","rbf","sigmoid"], "model__degree":[1,2,3]},
        "Decision Tree": {"model__max_depth":[10,15,20,25,30], "model__min_samples_split":[5,6,7,8,9,10], "model__min_samples_leaf":[2,3,4,5], "model__max_leaf_nodes":[None,10,20,30]},
        "Random Forest": {"model__n_estimators":[100,200,300], "model__min_samples_split":[5,6,7,8,9,10], "model__max_depth":[5,10,15,20,25,30]},
    }
    if "XGBRegressor" in model_zoo:
        param_grids["XGBRegressor"] = {
            "model__n_estimators":[200,400],
            "model__max_depth":[3,5,7],
            "model__learning_rate":[0.03,0.1,0.3],
            "model__subsample":[0.8,1.0],
            "model__colsample_bytree":[0.8,1.0],
        }

    model_name = st.selectbox("íŠœë‹í•  ëª¨ë¸ ì„ íƒ", list(model_zoo.keys()), index=0)
    kind, estimator = model_zoo[model_name]
    pipe = make_pipeline(kind, estimator)
    grid = param_grids[model_name]

    if st.button("GridSearch ì‹¤í–‰"):
        gs = GridSearchCV(pipe, grid, cv=int(cv), scoring=scoring, n_jobs=-1, refit=True, return_train_score=True)
        with st.spinner("GridSearchCV ì‹¤í–‰ ì¤‘..."):
            gs.fit(X_train, y_train)

        st.subheader("ë² ìŠ¤íŠ¸ ê²°ê³¼")
        st.json(gs.best_params_)
        if scoring == "neg_root_mean_squared_error":
            st.write(f"Best CV RMSE: {-gs.best_score_:.6f}")
        else:
            st.write(f"Best CV {scoring}: {gs.best_score_:.6f}")

        y_pred = gs.predict(X_test)
        st.write(f"Test RMSE: {rmse(y_test, y_pred):.6f}")
        st.write(f"Test RÂ²  : {r2_score(y_test, y_pred):.6f}")

        # â–¶ ëª¨ë¸ë§ íƒ­ì—ì„œ ì¦‰ì‹œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
        st.session_state["best_estimator"] = gs.best_estimator_
        st.session_state["best_params"] = gs.best_params_
        st.session_state["best_name"] = model_name
        st.session_state["best_cv_score"] = gs.best_score_
        st.session_state["best_scoring"] = scoring
        st.session_state["best_split_key"] = st.session_state.get("split_key")

        cvres = pd.DataFrame(gs.cv_results_)
        cols = ["rank_test_score","mean_test_score","std_test_score","mean_train_score","std_train_score","params"]
        st.dataframe(cvres[cols].sort_values("rank_test_score").reset_index(drop=True))

    if model_name == "XGBRegressor" and not XGB_AVAILABLE:
        st.warning("xgboostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. requirements.txtì— `xgboost`ë¥¼ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•´ ì£¼ì„¸ìš”.")

# --- 4.8 GridSearch íŠœë‹ (RandomForest, Colab ê·¸ë¦¬ë“œ) ---
# --- 4.7 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ (Colab ì„¤ì • ê·¸ëŒ€ë¡œ/ë² ìŠ¤íŠ¸ ìë™ ì ìš©) ---
with tabs[7]:
    st.header("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ (Colab ì„¤ì •)")

    # split ë³´ì¥
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    # â–¶ ë² ìŠ¤íŠ¸ ëª¨ë¸ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ RF ë² ì´ìŠ¤ë¼ì¸
    if "best_estimator" in st.session_state:
        model = st.session_state["best_estimator"]  # ì´ë¯¸ fitë¨
        st.caption(f"í˜„ì¬ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© ({st.session_state.get('best_name')})")
        if st.session_state.get("best_split_key") != st.session_state.get("split_key"):
            st.warning("ì£¼ì˜: ë² ìŠ¤íŠ¸ ëª¨ë¸ì€ ì´ì „ ë¶„í• ë¡œ í•™ìŠµë¨. ìƒˆ ë¶„í• ë¡œ ë‹¤ì‹œ íŠœë‹í•´ ì£¼ì„¸ìš”.", icon="âš ï¸")
    else:
        model = Pipeline([('preprocessor', preprocessor),
                          ('model', RandomForestRegressor(random_state=SEED))])
        model.fit(X_train, y_train)
        st.caption("í˜„ì¬ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

    # ì§€í‘œ ì¶œë ¥
    y_pred_tr = model.predict(X_train)
    y_pred_te = model.predict(X_test)
    st.metric("Train RÂ²", f"{r2_score(y_train, y_pred_tr):.3f}")
    st.metric("Test  RÂ²", f"{r2_score(y_test,  y_pred_te):.3f}")
    st.metric("Train RMSE", f"{rmse(y_train, y_pred_tr):.3f}")
    st.metric("Test  RMSE", f"{rmse(y_test,  y_pred_te):.3f}")

    if "best_params" in st.session_state:
        with st.expander("ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³´ê¸°"):
            st.json(st.session_state["best_params"])

# --- 4.9 ì˜ˆì¸¡ ì‹¤í–‰ â€” ì…ë ¥ ë¬¶ìŒ/ì¥ë¥´êµ¬ë¶„ ìƒì„± & ë² ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© ---
from sklearn.base import clone

with tabs[8]:
    st.header("í‰ì  ì˜ˆì¸¡")

    # ì„ íƒì§€ ì¤€ë¹„
    genre_opts   = sorted({g for sub in raw_df['ì¥ë¥´'].dropna().apply(clean_cell_colab) for g in sub})
    week_opts    = sorted({d for sub in raw_df['ë°©ì˜ìš”ì¼'].dropna().apply(clean_cell_colab) for d in sub})
    plat_opts    = sorted({p for sub in raw_df['í”Œë«í¼'].dropna().apply(clean_cell_colab) for p in sub})
    gender_opts  = sorted(raw_df['ì„±ë³„'].dropna().unique())
    role_opts    = sorted(raw_df['ì—­í• '].dropna().unique())
    quarter_opts = sorted(raw_df['ë°©ì˜ë¶„ê¸°'].dropna().unique())
    married_opts = sorted(raw_df['ê²°í˜¼ì—¬ë¶€'].dropna().unique())

    # ì…ë ¥ì„ ë‘ ë¬¶ìŒìœ¼ë¡œ ë°°ì¹˜
    st.subheader("1) ì…ë ¥")
    col_left, col_right = st.columns(2)

    with col_left:
        st.markdown("**â‘  ì»¨í…ì¸  íŠ¹ì„±**")
        input_age     = st.number_input("ë‚˜ì´", 10, 80, 30)
        input_gender  = st.selectbox("ì„±ë³„", gender_opts) if gender_opts else st.text_input("ì„±ë³„ ì…ë ¥", "")
        input_role    = st.selectbox("ì—­í• ", role_opts) if role_opts else st.text_input("ì—­í•  ì…ë ¥", "")
        input_married = st.selectbox("ê²°í˜¼ì—¬ë¶€", married_opts) if married_opts else st.text_input("ê²°í˜¼ì—¬ë¶€ ì…ë ¥", "")
        input_genre   = st.multiselect("ì¥ë¥´ (ë©€í‹° ì„ íƒ)", genre_opts, default=genre_opts[:1] if genre_opts else [])

        # ë‚˜ì´ â†’ ì—°ë ¹ëŒ€ ìë™ ì‚°ì¶œ + ì¥ë¥´êµ¬ë¶„ ìƒì„±
        derived_age_group = age_to_age_group(int(input_age))
        if len(input_genre) == 0:
            genre_group_label = "ì¥ë¥´ì—†ìŒ"
        elif len(input_genre) == 1:
            genre_group_label = "ë‹¨ì¼ì¥ë¥´"
        else:
            genre_group_label = "ë©€í‹°ì¥ë¥´"

        st.caption(f"ìë™ ì—°ë ¹ëŒ€: **{derived_age_group}**  |  ì¥ë¥´êµ¬ë¶„: **{genre_group_label}**")

    with col_right:
        st.markdown("**â‘¡ í¸ì„± íŠ¹ì„±**")
        input_quarter = st.selectbox("ë°©ì˜ë¶„ê¸°", quarter_opts) if quarter_opts else st.text_input("ë°©ì˜ë¶„ê¸° ì…ë ¥", "")
        input_week    = st.multiselect("ë°©ì˜ìš”ì¼ (ë©€í‹° ì„ íƒ)", week_opts, default=week_opts[:1] if week_opts else [])
        input_plat    = st.multiselect("í”Œë«í¼ (ë©€í‹° ì„ íƒ)", plat_opts, default=plat_opts[:1] if plat_opts else [])

    predict_btn = st.button("ì˜ˆì¸¡ ì‹¤í–‰")

    if predict_btn:
        # 1) ì˜ˆì¸¡ ëª¨ë¸: ë² ìŠ¤íŠ¸ ìˆìœ¼ë©´ cloneí•´ì„œ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
        if "best_estimator" in st.session_state:
            model_full = clone(st.session_state["best_estimator"])
            st.caption(f"ì˜ˆì¸¡ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ì¬í•™ìŠµ ì‚¬ìš© ({st.session_state.get('best_name')})")
        else:
            model_full = Pipeline([
                ('preprocessor', preprocessor),
                ('model', RandomForestRegressor(n_estimators=100, random_state=SEED))
            ])
            st.caption("ì˜ˆì¸¡ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

        # 2) ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
        model_full.fit(X_colab_base, y_all)

        # 3) ì‚¬ìš©ì ì…ë ¥ â†’ DF (ë©€í‹°ë¼ë²¨ì€ ë¦¬ìŠ¤íŠ¸ ìœ ì§€, ì¥ë¥´êµ¬ë¶„ ì¶”ê°€)
        user_raw = pd.DataFrame([{
            'ë‚˜ì´'    : input_age,
            'ì„±ë³„'    : input_gender,
            'ì—­í• '    : input_role,
            'ê²°í˜¼ì—¬ë¶€': input_married,
            'ë°©ì˜ë¶„ê¸°': input_quarter,
            'ì—°ë ¹ëŒ€'  : derived_age_group,   # ìë™ ë§¤í•‘
            'ì¥ë¥´'    : input_genre,         # list
            'ë°©ì˜ìš”ì¼' : input_week,          # list
            'í”Œë«í¼'  : input_plat,          # list
            'ì¥ë¥´êµ¬ë¶„' : genre_group_label,   # ìƒˆ íŒŒìƒ ë³€ìˆ˜(í˜„ì¬ ëª¨ë¸ì—ëŠ” ë¯¸ì‚¬ìš©)
        }])

        # 4) ë©€í‹°ë¼ë²¨ ë³€í™˜ + X ìŠ¤í‚¤ë§ˆ ì •ë ¬
        user_mlb = colab_multilabel_transform(user_raw, cols=('ì¥ë¥´','ë°©ì˜ìš”ì¼','í”Œë«í¼'))

        # í•™ìŠµ X ìŠ¤í‚¤ë§ˆì™€ ì»¬ëŸ¼ ì •í•© (ì—¬ë¶„ì€ ì œê±°, ë¶€ì¡±ë¶„ì€ 0ìœ¼ë¡œ ì±„ì›€)
        user_base = pd.concat([X_colab_base.iloc[:0].copy(), user_mlb], ignore_index=True)
        # ë“œë¡­ ëŒ€ìƒ ì œê±°(í›ˆë ¨ ì‹œ ì œì™¸í–ˆë˜ ì»¬ëŸ¼ë“¤)
        user_base = user_base.drop(columns=[c for c in drop_cols if c in user_base.columns], errors='ignore')
        # í›ˆë ¨ì— ì—†ëŠ” ì»¬ëŸ¼ì€ ì‚­ì œ, ìˆëŠ”ë° ë¹ ì§„ ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€
        for c in X_colab_base.columns:
            if c not in user_base.columns:
                user_base[c] = 0
        user_base = user_base[X_colab_base.columns].tail(1)

        # 5) ì˜ˆì¸¡
        pred = model_full.predict(user_base)[0]
        st.success(f"ğŸ’¡ ì˜ˆìƒ í‰ì : {pred:.2f}")

        # ì°¸ê³ : ì¥ë¥´êµ¬ë¶„ì„ ì‹¤ì œ íŠ¹ì§•ìœ¼ë¡œ ì“°ê³  ì‹¶ë‹¤ë©´,
        #  - í•™ìŠµ ë°ì´í„°(df_mlb)ì—ë„ ë™ì¼ ê·œì¹™ìœ¼ë¡œ 'ì¥ë¥´êµ¬ë¶„'ì„ ë§Œë“¤ì–´ X_colab_baseì— í¬í•¨ì‹œí‚¤ê³ 
        #  - preprocessorì˜ ë²”ì£¼í˜• ëª©ë¡ì— 'ì¥ë¥´êµ¬ë¶„'ì„ ì¶”ê°€í•˜ì„¸ìš”.

