# =========================================
# app.py  â€”  K-ë“œë¼ë§ˆ ì¼€ë¯¸ìŠ¤ì½”ì–´ (Sparrow UI ìŠ¤í‚¨ + ì‚¬ì´ë“œ ë„¤ë¹„, Safe-Mode/403 íšŒí”¼ ê°€ë“œ í¬í•¨)
# =========================================

# ---- page config MUST be first ----
import streamlit as st
st.set_page_config(page_title="ì¼€ë¯¸ìŠ¤ì½”ì–´ | K-ë“œë¼ë§ˆ ë¶„ì„/ì˜ˆì¸¡", page_icon="ğŸ¬", layout="wide")

# =========================
# âš™ï¸ Safe Mode & Limits
# =========================
import os, time
SAFE_MODE = os.getenv("CHEMI_SAFE_MODE", "1") not in ("0", "false", "False")
SAFE_MAX_ROWS            = int(os.getenv("CHEMI_MAX_ROWS", "5000"))     # íŠœë‹ì—ì„œ ì‚¬ìš©í•  ìµœëŒ€ í–‰ ìˆ˜(ìƒ˜í”Œ)
SAFE_MAX_GSCV_EVALS      = int(os.getenv("CHEMI_MAX_EVALS", "120"))     # (ê·¸ë¦¬ë“œ ì¡°í•©ìˆ˜ Ã— CV í´ë“œìˆ˜)
SAFE_MAX_PRUNED_ALPHAS   = int(os.getenv("CHEMI_MAX_PRUNED", "25"))     # Pruned íŠ¸ë¦¬ í›„ë³´ ìƒí•œ
SAFE_GS_COOLDOWN_SEC     = int(os.getenv("CHEMI_GS_COOLDOWN", "15"))    # GridSearch ì—°íƒ€ ë°©ì§€
SAFE_PRED_COOLDOWN_SEC   = int(os.getenv("CHEMI_PRED_COOLDOWN", "5"))   # ì˜ˆì¸¡ ì—°íƒ€ ë°©ì§€

# ---- dependency guard (optional) ----
import importlib.util
_missing = [m for m in ("numpy","scipy","sklearn","joblib","threadpoolctl","xgboost") if importlib.util.find_spec(m) is None]
if _missing:
    st.error(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {_missing}. requirements.txt / runtime.txt ë²„ì „ì„ ê³ ì •í•´ ì¬ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

# ---- imports ----
import ast, random, re, platform
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
from sklearn.metrics import mean_squared_error, r2_score
import plotly.express as px

from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, PolynomialFeatures, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.base import clone

# XGBê°€ ì„¤ì¹˜ë¼ ìˆìœ¼ë©´ ì“°ë„ë¡ ì•ˆì „í•˜ê²Œ ì¶”ê°€
try:
    from xgboost import XGBRegressor
    XGB_AVAILABLE = True
except Exception:
    XGB_AVAILABLE = False

def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

# ===== Global seed =====
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED)

# ===== Matplotlib (í•œê¸€ í°íŠ¸) =====
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

if st.session_state.get("font_cache_cleared") is not True:
    import shutil
    shutil.rmtree(matplotlib.get_cachedir(), ignore_errors=True)
    st.session_state["font_cache_cleared"] = True

def ensure_korean_font():
    matplotlib.rcParams['axes.unicode_minus'] = False
    base = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()
    candidates = [
        base / "fonts" / "NanumGothic-Regular.ttf",
        base / "fonts" / "NanumGothic.ttf",
    ]
    if platform.system() == "Windows":
        candidates += [Path(r"C:\Windows\Fonts\malgun.ttf"), Path(r"C:\Windows\Fonts\malgunbd.ttf")]
    wanted = ("nanum","malgun","applegothic","notosanscjk","sourcehan","gulim","dotum","batang",
              "pretendard","gowun","spoqa")
    for f in fm.findSystemFonts(fontext="ttf"):
        if any(k in os.path.basename(f).lower() for k in wanted):
            candidates.append(Path(f))
    for p in candidates:
        try:
            if p.exists():
                fm.fontManager.addfont(str(p))
                family = fm.FontProperties(fname=str(p)).get_name()
                matplotlib.rcParams['font.family'] = family
                st.session_state["kfont_path"] = str(p)  # WordCloudìš©
                return family
        except Exception:
            continue
    st.session_state["kfont_path"] = None
    return None

_ = ensure_korean_font()

# ===== Safe-run helpers (ì¿¨ë‹¤ìš´ & 403 í•¸ë“¤ë§) =====
def _cooldown_ok(key: str, cooldown: int) -> bool:
    now = time.time()
    last = st.session_state.get(key)
    if last is None or (now - last) >= cooldown:
        st.session_state[key] = now
        return True
    return False

def run_safely(fn, *args, **kwargs):
    """403(Fair-use) ë“± ì¹˜ëª… ì˜¤ë¥˜ë¥¼ ì¡ì•„ UIì—ì„œ ì•ˆë‚´í•˜ê³  ê°™ì€ ëŸ°ì—ì„œ ì¶”ê°€ ì‹¤í–‰ì„ ë©ˆì¶¤"""
    try:
        return fn(*args, **kwargs)
    except Exception as e:
        msg = str(e).lower()
        if "403" in msg or "fair-use" in msg or "blocked" in msg:
            st.error("í˜¸ìŠ¤íŒ… í™˜ê²½ì˜ Fair-use ì œí•œ(403)ìœ¼ë¡œ ìš”ì²­ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. "
                     "íŠœë‹ ê·¸ë¦¬ë“œ/í´ë“œ/í›„ë³´ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ Safe Modeë¥¼ ìœ ì§€í•´ ì£¼ì„¸ìš”.")
            st.stop()
        else:
            st.exception(e)
            st.stop()

def count_param_evals(grid: Dict[str, List[Any]]) -> int:
    total = 1
    for k, v in grid.items():
        total *= max(1, len(v))
    return total

# ====== Sparrow UI CSS ======
def _inject_sparrow_css():
    st.markdown("""
    <style>
      /* ---------- Layout / Typography ---------- */
      .block-container{padding-top:5rem; padding-bottom:5rem;}
      h1,h2,h3{font-weight:800;}
      h3, .stPlotlyChart { margin-top: 1rem !important; }
      /* ---------- Sidebar ---------- */
      section[data-testid="stSidebar"]{
        width:220px !important; min-width:220px;
        background:#0b1220; color:#e5e7eb; border-right:1px solid #070c16;
      }
      .sb-wrap{display:flex; flex-direction:column; height:100%;}
      .sb-brand{display:flex; align-items:center; gap:10px; padding:14px 12px 10px;}
      .sb-brand .logo{font-size:20px}
      .sb-brand .name{font-size:16px; font-weight:800; letter-spacing:.2px}
      .sb-menu{padding:6px 8px 8px; display:flex; flex-direction:column;}
      .sb-nav{margin:2px 0;}
      .sb-nav .stButton>button{
        width:100% !important; display:flex; align-items:center; gap:10px; justify-content:flex-start;
        background:transparent !important; color:#e5e7eb !important;
        border:1px solid #162033 !important; border-radius:10px !important;
        padding:8px 10px !important; font-size:14px !important; box-shadow:none !important; opacity:1 !important;
      }
      .sb-nav .stButton>button:hover{ background:#111a2b !important; border-color:#25324a !important; }
      .sb-nav.active .stButton>button{ background:#2563eb !important; border-color:#2563eb !important; color:#ffffff !important; }
      .sb-card{background:#0f172a; border:1px solid #1f2937; border-radius:12px; padding:10px; margin-top:8px;}
      .sb-card h4{margin:0 0 6px 0; font-size:12px; color:#cbd5e1; font-weight:800;}
      .sb-footer{margin-top:auto; padding:10px 12px; font-size:11px; color:#9ca3af; border-top:1px solid #070c16;}
      /* ---------- Cards ---------- */
      .kpi-row{display:grid; grid-template-columns:repeat(4,1fr); gap:12px; margin:8px 0 6px;}
      .kpi{ background:#ffffff; border:1px solid #e5e7eb; border-radius:12px; padding:12px 14px;
            box-shadow:0 6px 18px rgba(17,24,39,.04); }
      .kpi h6{margin:0 0 4px; font-size:12px; color:#6b7280; font-weight:800;}
      .kpi .v{font-size:22px; font-weight:800; line-height:1;}
      .kpi .d{font-size:12px; color:#10b981; font-weight:700;}
      div[data-testid="stPlotlyChart"], div.stPlot {margin-top:8px;}
      .block-container{padding-top:1.4rem; padding-bottom:2.2rem;}
      .kpi-row{ margin-bottom: 18px; }
      div[data-testid="stPlotlyChart"]{ margin-top:8px; }
    </style>
    """, unsafe_allow_html=True)

_inject_sparrow_css()

# ===== Data helpers =====
def clean_cell_colab(x):
    if isinstance(x, list):
        return [str(i).strip() for i in x if pd.notna(i)]
    if pd.isna(x):
        return []
    if isinstance(x, str):
        try:
            parsed = ast.literal_eval(x)
            if isinstance(parsed, list):
                return [str(i).strip() for i in parsed if pd.notna(i)]
            return [x.strip()]
        except Exception:
            return [x.strip()]
    return [str(x).strip()]

@st.cache_data(show_spinner=False)
def load_data():
    raw = pd.read_json('drama_d.json')
    if isinstance(raw, pd.Series):
        raw = pd.DataFrame({c: pd.Series(v) for c, v in raw.to_dict().items()})
    else:
        raw = pd.DataFrame({c: pd.Series(v) for c, v in raw.to_dict().items()})
    # Safe mode: ë„ˆë¬´ í° ë°ì´í„°ë©´ ìƒ˜í”Œë§
    if SAFE_MODE and len(raw) > SAFE_MAX_ROWS:
        raw = raw.sample(SAFE_MAX_ROWS, random_state=SEED).reset_index(drop=True)
    return raw

raw_df = load_data()

# ===== Multi-label encoding base =====
def colab_multilabel_fit_transform(df: pd.DataFrame, cols=('genres','day','network')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)
        mlb = MultiLabelBinarizer()
        arr = mlb.fit_transform(out[col])
        new_cols = [f"{col}_{c.strip().upper()}" for c in mlb.classes_]
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)
        st.session_state[f"mlb_{col}"] = mlb
        st.session_state[f"mlb_classes_{col}"] = mlb.classes_.tolist()
    return out

def colab_multilabel_transform(df: pd.DataFrame, cols=('genres','day','network')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)
        mlb = st.session_state.get(f"mlb_{col}", None)
        if mlb is None:
            classes = st.session_state.get(f"mlb_classes_{col}", [])
            mlb = MultiLabelBinarizer()
            if classes:
                mlb.classes_ = np.array(classes)
            else:
                try:
                    prefix = f"{col}_"
                    labels = [c[len(prefix):] for c in df_mlb.columns if c.startswith(prefix)]
                    if labels:
                        mlb.classes_ = np.array(labels)
                    else:
                        mlb.fit(out[col])
                except Exception:
                    mlb.fit(out[col])
        arr = mlb.transform(out[col])
        new_cols = [f"{col}_{c.strip().upper()}" for c in mlb.classes_]
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)
    return out

df_mlb = colab_multilabel_fit_transform(raw_df, cols=('genres','day','network'))

# ===== Feature base =====
drop_cols = [c for c in ['ë°°ìš°ëª…','ë“œë¼ë§ˆëª…','genres','day','network','score','start airing'] if c in df_mlb.columns]
if 'score' in df_mlb.columns:
    df_mlb['score'] = pd.to_numeric(df_mlb['score'], errors='coerce')

X_colab_base = df_mlb.drop(columns=drop_cols, errors='ignore')
y_all = df_mlb['score']

categorical_features = [c for c in ['role','gender','air_q','married','age_group'] if c in X_colab_base.columns]
try:
    ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)
except TypeError:
    ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False)

preprocessor = ColumnTransformer(
    transformers=[('cat',
                   Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='(missing)')),
                                   ('ohe', ohe)]),
                   categorical_features)],
    remainder='passthrough'
)

# ===== EDA lists =====
genre_list = [g for sub in raw_df.get('genres', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for g in sub]
broadcaster_list = [b for sub in raw_df.get('network', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for b in sub]
week_list = [w for sub in raw_df.get('day', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for w in sub]
unique_genres = sorted(set(genre_list))

# ===== Age-group helper =====
def age_to_age_group(age: int) -> str:
    s = raw_df.get('age_group')
    if s is None or s.dropna().empty:
        if age < 20: return "10ëŒ€"
        if age < 30: return "20ëŒ€"
        if age < 40: return "30ëŒ€"
        if age < 50: return "40ëŒ€"
        if age < 60: return "50ëŒ€"
        return "60ëŒ€ ì´ìƒ"
    series = s.dropna().astype(str)
    vocab = series.unique().tolist()
    counts = series.value_counts()
    decade = (int(age)//10)*10
    exact = [g for g in vocab if re.search(rf"{decade}\s*ëŒ€", g)]
    if exact: return counts[exact].idxmax()
    loose = [g for g in vocab if str(decade) in g]
    if loose: return counts[loose].idxmax()
    if decade >= 60:
        over = [g for g in vocab if ('60' in g) or ('ì´ìƒ' in g)]
        if over: return counts[over].idxmax()
    with_num = []
    for g in vocab:
        m = re.search(r'(\d+)', g)
        if m: with_num.append((g, int(m.group(1))))
    if with_num:
        nearest = min(with_num, key=lambda t: abs(t[1]-decade))[1]
        candidates = [g for g,n in with_num if n==nearest]
        return counts[candidates].idxmax()
    return counts.idxmax()

# =============================
# ë„¤ë¹„ê²Œì´ì…˜ ì •ì˜ & ì¿¼ë¦¬íŒŒëŒ ë™ê¸°í™”
# =============================
def _get_nav_from_query():
    if hasattr(st, "query_params"):  # Streamlit 1.30+
        val = st.query_params.get("nav", None)
        return val[0] if isinstance(val, list) else val
    else:
        qp = st.experimental_get_query_params()
        val = qp.get("nav", [None])
        return val[0]

def _set_nav_query(slug: str):
    if hasattr(st, "query_params"):
        st.query_params["nav"] = slug
    else:
        st.experimental_set_query_params(nav=slug)

# ---------- ê° í˜ì´ì§€ ----------
def page_overview():
    total_titles = int(raw_df['ë“œë¼ë§ˆëª…'].nunique()) if 'ë“œë¼ë§ˆëª…' in raw_df.columns else int(raw_df.shape[0])
    total_actors = int(raw_df['ë°°ìš°ëª…'].nunique()) if 'ë°°ìš°ëª…' in raw_df.columns else \
                   (int(raw_df['actor'].nunique()) if 'actor' in raw_df.columns else int(raw_df.shape[0]))
    avg_score = float(pd.to_numeric(raw_df['score'], errors='coerce').mean())

    st.markdown(f"""
    <div class="kpi-row">
      <div class="kpi"><h6>TOTAL TITLES</h6><div class="v">{total_titles}</div><div class="d">ì „ì²´ ì‘í’ˆ</div></div>
      <div class="kpi"><h6>TOTAL ACTORS</h6><div class="v">{total_actors}</div><div class="d">ëª…</div></div>
      <div class="kpi"><h6>AVG CHEMI SCORE</h6><div class="v">{0.0 if np.isnan(avg_score) else round(avg_score,2):.2f}</div><div class="d">ì „ì²´ í‰ê· </div></div>
      <div class="kpi"><h6>GENRES</h6><div class="v">{len(unique_genres)}</div><div class="d">ìœ ë‹ˆí¬</div></div>
    </div>
    """, unsafe_allow_html=True)

    st.subheader("ì—°ë„ë³„ í‰ê·  ì¼€ë¯¸ìŠ¤ì½”ì–´")
    df_year = raw_df.copy()
    df_year['start airing'] = pd.to_numeric(df_year['start airing'], errors='coerce')
    df_year['score'] = pd.to_numeric(df_year['score'], errors='coerce')
    df_year = df_year.dropna(subset=['start airing','score'])
    fig = px.line(df_year.groupby('start airing')['score'].mean().reset_index(),
                  x='start airing', y='score', markers=True)
    st.plotly_chart(fig, use_container_width=True)

    c1, c2 = st.columns([1, 1])
    with c1:
        st.subheader("ìµœê·¼ ì—°ë„ ìƒìœ„ ì‘í’ˆ (ì¤‘ë³µ ì œê±°)")
        _df = raw_df.copy()
        _df['start airing'] = pd.to_numeric(_df['start airing'], errors='coerce')
        _df['score'] = pd.to_numeric(_df['score'], errors='coerce')
        _df = _df.dropna(subset=['start airing', 'score'])
        if not _df.empty:
            last_year = int(_df['start airing'].max())
            recent = _df[_df['start airing'].between(last_year-1, last_year)]
            name_col = 'ë“œë¼ë§ˆëª…' if 'ë“œë¼ë§ˆëª…' in recent.columns else ('title' if 'title' in recent.columns else recent.columns[0])
            recent_unique = (recent.sort_values('score', ascending=False)
                                   .drop_duplicates(subset=[name_col], keep='first'))
            top_recent = recent_unique.sort_values('score', ascending=False).head(10)
            fig_recent = px.bar(top_recent, x=name_col, y='score', text='score')
            fig_recent.update_traces(texttemplate='%{text:.2f}', textposition='outside')
            fig_recent.update_layout(height=320, margin=dict(l=10, r=10, t=20, b=40))
            st.plotly_chart(fig_recent, use_container_width=True)
        else:
            st.info("ìµœê·¼ ì—°ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    with c2:
        st.subheader("í”Œë«í¼ë³„ ì‘í’ˆ ìˆ˜ (TOP 10)")
        p_cnt = (raw_df.assign(network=raw_df["network"].apply(clean_cell_colab))
                        .explode("network")
                        .dropna(subset=["network"])
                        .groupby("network").size()
                        .reset_index(name="count"))
        p_cnt = p_cnt.loc[:, ~p_cnt.columns.duplicated()].copy()
        fig_p = px.bar(p_cnt, x='network', y='count')
        fig_p.update_layout(height=320, margin=dict(l=10, r=10, t=20, b=40))
        st.plotly_chart(fig_p, use_container_width=True)

def page_basic():
    st.header("ê¸°ì´ˆ í†µê³„: score")
    st.write(pd.to_numeric(raw_df['score'], errors='coerce').describe())
    fig,ax=plt.subplots(figsize=(6,3))
    ax.hist(pd.to_numeric(raw_df['score'], errors='coerce'), bins=20)
    ax.set_title("ì „ì²´ í‰ì  ë¶„í¬")
    st.pyplot(fig)

def page_dist():
    st.header("ë¶„í¬ ë° êµì°¨ë¶„ì„")
    st.subheader("ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    ct = (
        pd.DataFrame({'start airing': raw_df['start airing'], 'network': raw_df['network'].apply(clean_cell_colab)})
        .explode('network').groupby(['start airing','network']).size().reset_index(name='count')
    )
    ct['NETWORK_UP'] = ct['network'].astype(str).str.upper()
    focus = ['KBS','MBC','TVN','NETFLIX','SBS']
    fig3 = px.line(ct[ct['NETWORK_UP'].isin(focus)], x='start airing', y='count', color='network',
                   log_y=True, title="ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    st.plotly_chart(fig3, use_container_width=True)

    p = (ct.pivot_table(index='start airing', columns='NETWORK_UP', values='count', aggfunc='sum')
           .fillna(0).astype(int))
    years = sorted(p.index)
    insights = []
    if 'NETFLIX' in p.columns:
        s = p['NETFLIX']; nz = s[s > 0]
        if not nz.empty:
            first_year = int(nz.index.min())
            max_year, max_val = int(s.idxmax()), int(s.max())
            insights.append(f"- **ë„·í”Œë¦­ìŠ¤(OTT)ì˜ ê¸‰ì„±ì¥**: {first_year}ë…„ ì´í›„ ì¦ê°€, **{max_year}ë…„ {max_val}í¸** ìµœê³ ì¹˜.")
    down_ter = []
    for b in ['KBS','MBC','SBS']:
        if b in p.columns and len(years) >= 2:
            slope = np.polyfit(years, p[b].reindex(years, fill_value=0), 1)[0]
            if slope < 0: down_ter.append(b)
    if down_ter:
        insights.append(f"- **ì§€ìƒíŒŒ ê°ì†Œ ì¶”ì„¸**: {' / '.join(down_ter)} ì „ë°˜ì  í•˜ë½.")
    st.markdown("**ì¸ì‚¬ì´íŠ¸**\n" + "\n".join(insights))

    st.subheader("ì¥ë¥´ ê°œìˆ˜ë³„ í‰ê·  í‰ì  (ë°°ìš° ë‹¨ìœ„)")
    actor_col = 'ë°°ìš°ëª…' if 'ë°°ìš°ëª…' in raw_df.columns else ('actor' if 'actor' in raw_df.columns else None)
    if actor_col is None:
        st.info("ë°°ìš° ì‹ë³„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´(ë°°ìš°ëª…/actor) ì´ ì„¹ì…˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    gdf = (pd.DataFrame({actor_col: raw_df[actor_col], 'genres': raw_df['genres'].apply(clean_cell_colab)})
              .explode('genres').dropna(subset=[actor_col,'genres']))
    genre_cnt = gdf.groupby(actor_col)['genres'].nunique().rename('ì¥ë¥´ê°œìˆ˜')
    actor_mean = (raw_df.groupby(actor_col, as_index=False)['score']
                  .mean().rename(columns={'score':'ë°°ìš°í‰ê· ì ìˆ˜'}))
    df_actor = actor_mean.merge(genre_cnt.reset_index(), on=actor_col, how='left')
    df_actor['ì¥ë¥´ê°œìˆ˜'] = df_actor['ì¥ë¥´ê°œìˆ˜'].fillna(0).astype(int)
    df_actor = df_actor[df_actor['ì¥ë¥´ê°œìˆ˜'] > 0].copy()
    def bucket(n: int) -> str:
        if n <= 2:  return '1~2ê°œ'
        if n <= 4:  return '3~4ê°œ'
        if n <= 6:  return '5~6ê°œ'
        return '7ê°œ ì´ìƒ'
    df_actor['ì¥ë¥´ê°œìˆ˜êµ¬ê°„'] = pd.Categorical(
        df_actor['ì¥ë¥´ê°œìˆ˜'].apply(bucket),
        categories=['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ'],
        ordered=True
    )
    fig_box = px.box(
        df_actor, x='ì¥ë¥´ê°œìˆ˜êµ¬ê°„', y='ë°°ìš°í‰ê· ì ìˆ˜',
        category_orders={'ì¥ë¥´ê°œìˆ˜êµ¬ê°„': ['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ']},
        title="ì¥ë¥´ ê°œìˆ˜ë³„ ë°°ìš° í‰ê·  ì ìˆ˜ ë¶„í¬"
    )
    st.plotly_chart(fig_box, use_container_width=True)

def page_filter():
    st.header("ì‹¤ì‹œê°„ í•„í„°")
    smin = float(pd.to_numeric(raw_df['score'], errors='coerce').min())
    smax = float(pd.to_numeric(raw_df['score'], errors='coerce').max())
    sfilter = st.slider("ìµœì†Œ í‰ì ", smin, smax, smin)
    y_min = int(pd.to_numeric(raw_df['start airing'], errors='coerce').min())
    y_max = int(pd.to_numeric(raw_df['start airing'], errors='coerce').max())
    yfilter = st.slider("ë°©ì˜ë…„ë„ ë²”ìœ„", y_min, y_max, (y_min, y_max))
    filt = raw_df[(pd.to_numeric(raw_df['score'], errors='coerce')>=sfilter) &
                  pd.to_numeric(raw_df['start airing'], errors='coerce').between(*yfilter)]
    st.dataframe(filt.head(20), use_container_width=True)

def page_all():
    st.header("ì›ë³¸ ì „ì²´ë³´ê¸°")
    st.dataframe(raw_df, use_container_width=True)

def make_pipeline(model_name, kind, estimator):
    if kind == "tree":
        return Pipeline([('preprocessor', preprocessor), ('model', estimator)])
    if model_name == "SVR":
        return Pipeline([('preprocessor', preprocessor), ('scaler', StandardScaler()), ('model', estimator)])
    if model_name == "KNN":
        return Pipeline([('preprocessor', preprocessor), ('poly', PolynomialFeatures(include_bias=False)),
                         ('scaler', StandardScaler()), ('knn', estimator)])
    if model_name == "Linear Regression (Poly)":
        return Pipeline([('preprocessor', preprocessor), ('poly', PolynomialFeatures(include_bias=False)),
                         ('scaler', StandardScaler()), ('linreg', estimator)])
    return Pipeline([('preprocessor', preprocessor), ('poly', PolynomialFeatures(include_bias=False)),
                     ('scaler', StandardScaler()), ('model', estimator)])

def page_tuning():
    st.header("GridSearchCV íŠœë‹")
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(0.2):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=0.2, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(0.2)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    # Safe Mode ì•ˆë‚´
    if SAFE_MODE:
        st.info("ğŸ›Ÿ Safe Mode: ë°ì´í„°ê°€ í¬ë©´ ìë™ ìƒ˜í”Œë§í•˜ê³ , ê³¼ë„í•œ GridSearchë¥¼ ì°¨ë‹¨í•©ë‹ˆë‹¤. "
                "í™˜ê²½ë³€ìˆ˜ CHEMI_SAFE_MODE=0 ìœ¼ë¡œ í•´ì œí•  ìˆ˜ ìˆì–´ìš”.")

    scoring = st.selectbox("ìŠ¤ì½”ì–´ë§", ["neg_root_mean_squared_error", "r2"], index=0)
    cv = st.number_input("CV í´ë“œ ìˆ˜", 3, 5 if SAFE_MODE else 10, 5 if SAFE_MODE else 5, 1)
    cv_shuffle = st.checkbox("CV ì…”í”Œ(shuffle)", value=False)

    # --- íŒŒë¼ë¯¸í„° ì„ íƒê¸° ---
    def render_param_selector(label, options):
        display_options, to_py = [], {}
        for v in options:
            if v is None: s="(None)"; to_py[s]=None
            else:
                s = str(int(v)) if isinstance(v, float) and v.is_integer() else str(v)
                to_py[s] = v
            display_options.append(s)
        sel = st.multiselect(f"{label}", display_options, default=display_options, key=f"sel_{label}")
        extra = st.text_input(f"{label} ì¶”ê°€ê°’(ì½¤ë§ˆ, ì˜ˆ: 50,75,100 ë˜ëŠ” None)", value="", key=f"extra_{label}")
        chosen = [to_py[s] for s in sel]
        if extra.strip():
            for tok in extra.split(","):
                t = tok.strip()
                if not t: continue
                if t.lower()=="none": val=None
                else:
                    try: val=int(t)
                    except:
                        try: val=float(t)
                        except: val=t
                chosen.append(val)
        uniq=[];  [uniq.append(v) for v in chosen if v not in uniq]
        return uniq

    # --- ëª¨ë¸ ëª©ë¡ ---
    model_zoo = {
        "KNN": ("nonsparse", KNeighborsRegressor()),
        "Linear Regression (Poly)": ("nonsparse", LinearRegression()),
        "Ridge": ("nonsparse", Ridge()),
        "Lasso": ("nonsparse", Lasso()),
        "ElasticNet": ("nonsparse", ElasticNet(max_iter=10000)),
        "SGDRegressor": ("nonsparse", SGDRegressor(max_iter=10000, random_state=SEED)),
        "SVR": ("nonsparse", SVR()),
        "Decision Tree": ("tree", DecisionTreeRegressor(random_state=SEED)),
        "Decision Tree (Pruned)": ("tree", DecisionTreeRegressor(random_state=SEED)),
        "Random Forest": ("tree", RandomForestRegressor(random_state=SEED)),
    }
    if XGB_AVAILABLE:
        model_zoo["XGBRegressor"] = ("tree", XGBRegressor(
            random_state=SEED, objective="reg:squarederror", n_jobs=1, tree_method="hist"
        ))

    # --- ê¸°ë³¸ ê·¸ë¦¬ë“œ ---
    default_param_grids = {
        "KNN": {"poly__degree":[2,3], "knn__n_neighbors":[3,4,5]},
        "Linear Regression (Poly)": {"poly__degree":[1,2,3]},
        "Ridge": {"poly__degree":[2,3], "model__alpha":[0.1,1,10,100,1000]},
        "Lasso": {"poly__degree":[2,3], "model__alpha":[0.001,0.01,0.1,1]},
        "ElasticNet": {"poly__degree":[2,3], "model__alpha":[0.001,0.01,0.1,1], "model__l1_ratio":[0.1,0.5,0.9]},
        "SGDRegressor": {"poly__degree":[1,2,3], "model__learning_rate":["constant","invscaling","adaptive"]},
        "SVR": {"model__kernel":["poly","rbf","sigmoid"], "model__degree":[1,2,3]},
        "Decision Tree": {
            "model__max_depth":[10,15,20],
            "model__min_samples_split":[5,6,7,8],
            "model__min_samples_leaf":[2,3],
            "model__max_leaf_nodes":[None,10,20],
        },
        "Decision Tree (Pruned)": {
            "model__ccp_alpha": [0.0, 3.146231327807963e-05, 7.543988269811632e-05]
        },
        "Random Forest": {
            "model__n_estimators":[200,300],
            "model__min_samples_split":[5,6,7,8],
            "model__max_depth":[10,15,20,25],
        },
    }
    if "XGBRegressor" in model_zoo:
        default_param_grids["XGBRegressor"] = {
            "model__n_estimators":[100, 200],
            "model__max_depth":[1, 5, 10],
            "model__learning_rate":[0.1,0.2,0.3],
            "model__colsample_bytree":[0.8,1.0],
        }

    model_name = st.selectbox("íŠœë‹í•  ëª¨ë¸ ì„ íƒ", list(model_zoo.keys()), index=0)
    kind, estimator = model_zoo[model_name]
    pipe = make_pipeline(model_name, kind, estimator)

    st.markdown("**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ**")
    base_grid = dict(default_param_grids.get(model_name, {}))

    # Pruned ì•ŒíŒŒ í›„ë³´ ìë™ ìƒì„± + ìƒí•œ
    if model_name == "Decision Tree (Pruned)":
        X_train_transformed = preprocessor.fit_transform(X_train, y_train)
        tmp_tree = DecisionTreeRegressor(random_state=SEED)
        path = tmp_tree.cost_complexity_pruning_path(X_train_transformed, y_train)
        ccp_alphas = np.array(path.ccp_alphas, dtype=float)
        ccp_alphas = ccp_alphas[ccp_alphas >= 0.0]
        if ccp_alphas.size > 0:
            ccp_alphas = np.unique(ccp_alphas)
        if ccp_alphas.size > 1:
            ccp_alphas = ccp_alphas[:-1]
        must_include = np.array([3.146231327807963e-05, 7.543988269811632e-05], dtype=float)
        ccp_candidates = np.unique(np.concatenate([ccp_alphas, must_include]))
        # ìƒí•œ
        if SAFE_MODE and len(ccp_candidates) > SAFE_MAX_PRUNED_ALPHAS:
            idx = np.linspace(0, len(ccp_candidates)-1, SAFE_MAX_PRUNED_ALPHAS).astype(int)
            ccp_candidates = ccp_candidates[idx]
        base_grid["model__ccp_alpha"] = list(ccp_candidates.tolist())
        st.caption(f"ccp_alpha í›„ë³´: {len(base_grid['model__ccp_alpha'])}ê°œ")

    # íŒŒë¼ë¯¸í„° ì„ íƒ UI
    def _render(label, options):
        # Safe modeì¼ ë• ì˜µì…˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©(ë¶ˆí•„ìš”í•œ í­ì¦ ë°©ì§€)
        return options if SAFE_MODE else render_param_selector(label, options)
    user_grid = {k: _render(k, v) for k, v in base_grid.items()}

    with st.expander("ì„ íƒí•œ íŒŒë¼ë¯¸í„° í™•ì¸"):
        st.write(user_grid)

    # =========================
    # ì‹¤í–‰ ë²„íŠ¼ (+ì¿¨ë‹¤ìš´/ì•ˆì „ì‹¤í–‰/ê³¼ë‹¤íƒìƒ‰ ì°¨ë‹¨)
    # =========================
    if st.button("GridSearch ì‹¤í–‰", key="btn_gs"):
        if not _cooldown_ok("last_gs_time", SAFE_GS_COOLDOWN_SEC):
            st.warning("ì ê¹ë§Œìš”! ì—°ì† ì‹¤í–‰ì„ ì ì‹œ ì œí•œ ì¤‘ì´ì—ìš”. ëª‡ ì´ˆ í›„ ë‹¤ì‹œ ëˆŒëŸ¬ ì£¼ì„¸ìš”.")
            st.stop()

        if model_name != "Decision Tree (Pruned)":
            # ê³¼ë‹¤ íƒìƒ‰ ë°©ì§€
            combos = count_param_evals(user_grid)
            total_evals = combos * int(cv)
            if SAFE_MODE and total_evals > SAFE_MAX_GSCV_EVALS:
                st.error(f"íƒìƒ‰ëŸ‰ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: ì¡°í•© {combos} Ã— CV {int(cv)} = {total_evals} > {SAFE_MAX_GSCV_EVALS}\n"
                         f"â†’ ì˜µì…˜/í´ë“œ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ Safe Mode í•´ì œ(CHEMI_SAFE_MODE=0) í›„ ì†Œê·œëª¨ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.")
                st.stop()

            cv_obj = KFold(n_splits=int(cv), shuffle=True, random_state=SEED) if cv_shuffle else int(cv)
            gs = GridSearchCV(estimator=pipe, param_grid=user_grid, cv=cv_obj,
                              scoring=scoring, n_jobs=1, refit=True, return_train_score=True)
            with st.spinner("GridSearchCV ì‹¤í–‰ ì¤‘..."):
                run_safely(gs.fit, X_train, y_train)

            st.subheader("ë² ìŠ¤íŠ¸ ê²°ê³¼")
            st.write("Best Params:", gs.best_params_)
            if scoring == "neg_root_mean_squared_error":
                st.write("Best CV RMSE (ìŒìˆ˜):", gs.best_score_)
            else:
                st.write(f"Best CV {scoring}:", gs.best_score_)

            y_pred_tr = gs.predict(X_train); y_pred_te = gs.predict(X_test)
            st.write("Train RMSE:", rmse(y_train, y_pred_tr))
            st.write("Test RMSE:", rmse(y_test, y_pred_te))
            st.write("Train RÂ² Score:", r2_score(y_train, y_pred_tr))
            st.write("Test RÂ² Score:", r2_score(y_test, y_pred_te))

            st.session_state["best_estimator"] = gs.best_estimator_
            st.session_state["best_params"] = gs.best_params_
            st.session_state["best_name"] = model_name
            st.session_state["best_cv_score"] = gs.best_score_
            st.session_state["best_scoring"] = scoring
            st.session_state["best_split_key"] = st.session_state.get("split_key")

            cvres = pd.DataFrame(gs.cv_results_)
            safe_cols = [c for c in ["rank_test_score","mean_test_score","std_test_score",
                                     "mean_train_score","std_train_score","params"] if c in cvres.columns]
            sorted_cvres = cvres.loc[:, safe_cols].sort_values("rank_test_score").reset_index(drop=True)
            st.dataframe(sorted_cvres, use_container_width=True)
            st.session_state["last_cvres"] = cvres

            if model_name == "XGBRegressor" and not XGB_AVAILABLE:
                st.warning("xgboostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. requirements.txtì— `xgboost`ë¥¼ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•´ ì£¼ì„¸ìš”.")
            return

        # --- Pruned: ìˆ˜ë™ ìŠ¤ìœ• ---
        with st.spinner("Cost-Complexity Pruning ì‹¤í–‰ ì¤‘..."):
            X_train_t = preprocessor.fit_transform(X_train, y_train)
            X_test_t  = preprocessor.transform(X_test)

            cand = user_grid.get("model__ccp_alpha", [])
            # Safe Mode ìƒí•œ
            if SAFE_MODE and len(cand) > SAFE_MAX_PRUNED_ALPHAS:
                idx = np.linspace(0, len(cand)-1, SAFE_MAX_PRUNED_ALPHAS).astype(int)
                cand = [float(cand[i]) for i in idx]

            results = []
            for a in cand:
                m = DecisionTreeRegressor(random_state=SEED, ccp_alpha=float(a))
                run_safely(m.fit, X_train_t, y_train)
                ytr = m.predict(X_train_t); yte = m.predict(X_test_t)
                results.append({
                    "alpha": float(a),
                    "train_rmse": rmse(y_train, ytr),
                    "test_rmse":  rmse(y_test,  yte),
                    "train_r2":   float(r2_score(y_train, ytr)),
                    "test_r2":    float(r2_score(y_test,  yte)),
                    "estimator":  m
                })

            df_res = pd.DataFrame(results).sort_values("alpha").reset_index(drop=True)
            best_idx = int(df_res["test_r2"].idxmax())
            best_row = df_res.loc[best_idx]

        st.subheader("ë² ìŠ¤íŠ¸ ê²°ê³¼ (ë…¸íŠ¸ë¶ ë°©ì‹)")
        st.write("Best Params:\n\n", {"model__ccp_alpha": best_row["alpha"]})
        st.write("Train RMSE:", best_row["train_rmse"])
        st.write("Test RMSE:",  best_row["test_rmse"])
        st.write("Train RÂ² Score:", best_row["train_r2"])
        st.write("Test RÂ² Score:",  best_row["test_r2"])

        best_alpha = float(best_row["alpha"])
        best_pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('model', DecisionTreeRegressor(random_state=SEED, ccp_alpha=best_alpha))
        ])
        best_pipeline.fit(X_train, y_train)

        st.session_state["best_estimator"] = best_pipeline
        st.session_state["best_params"] = {"model__ccp_alpha": best_alpha}
        st.session_state["best_name"] = "Decision Tree (Pruned) - NotebookStyle"
        st.session_state["best_cv_score"] = None
        st.session_state["best_scoring"] = "test_r2_max"
        st.session_state["best_split_key"] = st.session_state.get("split_key")

        st.markdown("**alpha sweep ë¡œê·¸**")
        st.dataframe(df_res[["alpha","train_rmse","test_rmse","train_r2","test_r2"]], use_container_width=True)

def page_ml():
    st.header("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§")
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(0.2):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=0.2, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(0.2)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    if "best_estimator" in st.session_state:
        model = st.session_state["best_estimator"]
        st.caption(f"í˜„ì¬ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© ({st.session_state.get('best_name')})")
        if st.session_state.get("best_split_key") != st.session_state.get("split_key"):
            st.warning("ì£¼ì˜: ë² ìŠ¤íŠ¸ ëª¨ë¸ì€ ì´ì „ ë¶„í• ë¡œ í•™ìŠµë¨. ìƒˆ ë¶„í• ë¡œ ë‹¤ì‹œ íŠœë‹í•´ ì£¼ì„¸ìš”.", icon="âš ï¸")
    else:
        model = Pipeline([('preprocessor', preprocessor),
                          ('model', RandomForestRegressor(random_state=SEED))])
        model.fit(X_train, y_train)
        st.caption("í˜„ì¬ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

    y_pred_tr = model.predict(X_train); y_pred_te = model.predict(X_test)
    st.metric("Train RÂ²", f"{r2_score(y_train, y_pred_tr):.3f}")
    st.metric("Test  RÂ²", f"{r2_score(y_test,  y_pred_te):.3f}")
    st.metric("Train RMSE", f"{rmse(y_train, y_pred_tr):.3f}")
    st.metric("Test  RMSE", f"{rmse(y_test,  y_pred_te):.3f}")

    if "best_params" in st.session_state:
        with st.expander("ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³´ê¸°"):
            st.json(st.session_state["best_params"])

def page_predict():
    st.header("í‰ì  ì˜ˆì¸¡")

    genre_opts   = sorted({g for sub in raw_df['genres'].dropna().apply(clean_cell_colab) for g in sub})
    week_opts    = sorted({d for sub in raw_df['day'].dropna().apply(clean_cell_colab) for d in sub})
    plat_opts    = sorted({p for sub in raw_df['network'].dropna().apply(clean_cell_colab) for p in sub})
    gender_opts  = sorted(raw_df['gender'].dropna().unique())
    role_opts    = sorted(raw_df['role'].dropna().unique())
    quarter_opts = sorted(raw_df['air_q'].dropna().unique())
    married_opts = sorted(raw_df['married'].dropna().unique())

    st.subheader("1) ì…ë ¥")
    col_left, col_right = st.columns(2)

    with col_left:
        st.markdown("**â‘  ì»¨í…ì¸  íŠ¹ì„±**")
        input_age     = st.number_input("ë‚˜ì´", 10, 80, 30)
        input_gender  = st.selectbox("ì„±ë³„", gender_opts) if gender_opts else st.text_input("ì„±ë³„ ì…ë ¥", "")
        input_role    = st.selectbox("ì—­í• ", role_opts) if role_opts else st.text_input("ì—­í•  ì…ë ¥", "")
        input_married = st.selectbox("ê²°í˜¼ì—¬ë¶€", married_opts) if married_opts else st.text_input("ê²°í˜¼ì—¬ë¶€ ì…ë ¥", "")
        input_genre   = st.multiselect("ì¥ë¥´ (ë©€í‹° ì„ íƒ)", genre_opts, default=genre_opts[:1] if genre_opts else [])
        derived_age_group = age_to_age_group(int(input_age))

        n_genre = len(input_genre)
        if n_genre == 0:  genre_bucket = "ì¥ë¥´ì—†ìŒ"
        elif n_genre <= 2: genre_bucket = "1~2ê°œ"
        elif n_genre <= 4: genre_bucket = "3~4ê°œ"
        elif n_genre <= 6: genre_bucket = "5~6ê°œ"
        else: genre_bucket = "7ê°œ ì´ìƒ"
        st.caption(f"ìë™ ì—°ë ¹ëŒ€: **{derived_age_group}**  |  ì¥ë¥´ ê°œìˆ˜: **{genre_bucket}**")

    with col_right:
        st.markdown("**â‘¡ í¸ì„± íŠ¹ì„±**")
        input_quarter = st.selectbox("ë°©ì˜ë¶„ê¸°", quarter_opts) if quarter_opts else st.text_input("ë°©ì˜ë¶„ê¸° ì…ë ¥", "")
        input_week    = st.multiselect("ë°©ì˜ìš”ì¼ (ë©€í‹° ì„ íƒ)", week_opts, default=week_opts[:1] if week_opts else [])
        input_plat    = st.multiselect("í”Œë«í¼ (ë©€í‹° ì„ íƒ)", plat_opts, default=plat_opts[:1] if plat_opts else [])
        age_group_candidates = ["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€", "60ëŒ€ ì´ìƒ"]
        data_age_groups = sorted(set(str(x) for x in raw_df.get("age_group", pd.Series([], dtype=object)).dropna().unique()))
        opts_age_group = data_age_groups if data_age_groups else age_group_candidates
        safe_index = 0 if not opts_age_group else min(1, len(opts_age_group)-1)
        target_age_group = st.selectbox("ğŸ¯ íƒ€ê¹ƒ ì‹œì²­ì ì—°ë ¹ëŒ€",
                                        options=opts_age_group if opts_age_group else ["(ë°ì´í„° ì—†ìŒ)"],
                                        index=safe_index,
                                        key="target_age_group_main")
        st.session_state["target_age_group"] = target_age_group
        st.session_state["actor_age"] = int(input_age)

        # ì˜ˆì¸¡ ë²„íŠ¼(ì¿¨ë‹¤ìš´)
        predict_btn = st.button("ì˜ˆì¸¡ ì‹¤í–‰", key="btn_predict")

    # ---- ì˜ˆì¸¡ ìƒíƒœ ìœ ì§€/ì €ì¥ ----
    if predict_btn:
        if not _cooldown_ok("last_predict_time", SAFE_PRED_COOLDOWN_SEC):
            st.info("ì—°ì† ì˜ˆì¸¡ ìš”ì²­ì„ ì ì‹œ ì œí•œí•˜ê³  ìˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        else:
            # 1) ì‚¬ìš©í•  ëª¨ë¸ ê²°ì •
            if "best_estimator" in st.session_state:
                model_full = clone(st.session_state["best_estimator"])
                st.caption(f"ì˜ˆì¸¡ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ì¬í•™ìŠµ ì‚¬ìš© ({st.session_state.get('best_name')})")
            else:
                model_full = Pipeline([('preprocessor', preprocessor),
                                       ('model', RandomForestRegressor(n_estimators=100, random_state=SEED))])
                st.caption("ì˜ˆì¸¡ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

            # 2) ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ì•ˆì „ ì‹¤í–‰)
            run_safely(model_full.fit, X_colab_base, y_all)

            # 3) í˜„ì¬ ì…ë ¥ìœ¼ë¡œ ì˜ˆì¸¡
            user_raw = pd.DataFrame([{
                'age': int(input_age), 'gender': input_gender, 'role': input_role, 'married': input_married,
                'air_q': input_quarter, 'age_group': derived_age_group,
                'genres': input_genre, 'day': input_week, 'network': input_plat, 'ì¥ë¥´êµ¬ë¶„': genre_bucket,
            }])
            st.session_state["target_age_group"] = st.session_state.get("target_age_group", derived_age_group)

            def _build_user_base_for_pred(df_raw: pd.DataFrame) -> pd.DataFrame:
                _user_mlb = colab_multilabel_transform(df_raw, cols=('genres','day','network'))
                _base = pd.concat([X_colab_base.iloc[:0].copy(), _user_mlb], ignore_index=True)
                _base = _base.drop(columns=[c for c in ['ë°°ìš°ëª…','ë“œë¼ë§ˆëª…','genres','day','network','score','start airing'] if c in _base.columns], errors='ignore')
                for c in X_colab_base.columns:
                    if c not in _base.columns:
                        _base[c] = 0
                _base = _base[X_colab_base.columns].tail(1)
                num_cols_ = X_colab_base.select_dtypes(include=[np.number]).columns.tolist()
                if len(num_cols_) > 0:
                    _base[num_cols_] = _base[num_cols_].apply(pd.to_numeric, errors="coerce")
                    _base[num_cols_] = _base[num_cols_].replace([np.inf, -np.inf], np.nan).fillna(0.0)
                return _base

            user_base_now = _build_user_base_for_pred(user_raw)
            pred = float(run_safely(model_full.predict, user_base_now)[0])

            # 4) ì„¸ì…˜ ì €ì¥ (ì´í›„ ìœ„ì ¯ë§Œ ë°”ê¿”ë„ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ)
            st.session_state["cf_user_raw"] = user_raw.copy()
            st.session_state["cf_pred"] = float(pred)
            st.session_state["cf_model"] = model_full
            st.session_state["cf_inputs"] = {
                "age": int(input_age),
                "gender": input_gender,
                "role": input_role,
                "married": input_married,
                "air_q": input_quarter,
                "age_group": derived_age_group,
                "genres": list(input_genre),
                "day": list(input_week),
                "network": list(input_plat),
                "genre_bucket": genre_bucket,
            }

    # ì´ì „ ì˜ˆì¸¡ ìƒíƒœ ë³µêµ¬
    model_full = st.session_state.get("cf_model", None)
    user_raw   = st.session_state.get("cf_user_raw", None)
    pred       = st.session_state.get("cf_pred", None)

    if model_full is None or user_raw is None or pred is None:
        st.info("ì¢Œì¸¡ ì…ë ¥ì„ ì„¤ì •í•œ ë’¤ **[ì˜ˆì¸¡ ì‹¤í–‰]**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    st.success(f"ğŸ’¡ ì˜ˆìƒ í‰ì : {float(pred):.2f}")

    # =========================
    # ğŸ” What-if (ë…ë¦½ ì•¡ì…˜ Top N)
    # =========================
    st.markdown("---")
    st.subheader("2) ì¼€ë¯¸ìŠ¤ì½”ì–´ í‰ì  ì˜ˆì¸¡")

    target_age_group = st.session_state.get("target_age_group")
    if not target_age_group:
        target_age_group = "20ëŒ€"
        st.session_state["target_age_group"] = target_age_group

    def _age_group_to_decade(s: str) -> int:
        m = re.search(r"(\d+)", str(s))
        if m:
            n = int(m.group(1))
            return 60 if "ì´ìƒ" in str(s) and n < 60 else n
        return 0

    actor_decade  = (int(st.session_state.get("actor_age", 30))//10)*10
    target_decade = _age_group_to_decade(target_age_group)
    gap = abs(actor_decade - target_decade)

    with st.container():
        st.markdown("**ğŸ¯ íƒ€ê¹ƒ-ë°°ìš° ì—°ë ¹ëŒ€ ì •ë ¬ ê°€ì´ë“œ**")
        if target_decade <= 20:
            st.markdown("- í†¤/ì¥ë¥´: romance Â· comedy Â· action ìœ„ì£¼, ê°€ë²¼ìš´ ëª°ì… ìœ ë„")
            st.markdown("- í¸ì„±: í† ìš”ì¼/ì£¼ë§ ê°•ì„¸, í´ë¦½ ì¤‘ì‹¬ SNS í™•ì‚° ê³ ë ¤")
        elif target_decade <= 30:
            st.markdown("- í†¤/ì¥ë¥´: romance/dramaì— ìŠ¤ë¦´ëŸ¬/ë¯¸ìŠ¤í„°ë¦¬ ê°€ë¯¸(í•˜ì´ë¸Œë¦¬ë“œ)")
            st.markdown("- í”Œë«í¼: OTT ë™ì‹œ ê³µê°œë¡œ í™”ì œì„± í™•ë³´")
        elif target_decade <= 40:
            st.markdown("- í†¤/ì¥ë¥´: drama / thriller / society ì¤‘ì‹¬, ì£¼ì œ ë°€ë„ë¥¼ ë†’ì„")
            st.markdown("- í¸ì„±: ì£¼ì¤‘ ì§‘ì¤‘, ì—í”¼ì†Œë“œ í€„ë¦¬í‹° ë³€ë™ ìµœì†Œí™”")
        else:
            st.markdown("- í†¤/ì¥ë¥´: hist_war / family / society, ìŠ¤í† ë¦¬ ì™„ì„±ë„Â·ë©”ì‹œì§€ ê°•í™”")
            st.markë¡ ("- í¸ì„±: ì‹œì²­ ë£¨í‹´ ë°˜ì˜í•œ ì•ˆì •ì  ìŠ¬ë¡¯")

        if gap >= 20:
            st.info(f"ë°°ìš° ë‚˜ì´ {st.session_state.get('actor_age', 30)}ì„¸(â‰ˆ{actor_decade}ëŒ€) vs íƒ€ê¹ƒ {target_age_group} â†’ **ì—°ë ¹ëŒ€ ê²©ì°¨ í¼**. ì¥ë¥´/í¸ì„±/í”Œë«í¼ì„ íƒ€ê¹ƒ ì„±í–¥ì— ë§ì¶˜ ë³€ê²½ì•ˆì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë†’ì´ì„¸ìš”.")
        else:
            st.caption(f"ë°°ìš° ë‚˜ì´ {st.session_state.get('actor_age', 30)}ì„¸(â‰ˆ{actor_decade}ëŒ€)ì™€ íƒ€ê¹ƒ {target_age_group}ì˜ ê²©ì°¨ê°€ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    def _build_user_base(df_raw: pd.DataFrame) -> pd.DataFrame:
        _user_mlb = colab_multilabel_transform(df_raw, cols=('genres','day','network'))
        _base = pd.concat([X_colab_base.iloc[:0].copy(), _user_mlb], ignore_index=True)
        _base = _base.drop(columns=[c for c in ['ë°°ìš°ëª…','ë“œë¼ë§ˆëª…','genres','day','network','score','start airing'] if c in _base.columns], errors='ignore')
        for c in X_colab_base.columns:
            if c not in _base.columns:
                _base[c] = 0
        _base = _base[X_colab_base.columns].tail(1)
        num_cols_ = X_colab_base.select_dtypes(include=[np.number]).columns.tolist()
        if len(num_cols_) > 0:
            _base[num_cols_] = _base[num_cols_].apply(pd.to_numeric, errors="coerce")
            _base[num_cols_] = _base[num_cols_].replace([np.inf, -np.inf], np.nan).fillna(0.0)
        return _base

    def _predict_from_raw(df_raw: pd.DataFrame) -> float:
        vb = _build_user_base(df_raw)
        return float(model_full.predict(vb)[0])

    current_pred = float(pred)

    def _classes_safe(key: str):
        return [s for s in (st.session_state.get(f"mlb_classes_{key}", []) or [])]

    genre_classes   = [g for g in _classes_safe("genres") if isinstance(g, str)]
    day_classes     = [d for d in _classes_safe("day") if isinstance(d, str)]
    network_classes = [n for n in _classes_safe("network") if isinstance(n, str)]

    # ì•¡ì…˜ ë¹Œë”
    def _add_genre(tag: str):
        def _fn(df):
            new = df.copy()
            cur = list(new.at[0, "genres"])
            if tag not in cur:
                cur = cur + [tag]
            new.at[0, "genres"] = cur
            return new
        return _fn

    def _set_days(days_list: List[str]):
        def _fn(df):
            new = df.copy()
            new.at[0, "day"] = days_list
            return new
        return _fn

    def _ensure_platform(p: str):
        def _fn(df):
            new = df.copy()
            cur = list(new.at[0, "network"])
            if p not in cur:
                cur = cur + [p]
            new.at[0, "network"] = cur
            return new
        return _fn

    def _set_role(val: str):
        def _fn(df):
            new = df.copy()
            new.at[0, "role"] = val
            return new
        return _fn

    def _set_married(val: str):
        def _fn(df):
            new = df.copy()
            new.at[0, "married"] = val
            return new
        return _fn

    # í›„ë³´ ìƒì„±
    actions = []
    g_priority_by_target = {
        "young": ["romance", "comedy", "action", "thriller"],
        "adult": ["drama", "thriller", "hist_war", "romance", "comedy"],
        "senior": ["hist_war", "drama", "society", "thriller"]
    }
    if target_decade <= 30:
        glist = g_priority_by_target["young"]
    elif target_decade <= 40:
        glist = g_priority_by_target["adult"]
    else:
        glist = g_priority_by_target["senior"]
    for g in glist:
        if g in genre_classes:
            actions.append(("genre", f"ì¥ë¥´ ì¶”ê°€: {g}", _add_genre(g)))

    if "saturday" in day_classes:
        actions.append(("schedule", "í¸ì„± ìš”ì¼: í† ìš”ì¼ ë‹¨ì¼", _set_days(["saturday"])))
    if "friday" in day_classes:
        actions.append(("schedule", "í¸ì„± ìš”ì¼: ê¸ˆìš”ì¼ ë‹¨ì¼", _set_days(["friday"])))
    if "wednesday" in day_classes:
        actions.append(("schedule", "í¸ì„± ìš”ì¼: ìˆ˜ìš”ì¼ ë‹¨ì¼", _set_days(["wednesday"])))

    if "NETFLIX" in network_classes:
        actions.append(("platform", "í”Œë«í¼ í¬í•¨: NETFLIX", _ensure_platform("NETFLIX")))
    if "TVN" in network_classes:
        actions.append(("platform", "í”Œë«í¼ í¬í•¨: TVN", _ensure_platform("TVN")))
    if "WAVVE" in network_classes:
        actions.append(("platform", "í”Œë«í¼ í¬í•¨: WAVVE", _ensure_platform("WAVVE")))

    if "role" in user_raw.columns and str(user_raw.at[0, "role"]) != "ì£¼ì—°":
        actions.append(("casting", "ì—­í• : ì£¼ì—°ìœ¼ë¡œ ë³€ê²½", _set_role("ì£¼ì—°")))
    if "married" in user_raw.columns and str(user_raw.at[0, "married"]) != "ë¯¸í˜¼":
        actions.append(("married", "ê²°í˜¼ì—¬ë¶€: ë¯¸í˜¼ìœ¼ë¡œ ë³€ê²½", _set_married("ë¯¸í˜¼")))

    # ì•¡ì…˜ ìŠ¤ì½”ì–´ë§ (ëª¨ë¸ ì¬í•™ìŠµ/ì´ˆê¸°í™” ì—†ìŒ â†’ ìŠ¬ë¼ì´ë”ë§Œ ë°”ê¿”ë„ ì˜ˆì¸¡ ìœ ì§€)
    scored = []
    for cat, desc, fn in actions:
        cand = fn(user_raw)
        p = _predict_from_raw(cand)
        scored.append({"ì¹´í…Œê³ ë¦¬": cat, "ë³€ê²½ì•ˆ": desc, "ì˜ˆì¸¡": p, "ë¦¬í”„íŠ¸": p - current_pred})

    if not scored:
        st.info("ì¶”ì²œí•  ì•¡ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. (í˜„ì‹¤ ì œì•½/ì…ë ¥ê°’ìœ¼ë¡œ ì¸í•´ í›„ë³´ê°€ ì—†ì„ ìˆ˜ ìˆì–´ìš”)")
    else:
        df_scored = pd.DataFrame(scored)
        idx_best = df_scored.groupby("ì¹´í…Œê³ ë¦¬")["ë¦¬í”„íŠ¸"].idxmax()
        df_best_per_cat = df_scored.loc[idx_best].copy()

        # ğŸ”» ì—¬ê¸° ìŠ¬ë¼ì´ë”ë¥¼ ë°”ê¿”ë„ current_pred/stateëŠ” ìœ ì§€ë˜ë¯€ë¡œ ì´ˆê¸°í™” ì—†ìŒ
        top_n = st.slider("ì¶”ì²œ ê°œìˆ˜", 3, 7, 5 if not SAFE_MODE else 3, key="rec_topn_slider")
        df_top = df_best_per_cat.sort_values(["ë¦¬í”„íŠ¸", "ì˜ˆì¸¡"], ascending=False).head(top_n).reset_index(drop=True)

        st.dataframe(df_top[["ì¹´í…Œê³ ë¦¬","ë³€ê²½ì•ˆ"]], use_container_width=True)

        st.markdown("#### ğŸ” ì•¡ì…˜ë³„ ì†”ë£¨ì…˜ ìš”ì•½")
        genre_reason = {
            "thriller": "ê¸´ì¥ê°Â·ëª°ì…ë„ ìƒìŠ¹ â†’ ì²´ë¥˜ì‹œê°„/í‰ì  ìš°í˜¸ì ",
            "hist_war": "ì‘í’ˆì„±Â·ì™„ì„±ë„ í¬ì¸íŠ¸ë¡œ í‰ì  ìƒí–¥",
            "sf": "ì‹ ì„ í•œ ì†Œì¬/ì„¸ê³„ê´€ìœ¼ë¡œ ì´ˆë°˜ í¡ì…ë ¥â†‘",
            "action": "ì‹œê°ì  ì„íŒ©íŠ¸ë¡œ ì´ˆë°˜ ë§Œì¡±ë„ ìƒìŠ¹",
            "romance": "ëŒ€ì¤‘ì„± ë†’ì•„ ë„“ì€ íƒ€ê¹ƒ ì í•©",
            "drama": "ë³´í¸ì  ê³µê° ì„œì‚¬ë¡œ ì•ˆì •ì ",
            "comedy": "ê°€ë²¼ìš´ í†¤ìœ¼ë¡œ í­ë„“ì€ ìˆ˜ìš©ì¸µ í™•ë³´",
            "society": "ì‚¬íšŒì  ë©”ì‹œì§€Â·í˜„ì‹¤ê°ìœ¼ë¡œ ì¶©ì„±ë„â†‘",
        }
        day_reason = {
            "ì›”ìš”ì¼": "í•œ ì£¼ì˜ ì‹œì‘, ê³ ì • ì‹œì²­ì¸µ í™•ë³´ì™€ ì…ì†Œë¬¸ í˜•ì„± ê¸°íšŒ",
            "í™”ìš”ì¼": "ì£¼ì¤‘ ì´ˆë°˜, ì§€ì† ì‹œì²­ ìœ ë„ì— ìœ ë¦¬",
            "ìˆ˜ìš”ì¼": "ì£¼ì¤‘ ì¤‘ì•™ë¶€ ì§‘ì¤‘ ì‹œì²­ì¸µ ê³µëµ",
            "ëª©ìš”ì¼": "ì£¼ë§ ì „í™˜ ì§ì „, ê¸´ì¥ê° ìˆëŠ” ì „ê°œë¡œ ì‹œì²­ë¥  ìƒìŠ¹ ìœ ë„",
            "ê¸ˆìš”ì¼": "ì£¼ë§ ì´ˆì… ë…¸ì¶œë¡œ íšŒì°¨ ì „í™˜ìœ¨ í™•ë³´",
            "í† ìš”ì¼": "ì‹œì²­ ê°€ìš©ì‹œê°„â†‘ â†’ ëª°ì…/êµ¬ì „ íš¨ê³¼ ê¸°ëŒ€",
            "ì¼ìš”ì¼": "ê°€ì¡±Â·ì „ ì—°ë ¹ëŒ€ íƒ€ê¹ƒ, ë‹¤ìŒ ì£¼ ì‹œì²­ ì˜ˆê³  íš¨ê³¼ ê·¹ëŒ€í™”"
        }
        platform_reason = {
            "ENT": "ì˜ˆëŠ¥ ì¤‘ì‹¬ í¸ì„±ìœ¼ë¡œ ëŒ€ì¤‘ì  í™”ì œì„± í™•ë³´ ìš©ì´",
            "ETC_P": "ê¸°íƒ€ ì¼€ì´ë¸” ì±„ë„ë¡œ í‹ˆìƒˆ ì‹œì²­ì¸µ ê³µëµ",
            "GPC": "ì „ë¬¸ ì±„ë„ íŠ¹í™” íƒ€ê¹ƒ ì‹œì²­ì¸µ ì§‘ì¤‘",
            "JTBC": "í”„ë¼ì„ íƒ€ì„ ë“œë¼ë§ˆ ê°•ì„¸, í™”ì œì„± ë†’ì€ ì‹œë¦¬ì¦ˆ ì œì‘ ê²½í—˜",
            "KBS": "ì „êµ­ ë‹¨ìœ„ ì§€ìƒíŒŒ ì»¤ë²„ë¦¬ì§€, ì „ ì„¸ëŒ€ ì ‘ê·¼ì„± ìš°ìˆ˜",
            "NETFLIX": "ê¸€ë¡œë²Œ ë…¸ì¶œ/ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ â†’ í™”ì œì„±/ë¦¬ë·° í™•ë³´ ìš©ì´",
            "SBS": "íŠ¸ë Œë””í•œ ë“œë¼ë§ˆÂ·ì˜ˆëŠ¥ ë¼ì¸ì—…ìœ¼ë¡œ ì Šì€ì¸µ ì„ í˜¸",
            "TVN": "í”„ë¼ì„ í¸ì„±Â·ë¸Œëœë”© ì‹œë„ˆì§€",
            "MBC": "ì˜¤ëœ ë“œë¼ë§ˆ ì œì‘ ì „í†µ, ì•ˆì •ì  ì‹œì²­ì¸µ í™•ë³´"
        }
        etc_reason = {"ì£¼ì—°": "ìºë¦­í„° ê³µê°/ë…¸ì¶œ ê·¹ëŒ€í™”", "ë¯¸í˜¼": "ë¡œë§¨ìŠ¤/ì²­ì¶˜ë¬¼ í†¤ ê²°í•© ì‹œ ëª°ì…ë„â†‘"}

        def _explain(desc: str) -> str:
            why = []
            m = re.search(r"ì¥ë¥´ ì¶”ê°€:\s*([A-Za-z_]+)", desc)
            if m:
                g = m.group(1).lower()
                if g in genre_reason: why.append(f"ì¥ë¥´ íš¨ê³¼: {genre_reason[g]}")
                if target_decade <= 20 and g in {"romance","comedy","action"}: why.append("ì Šì€ íƒ€ê¹ƒê³¼ í†¤ ë§¤ì¹­ ì–‘í˜¸")
                if target_decade >= 40 and g in {"hist_war","drama","thriller","society"}: why.append("ì„±ìˆ™ íƒ€ê¹ƒ ì„ í˜¸ ì£¼ì œì™€ ë¶€í•©")
            if "í† ìš”ì¼" in desc or "saturday" in desc: why.append(f"í¸ì„± íš¨ê³¼: {day_reason['í† ìš”ì¼']}")
            if "ê¸ˆìš”ì¼" in desc or "friday" in desc:   why.append(f"í¸ì„± íš¨ê³¼: {day_reason['ê¸ˆìš”ì¼']}")
            if "ìˆ˜ìš”ì¼" in desc or "wednesday" in desc:why.append(f"í¸ì„± íš¨ê³¼: {day_reason['ìˆ˜ìš”ì¼']}")
            for k, v in platform_reason.items():
                if k in desc: why.append(f"í”Œë«í¼ íš¨ê³¼: {v}")
            if "ì£¼ì—°" in desc: why.append(f"ìºìŠ¤íŒ… íš¨ê³¼: {etc_reason['ì£¼ì—°']}")
            if "ë¯¸í˜¼" in desc: why.append(f"ìºë¦­í„° í†¤: {etc_reason['ë¯¸í˜¼']}")
            return " / ".join(why) if why else "ë°ì´í„° ê¸°ë°˜ ìƒ ìƒìŠ¹ ìš”ì¸"

        st.markdown("**ğŸ“ ìƒìœ„ ë³€ê²½ì•ˆ ì†”ë£¨ì…˜**")
        for _, r in df_top.iterrows():
            st.markdown(f"- **{r['ë³€ê²½ì•ˆ']}** Â· {_explain(r['ë³€ê²½ì•ˆ'])}")

# ================== ì‚¬ì´ë“œë°” (ë„¤ë¹„ + ì„¤ì •) ==================
NAV_ITEMS = [
    ("overview", "ğŸ ", "ê°œìš”",        page_overview),
    ("basic",    "ğŸ“‹", "ê¸°ì´ˆí†µê³„",    page_basic),
    ("dist",     "ğŸ“ˆ", "ë¶„í¬/êµì°¨",   page_dist),
    ("filter",   "ğŸ› ï¸", "í•„í„°",        page_filter),
    ("all",      "ğŸ—‚ï¸", "ì „ì²´ë³´ê¸°",    page_all),
    ("tuning",   "ğŸ§ª", "íŠœë‹",        page_tuning),
    ("ml",       "ğŸ¤–", "MLëª¨ë¸",      page_ml),
    ("predict",  "ğŸ¯", "ì˜ˆì¸¡",        page_predict),
]

if "nav" not in st.session_state:
    st.session_state["nav"] = _get_nav_from_query() or "overview"
current = st.session_state["nav"]

with st.sidebar:
    st.markdown('<div class="sb-wrap">', unsafe_allow_html=True)
    st.markdown("""
    <style>
    section[data-testid="stSidebar"] .sb-brand { display:flex; align-items:center; gap:8px; font-weight:900; }
    section[data-testid="stSidebar"] .sb-brand .logo { font-size:35px !important; line-height:1; }
    section[data-testid="stSidebar"] .sb-brand .name { font-size:26px !important; line-height:1.2; }
    </style>
    <div class="sb-brand">
        <span class="logo">ğŸ’«</span>
        <span class="name">ì¼€ë¯¸ìŠ¤ì½”ì–´</span>
    </div>
    """, unsafe_allow_html=True)

    st.markdown('<div class="sb-menu">', unsafe_allow_html=True)
    for slug, icon, label, _fn in NAV_ITEMS:
        active = (slug == current)
        st.markdown(f'<div class="sb-nav {"active" if active else ""}">', unsafe_allow_html=True)
        if st.button(f"{icon}  {label}", key=f"nav_{slug}", use_container_width=True):
            st.session_state["nav"] = slug
            _set_nav_query(slug)
            st.rerun()
        st.markdown('</div>', unsafe_allow_html=True)

    sm_text = "ON" if SAFE_MODE else "OFF"
    st.markdown(f'<div class="sb-card"><h4>Safe Mode: {sm_text}</h4>', unsafe_allow_html=True)
    st.caption("ê³¼ë„í•œ íƒìƒ‰/ì—°íƒ€/ëŒ€ìš©ëŸ‰ ì‘ì—…ì„ ì œí•œí•´ 403(Fair-use) ì°¨ë‹¨ ê°€ëŠ¥ì„±ì„ ë‚®ì¶¥ë‹ˆë‹¤.")
    st.markdown('</div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)
    st.markdown('<div class="sb-footer">Â© Chemiscore â€¢ <span class="ver">v0.2</span></div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

# ================== ë¼ìš°íŒ… ==================
PAGES = {slug: fn for slug, _, _, fn in NAV_ITEMS}
PAGES.get(st.session_state["nav"], page_overview)()
