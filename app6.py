# app.py
# ---- dependency guard (optional) ----
import importlib.util, streamlit as st
_missing = [m for m in ("numpy","scipy","sklearn","joblib","threadpoolctl","xgboost") if importlib.util.find_spec(m) is None]
if _missing:
    st.error(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜: {_missing}. requirements.txt / runtime.txt ë²„ì „ì„ ê³ ì •í•´ ì¬ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

import os
import ast
import random
import numpy as np
import pandas as pd
from pathlib import Path
import platform
from sklearn.metrics import mean_squared_error
import streamlit as st
import plotly.express as px
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.base import clone
import re
from sklearn.model_selection import KFold
from sklearn.impute import SimpleImputer

# XGBê°€ ì„¤ì¹˜ë¼ ìˆìœ¼ë©´ ì“°ë„ë¡ ì•ˆì „í•˜ê²Œ ì¶”ê°€
try:
    from xgboost import XGBRegressor
    XGB_AVAILABLE = True
except Exception:
    XGB_AVAILABLE = False

def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

# ===== í˜ì´ì§€ ì„¤ì • =====
st.set_page_config(page_title="K-ë“œë¼ë§ˆ ë¶„ì„/ì˜ˆì¸¡", page_icon="ğŸ¬", layout="wide")

# ===== ì „ì—­ ì‹œë“œ ê³ ì • =====
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED)

# ===== í•œê¸€ í°íŠ¸ ë¶€íŠ¸ìŠ¤íŠ¸ë© =====
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

if st.session_state.get("font_cache_cleared") is not True:
    import shutil
    shutil.rmtree(matplotlib.get_cachedir(), ignore_errors=True)
    st.session_state["font_cache_cleared"] = True

def ensure_korean_font():
    matplotlib.rcParams['axes.unicode_minus'] = False
    base = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()
    candidates = [
        base / "fonts" / "NanumGothic-Regular.ttf",
        base / "fonts" / "NanumGothic.ttf",
    ]
    if platform.system() == "Windows":
        candidates += [Path(r"C:\Windows\Fonts\malgun.ttf"), Path(r"C:\Windows\Fonts\malgunbd.ttf")]
    wanted = ("nanum","malgun","applegothic","notosanscjk","sourcehan","gulim","dotum","batang","pretendard","gowun","spoqa")
    for f in fm.findSystemFonts(fontext="ttf"):
        if any(k in os.path.basename(f).lower() for k in wanted):
            candidates.append(Path(f))
    for p in candidates:
        try:
            if p.exists():
                fm.fontManager.addfont(str(p))
                family = fm.FontProperties(fname=str(p)).get_name()
                matplotlib.rcParams['font.family'] = family
                st.session_state["kfont_path"] = str(p)  # WordCloudìš©
                return family
        except Exception:
            continue
    st.session_state["kfont_path"] = None
    return None

_ = ensure_korean_font()

def age_to_age_group(age: int) -> str:
    s = raw_df.get('age_group')
    if s is None or s.dropna().empty:
        if age < 20: return "10ëŒ€"
        if age < 30: return "20ëŒ€"
        if age < 40: return "30ëŒ€"
        if age < 50: return "40ëŒ€"
        if age < 60: return "50ëŒ€"
        return "60ëŒ€ ì´ìƒ"

    series = s.dropna().astype(str)
    vocab = series.unique().tolist()
    counts = series.value_counts()

    decade = (int(age)//10)*10

    exact = [g for g in vocab if re.search(rf"{decade}\s*ëŒ€", g)]
    if exact:
        return counts[exact].idxmax()

    loose = [g for g in vocab if str(decade) in g]
    if loose:
        return counts[loose].idxmax()

    if decade >= 60:
        over = [g for g in vocab if ('60' in g) or ('ì´ìƒ' in g)]
        if over:
            return counts[over].idxmax()

    with_num = []
    for g in vocab:
        m = re.search(r'(\d+)', g)
        if m:
            with_num.append((g, int(m.group(1))))
    if with_num:
        nearest_num = min(with_num, key=lambda t: abs(t[1]-decade))[1]
        candidates = [g for g,n in with_num if n==nearest_num]
        return counts[candidates].idxmax()

    return counts.idxmax()

# ===== ì „ì²˜ë¦¬ ìœ í‹¸ =====
from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

def clean_cell_colab(x):
    if isinstance(x, list):
        return [str(i).strip() for i in x if pd.notna(i)]
    if pd.isna(x):
        return []
    if isinstance(x, str):
        try:
            parsed = ast.literal_eval(x)
            if isinstance(parsed, list):
                return [str(i).strip() for i in parsed if pd.notna(i)]
            return [x.strip()]
        except Exception:
            return [x.strip()]
    return [str(x).strip()]

def colab_multilabel_fit_transform(df: pd.DataFrame, cols=('genres','day','network')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)
        mlb = MultiLabelBinarizer()
        arr = mlb.fit_transform(out[col])
        new_cols = [f"{col}_{c.strip().upper()}" for c in mlb.classes_]
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)
        st.session_state[f"mlb_{col}"] = mlb
        st.session_state[f"mlb_classes_{col}"] = mlb.classes_.tolist()
    return out

def colab_multilabel_transform(df: pd.DataFrame, cols=('genres','day','network')) -> pd.DataFrame:
    out = df.copy()
    for col in cols:
        out[col] = out[col].apply(clean_cell_colab)
        mlb = st.session_state.get(f"mlb_{col}", None)
        if mlb is None:
            classes = st.session_state.get(f"mlb_classes_{col}", [])
            mlb = MultiLabelBinarizer()
            if classes:
                mlb.classes_ = np.array(classes)
            else:
                try:
                    prefix = f"{col}_"
                    labels = [c[len(prefix):] for c in df_mlb.columns if c.startswith(prefix)]
                    if labels:
                        mlb.classes_ = np.array(labels)
                    else:
                        mlb.fit(out[col])
                except Exception:
                    mlb.fit(out[col])
        arr = mlb.transform(out[col])
        new_cols = [f"{col}_{c.strip().upper()}" for c in mlb.classes_]
        out = out.drop(columns=[c for c in new_cols if c in out.columns], errors='ignore')
        out = pd.concat([out, pd.DataFrame(arr, columns=new_cols, index=out.index)], axis=1)
    return out

def expand_feature_cols_for_training(base: pd.DataFrame, selected: list):
    cols = []
    for c in selected:
        if c in ('genres','day','network'):
            prefix = f"{c}_"
            cols += [k for k in base.columns if k.startswith(prefix)]
        else:
            cols.append(c)
    return cols

def build_X_from_selected(base: pd.DataFrame, selected: list) -> pd.DataFrame:
    X = pd.DataFrame(index=base.index)
    use_cols = expand_feature_cols_for_training(base, selected)
    if use_cols:
        X = pd.concat([X, base[use_cols]], axis=1)
    singles = [c for c in selected if c not in ('genres','day','network')]
    for c in singles:
        if c in base.columns and base[c].dtype == 'object':
            X = pd.concat([X, pd.get_dummies(base[c], prefix=c)], axis=1)
        elif c in base.columns:
            X[c] = base[c]
    return X

# ===== ë°ì´í„° ë¡œë“œ =====
@st.cache_data
def load_data():
    # drama_d.json ì€ {ì»¬ëŸ¼ëª…: {row_index: value}} í˜•íƒœ
    raw = pd.read_json('drama_d.json')
    if isinstance(raw, pd.Series):
        raw = pd.DataFrame({c: pd.Series(v) for c, v in raw.to_dict().items()})
    else:
        raw = pd.DataFrame({c: pd.Series(v) for c, v in raw.to_dict().items()})
    # â˜… ë…¸íŠ¸ë¶ê³¼ ë™ì¼: ì¸ë±ìŠ¤/ìˆœì„œ ìœ ì§€ (reset_index ì œê±°)
    return raw

raw_df = load_data()

# ===== ë©€í‹°ë¼ë²¨ ì¸ì½”ë”© ê²°ê³¼ ìƒì„± (genres / day / network) =====
df_mlb = colab_multilabel_fit_transform(raw_df, cols=('genres','day','network'))

# ===== Colab ìŠ¤íƒ€ì¼ X/y, ì „ì²˜ë¦¬ ì •ì˜ =====
drop_cols = [c for c in ['ë°°ìš°ëª…','ë“œë¼ë§ˆëª…','genres','day','network','score','start airing'] if c in df_mlb.columns]
X_colab_base = df_mlb.drop(columns=drop_cols, errors='ignore')
y_all = df_mlb['score']

categorical_features = [c for c in ['role','gender','air_q','married','age_group'] if c in X_colab_base.columns]

# â˜… OHEëŠ” ë°€ì§‘(dense)ë¡œ -> StandardScaler ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
try:
    ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)
except TypeError:
    ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False)

preprocessor = ColumnTransformer(
    transformers=[
        (
            'cat',
            Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='constant', fill_value='(missing)')),
                ('ohe', OneHotEncoder(
                    drop='first',
                    handle_unknown='ignore',
                    sparse_output=False   # í´ë¦¬/ìŠ¤ì¼€ì¼ëŸ¬ì™€ í˜¸í™˜ì„±â†‘
                )),
            ]),
            categorical_features
        )
    ],
    remainder='passthrough'
)
# ===== EDAìš© ë¦¬ìŠ¤íŠ¸ =====
genre_list = [g for sub in raw_df.get('genres', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for g in sub]
broadcaster_list = [b for sub in raw_df.get('network', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for b in sub]
week_list = [w for sub in raw_df.get('day', pd.Series(dtype=object)).dropna().apply(clean_cell_colab) for w in sub]
unique_genres = sorted(set(genre_list))

# ===== ì‚¬ì´ë“œë°” =====
with st.sidebar:
    st.header("ğŸ¤– ëª¨ë¸ ì„¤ì •")
    # â˜… ë…¸íŠ¸ë¶ ì¬í˜„: test_size ê³ ì •
    test_size = 0.2
    st.caption("ë…¸íŠ¸ë¶ ì¬í˜„ ëª¨ë“œ: test_size=0.2, random_state=42")

# ===== íƒ­ êµ¬ì„± =====
tabs = st.tabs(["ğŸ—‚ê°œìš”","ğŸ“Šê¸°ì´ˆí†µê³„","ğŸ“ˆë¶„í¬/êµì°¨","ğŸ’¬ì›Œë“œí´ë¼ìš°ë“œ","âš™ï¸í•„í„°","ğŸ”ì „ì²´ë³´ê¸°","ğŸ”§íŠœë‹","ğŸ¤–MLëª¨ë¸","ğŸ¯ì˜ˆì¸¡"])

# --- 4.1 ë°ì´í„° ê°œìš” ---
with tabs[0]:
    st.header("ë°ì´í„° ê°œìš”")
    c1,c2,c3 = st.columns(3)
    c1.metric("ìƒ˜í”Œ ìˆ˜", raw_df.shape[0])
    c2.metric("ì»¬ëŸ¼ ìˆ˜", raw_df.shape[1])
    c3.metric("ê³ ìœ  ì¥ë¥´", len(unique_genres))
    st.subheader("ê²°ì¸¡ì¹˜ ë¹„ìœ¨")
    st.dataframe(raw_df.isnull().mean())
    st.subheader("ì›ë³¸ ìƒ˜í”Œ")
    st.dataframe(raw_df.head(), use_container_width=True)

# --- 4.2 ê¸°ì´ˆí†µê³„ ---
with tabs[1]:
    st.header("ê¸°ì´ˆ í†µê³„: score")
    st.write(pd.to_numeric(raw_df['score'], errors='coerce').describe())
    fig,ax=plt.subplots(figsize=(6,3))
    ax.hist(pd.to_numeric(raw_df['score'], errors='coerce'), bins=20)
    ax.set_title("ì „ì²´ í‰ì  ë¶„í¬")
    st.pyplot(fig)

# --- 4.3 ë¶„í¬/êµì°¨ë¶„ì„ ---
with tabs[2]:
    st.header("ë¶„í¬ ë° êµì°¨ë¶„ì„")

    st.subheader("ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    ct = (
        pd.DataFrame({'start airing': raw_df['start airing'], 'network': raw_df['network'].apply(clean_cell_colab)})
        .explode('network').groupby(['start airing','network']).size().reset_index(name='count')
    )
    ct['NETWORK_UP'] = ct['network'].astype(str).str.upper()
    focus = ['KBS','MBC','TVN','NETFLIX','SBS']
    fig3 = px.line(ct[ct['NETWORK_UP'].isin(focus)], x='start airing', y='count', color='network',
                   log_y=True, title="ì—°ë„ë³„ ì£¼ìš” í”Œë«í¼ ì‘í’ˆ ìˆ˜")
    st.plotly_chart(fig3, use_container_width=True)

    p = (ct.pivot_table(index='start airing', columns='NETWORK_UP', values='count', aggfunc='sum')
           .fillna(0).astype(int))
    years = sorted(p.index)
    insights = []

    if 'NETFLIX' in p.columns:
        s = p['NETFLIX']; nz = s[s > 0]
        if not nz.empty:
            first_year = int(nz.index.min())
            max_year, max_val = int(s.idxmax()), int(s.max())
            txt = f"- **ë„·í”Œë¦­ìŠ¤(OTT)ì˜ ê¸‰ì„±ì¥**: {first_year}ë…„ ì´í›„ ë¹ ë¥´ê²Œ ì¦ê°€, **{max_year}ë…„ {max_val}í¸**ìœ¼ë¡œ ìµœê³ ì¹˜."
            if 2020 in p.index:
                comps = ", ".join([f"{b} {int(p.loc[2020,b])}í¸" for b in ['KBS','MBC','SBS'] if b in p.columns])
                txt += f" 2020ë…„ì—ëŠ” ë„·í”Œë¦­ìŠ¤ {int(p.loc[2020,'NETFLIX'])}í¸, ì§€ìƒíŒŒ({comps})ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€."
            insights.append(txt)

    import numpy as np
    down_ter = []
    for b in ['KBS','MBC','SBS']:
        if b in p.columns and len(years) >= 2:
            slope = np.polyfit(years, p[b].reindex(years, fill_value=0), 1)[0]
            if slope < 0:
                down_ter.append(b)
    if down_ter:
        insights.append(f"- **ì§€ìƒíŒŒì˜ ì§€ì†ì  ê°ì†Œ**: {' / '.join(down_ter)} ë“± ì „í†µ 3ì‚¬ì˜ ì‘í’ˆ ìˆ˜ê°€ ì „ë°˜ì ìœ¼ë¡œ í•˜ë½ ì¶”ì„¸.")

    if 'TVN' in p.columns:
        tvn = p['TVN']
        peak_year, peak_val = int(tvn.idxmax()), int(tvn.max())
        tail = []
        for y in [y for y in [2020, 2021, 2022] if y in tvn.index]:
            tail.append(f"{y}ë…„ {int(tvn.loc[y])}í¸")
        insights.append(f"- **tvNì˜ ì„±ì¥ê³¼ ì •ì²´**: ìµœê³  {peak_year}ë…„ {peak_val}í¸. ìµœê·¼ ìˆ˜ë…„({', '.join(tail)})ì€ ì •ì²´/ì†Œí­ ê°ì†Œ ê²½í–¥.")

    if 2021 in p.index and 2022 in p.index:
        downs = [c for c in p.columns if p.loc[2022, c] < p.loc[2021, c]]
        if downs:
            insights.append(f"- **2022ë…„ ì „ë…„ ëŒ€ë¹„ ê°ì†Œ**: {', '.join(downs)} ë“± ì—¬ëŸ¬ í”Œë«í¼ì´ 2021ë…„ë³´ë‹¤ ì¤„ì–´ë“¦.")

    st.markdown("**ì¸ì‚¬ì´íŠ¸**\n" + "\n".join(insights) +
                "\n\n*í•´ì„ ë©”ëª¨: OTT-ë°©ì†¡ì‚¬ ë™ì‹œë°©ì˜, ì œì‘í™˜ê²½(ì˜ˆì‚°/ì‹œì²­ë¥ ), ì½”ë¡œë‚˜19 ë“± ì™¸ë¶€ ìš”ì¸ì´ ì˜í–¥ì„ ì¤€ ê²ƒìœ¼ë¡œ í•´ì„ ê°€ëŠ¥.*")

    # ì¥ë¥´ 'ê°œìˆ˜'ë³„ ë°°ìš° í‰ê·  í‰ì 
    st.subheader("ì¥ë¥´ ê°œìˆ˜ë³„ í‰ê·  í‰ì  (ë°°ìš° ë‹¨ìœ„, 1 ~ 2 / 3 ~ 4 / 5 ~ 6 / 7+)")
    actor_col = 'ë°°ìš°ëª…' if 'ë°°ìš°ëª…' in raw_df.columns else ('actor' if 'actor' in raw_df.columns else None)
    if actor_col is None:
        st.info("ë°°ìš° ì‹ë³„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´(ë°°ìš°ëª…/actor) ì´ ì„¹ì…˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        gdf = (
            pd.DataFrame({actor_col: raw_df[actor_col], 'genres': raw_df['genres'].apply(clean_cell_colab)})
            .explode('genres').dropna(subset=[actor_col,'genres'])
        )
        genre_cnt = gdf.groupby(actor_col)['genres'].nunique().rename('ì¥ë¥´ê°œìˆ˜')
        actor_mean = (raw_df.groupby(actor_col, as_index=False)['score']
                      .mean().rename(columns={'score':'ë°°ìš°í‰ê· ì ìˆ˜'}))
        df_actor = actor_mean.merge(genre_cnt.reset_index(), on=actor_col, how='left')
        df_actor['ì¥ë¥´ê°œìˆ˜'] = df_actor['ì¥ë¥´ê°œìˆ˜'].fillna(0).astype(int)
        df_actor = df_actor[df_actor['ì¥ë¥´ê°œìˆ˜'] > 0].copy()

        def bucket(n: int) -> str:
            if n <= 2:  return '1~2ê°œ'
            if n <= 4:  return '3~4ê°œ'
            if n <= 6:  return '5~6ê°œ'
            return '7ê°œ ì´ìƒ'

        df_actor['ì¥ë¥´ê°œìˆ˜êµ¬ê°„'] = pd.Categorical(
            df_actor['ì¥ë¥´ê°œìˆ˜'].apply(bucket),
            categories=['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ'],
            ordered=True
        )

        fig_box = px.box(
            df_actor, x='ì¥ë¥´ê°œìˆ˜êµ¬ê°„', y='ë°°ìš°í‰ê· ì ìˆ˜',
            category_orders={'ì¥ë¥´ê°œìˆ˜êµ¬ê°„': ['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ']},
            title="ì¥ë¥´ ê°œìˆ˜ë³„ ë°°ìš° í‰ê·  ì ìˆ˜ ë¶„í¬ (1 ~ 2 / 3 ~ 4 / 5 ~ 6 / 7+)"
        )
        st.plotly_chart(fig_box, use_container_width=True)

        stats = (df_actor.groupby('ì¥ë¥´ê°œìˆ˜êµ¬ê°„')['ë°°ìš°í‰ê· ì ìˆ˜']
                 .agg(í‰ê· ='mean', ì¤‘ì•™ê°’='median', í‘œë³¸ìˆ˜='count')
                 .reindex(['1~2ê°œ','3~4ê°œ','5~6ê°œ','7ê°œ ì´ìƒ']).dropna(how='all').round(3))

        if not stats.empty and stats['í‘œë³¸ìˆ˜'].sum() > 0:
            best_mean_grp   = stats['í‰ê· '].idxmax()
            best_median_grp = stats['ì¤‘ì•™ê°’'].idxmax()
            vals = stats['í‰ê· '].dropna().values
            diffs = pd.Series(vals).diff().dropna()
            if (diffs >= 0).all():
                trend = "ì¥ë¥´ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í‰ê·  í‰ì ì´ **ë†’ì•„ì§€ëŠ” ê²½í–¥**"
            elif (diffs <= 0).all():
                trend = "ì¥ë¥´ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í‰ê·  í‰ì ì´ **ë‚®ì•„ì§€ëŠ” ê²½í–¥**"
            else:
                trend = "ì¥ë¥´ ìˆ˜ì™€ í‰ê·  í‰ì  ê°„ **ì¼ê´€ëœ ë‹¨ì¡° ê²½í–¥ì€ ì•½í•¨**"

            comp_txt = ""
            if {'1~2ê°œ','7ê°œ ì´ìƒ'}.issubset(stats.index):
                diff_mean = stats.loc['1~2ê°œ','í‰ê· '] - stats.loc['7ê°œ ì´ìƒ','í‰ê· ']
                diff_med  = stats.loc['1~2ê°œ','ì¤‘ì•™ê°’'] - stats.loc['7ê°œ ì´ìƒ','ì¤‘ì•™ê°’']
                sign = "ë†’ìŒ" if diff_mean >= 0 else "ë‚®ìŒ"
                comp_txt = f"- **1~2ê°œ vs 7ê°œ ì´ìƒ**: í‰ê·  {abs(diff_mean):.3f}p {sign}, ì¤‘ì•™ê°’ ì°¨ì´ {abs(diff_med):.3f}p\n"

            st.markdown("**ìš”ì•½ í†µê³„(ë°°ìš° ë‹¨ìœ„)**")
            try:
                st.markdown(stats.to_markdown())
            except Exception:
                st.dataframe(stats.reset_index(), use_container_width=True)

            st.markdown(
                f"""
**ì¸ì‚¬ì´íŠ¸**
- í‰ê·  ê¸°ì¤€ ìµœê³  ê·¸ë£¹: **{best_mean_grp}** / ì¤‘ì•™ê°’ ê¸°ì¤€ ìµœê³  ê·¸ë£¹: **{best_median_grp}**  
- {trend}  
{comp_txt if comp_txt else ""}
- ì¥ë¥´ ë‹¤ì–‘ì„±â†‘ â†’ í‰ì â†‘ (ë‹¨ì¡° ì¦ê°€)  
-> ë‹¤ì¥ë¥´ ê²½í—˜ ì¦ê°€ê°€ ì‘í’ˆ ì„ íƒ í’ˆì§ˆ í–¥ìƒì— ê¸°ì—¬í–ˆì„ ê°€ëŠ¥ì„±.
"""
            )
        else:
            st.info("ì¥ë¥´ ê°œìˆ˜ êµ¬ê°„ë³„ í†µê³„ë¥¼ ê³„ì‚°í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    # ê²°í˜¼ ìƒíƒœë³„ í‰ê·  ì ìˆ˜
    st.subheader("ì£¼ì—° ë°°ìš° ê²°í˜¼ ìƒíƒœë³„ í‰ê·  ì ìˆ˜ ë¹„êµ")
    main_roles = raw_df[raw_df['role']=='ì£¼ì—°'].copy()
    main_roles['ê²°í˜¼ìƒíƒœ'] = main_roles['married'].apply(lambda x: 'ë¯¸í˜¼' if x=='ë¯¸í˜¼' else 'ë¯¸í˜¼ ì™¸')
    avg_scores_by_marriage = main_roles.groupby('ê²°í˜¼ìƒíƒœ')['score'].mean()
    fig, ax = plt.subplots(figsize=(3,3))
    bars = ax.bar(avg_scores_by_marriage.index, avg_scores_by_marriage.values, color=['mediumseagreen','gray'])
    for b in bars:
        yv = b.get_height()
        ax.text(b.get_x()+b.get_width()/2, yv+0.005, f'{yv:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')
    ax.set_title('ì£¼ì—° ë°°ìš° ê²°í˜¼ ìƒíƒœë³„ í‰ê·  ì ìˆ˜ ë¹„êµ'); ax.set_ylabel('í‰ê·  ì ìˆ˜'); ax.set_xlabel('ê²°í˜¼ ìƒíƒœ')
    ax.set_ylim(min(avg_scores_by_marriage.values)-0.05, max(avg_scores_by_marriage.values)+0.05)
    ax.grid(axis='y', linestyle='--', alpha=0.5)
    st.pyplot(fig)

    m_single = avg_scores_by_marriage.get('ë¯¸í˜¼')
    m_else   = avg_scores_by_marriage.get('ë¯¸í˜¼ ì™¸')
    diff_txt = f"(ì°¨ì´ {m_single - m_else:+.3f}p)" if (m_single is not None and m_else is not None) else ""
    st.markdown(
        f"""
**ìš”ì•½**
- ë¯¸í˜¼ í‰ê· : **{m_single:.3f}**, ë¯¸í˜¼ ì™¸ í‰ê· : **{m_else:.3f}** {diff_txt}
"""
    )

    # ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜
    st.subheader("ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜")
    dfg = raw_df.copy()
    dfg['genres'] = dfg['genres'].apply(clean_cell_colab)
    dfg = dfg.explode('genres').dropna(subset=['genres','score'])
    g_score = dfg.groupby('genres')['score'].mean().round(3)
    g_count = dfg['genres'].value_counts()
    gdf = pd.DataFrame({'í‰ê·  ì ìˆ˜': g_score, 'ì‘í’ˆ ìˆ˜': g_count}).reset_index()
    name_col = 'index' if 'index' in gdf.columns else ('genres' if 'genres' in gdf.columns else gdf.columns[0])
    gdf = gdf.rename(columns={name_col: 'ì¥ë¥´'})
    gdf = gdf.sort_values('ì‘í’ˆ ìˆ˜', ascending=False).reset_index(drop=True)
    fig, ax1 = plt.subplots(figsize=(12,6))
    bars = ax1.bar(range(len(gdf)), gdf['ì‘í’ˆ ìˆ˜'], color='lightgray')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜')
    ax1.set_xticks(range(len(gdf)))
    ax1.set_xticklabels(gdf['ì¥ë¥´'], rotation=45, ha='right')
    for i, r in enumerate(bars):
        h = r.get_height()
        ax1.text(i, h+max(2, h*0.01), f'{int(h)}', ha='center', va='bottom',
                 fontsize=10, fontweight='bold', color='#444')
    ax2 = ax1.twinx()
    ax2.plot(range(len(gdf)), gdf['í‰ê·  ì ìˆ˜'], marker='o', linewidth=2, color='tab:blue')
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', color='tab:blue')
    ax2.tick_params(axis='y', colors='tab:blue')
    ax2.set_ylim(gdf['í‰ê·  ì ìˆ˜'].min()-0.1, gdf['í‰ê·  ì ìˆ˜'].max()+0.1)
    for i, v in enumerate(gdf['í‰ê·  ì ìˆ˜']):
        ax2.text(i, v+0.01, f'{v:.3f}', ha='center', va='bottom',
                 fontsize=10, fontweight='bold', color='tab:blue')
    plt.title('ì¥ë¥´ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜')
    ax1.set_xlabel('ì¥ë¥´')
    ax1.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    st.pyplot(fig)

    top_cnt = gdf.nlargest(3, 'ì‘í’ˆ ìˆ˜')
    low_cnt = gdf.nsmallest(3, 'ì‘í’ˆ ìˆ˜')
    top_score = gdf.nlargest(4, 'í‰ê·  ì ìˆ˜')
    def fmt_counts(df): return ", ".join([f"{r['ì¥ë¥´']}({int(r['ì‘í’ˆ ìˆ˜']):,}í¸)" for _, r in df.iterrows()])
    def fmt_scores(df): return ", ".join([f"{r['ì¥ë¥´']}({r['í‰ê·  ì ìˆ˜']:.3f})" for _, r in df.iterrows()])
    st.markdown(
        f"""
**ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
- ì‘í’ˆ ìˆ˜ ìƒìœ„: **{fmt_counts(top_cnt)}**  
- ì‘í’ˆ ìˆ˜ í•˜ìœ„: **{fmt_counts(low_cnt)}**  
- í‰ê·  í‰ì  ìƒìœ„: **{fmt_scores(top_score)}**
"""
    )

    # ë°©ì˜ ìš”ì¼ë³„
    st.subheader("ë°©ì˜ ìš”ì¼ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜ (ì›”â†’ì¼)")
    dfe = raw_df.copy(); dfe['day'] = dfe['day'].apply(clean_cell_colab)
    dfe = dfe.explode('day').dropna(subset=['day','score']).copy()
    dfe['day'] = dfe['day'].astype(str).str.strip().str.lower()
    ordered = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']
    day_ko = {'monday':'ì›”','tuesday':'í™”','wednesday':'ìˆ˜','thursday':'ëª©','friday':'ê¸ˆ','saturday':'í† ','sunday':'ì¼'}
    mean_by = dfe.groupby('day')['score'].mean().reindex(ordered)
    cnt_by = dfe['day'].value_counts().reindex(ordered).fillna(0).astype(int)
    fig, ax1 = plt.subplots(figsize=(12,6))
    bars = ax1.bar(ordered, cnt_by.values, alpha=0.3, color='tab:gray')
    ax1.set_ylabel('ì‘í’ˆ ìˆ˜', color='tab:gray'); ax1.tick_params(axis='y', labelcolor='tab:gray')
    for b in bars:
        h = b.get_height(); ax1.text(b.get_x()+b.get_width()/2, h+0.5, f'{int(h)}', ha='center', va='bottom', fontsize=9, color='black')
    ax2 = ax1.twinx(); ax2.plot(ordered, mean_by.values, marker='o', color='tab:blue')
    ax2.set_ylabel('í‰ê·  ì ìˆ˜', color='tab:blue'); ax2.tick_params(axis='y', labelcolor='tab:blue')
    if mean_by.notna().any(): ax2.set_ylim(mean_by.min()-0.05, mean_by.max()+0.05)
    for x, yv in zip(ordered, mean_by.values):
        if pd.notna(yv): ax2.text(x, yv+0.005, f'{yv:.3f}', color='tab:blue', fontsize=9, ha='center')
    ax1.set_xticks(ordered); ax1.set_xticklabels([day_ko[d] for d in ordered])
    plt.title('ë°©ì˜ ìš”ì¼ë³„ ì‘í’ˆ ìˆ˜ ë° í‰ê·  ì ìˆ˜ (ì›”ìš”ì¼ â†’ ì¼ìš”ì¼ ìˆœ)'); plt.tight_layout(); st.pyplot(fig)

    weekday = ['monday','tuesday','wednesday','thursday']
    weekend = ['friday','saturday','sunday']
    wk_avg = mean_by.loc[weekday].mean(skipna=True)
    we_avg = mean_by.loc[weekend].mean(skipna=True)
    top_day_en  = mean_by.idxmax() if mean_by.notna().any() else None
    top_day_ko  = day_ko.get(top_day_en, "N/A") if top_day_en else "N/A"
    top_mean    = float(mean_by.max()) if mean_by.notna().any() else float("nan")
    wk_cnt = int(cnt_by.loc[weekday].sum()); we_cnt = int(cnt_by.loc[weekend].sum())

    st.markdown(
        f"""
**ìš”ì•½(ë°ì´í„° ê·¼ê±°)**  
- ì£¼ì¤‘ í‰ê·  í‰ì (ì›”~ëª©): **{wk_avg:.3f}** Â· ì£¼ì¤‘ ì‘í’ˆ ìˆ˜ í•©ê³„: **{wk_cnt}í¸**  
- ì£¼ë§ í‰ê·  í‰ì (ê¸ˆ~ì¼): **{we_avg:.3f}** Â· ì£¼ë§ ì‘í’ˆ ìˆ˜ í•©ê³„: **{we_cnt}í¸**  
- í‰ê·  í‰ì  ìµœê³  ìš”ì¼: **{top_day_ko} {top_mean:.3f}ì **
"""
    )

# --- 4.4 ì›Œë“œí´ë¼ìš°ë“œ ---
from wordcloud import WordCloud
with tabs[3]:
    st.header("ì›Œë“œí´ë¼ìš°ë“œ")
    from collections import Counter

    def top_pairs(words, n=5, keyfn=lambda x: str(x).strip().lower()):
        vals = [keyfn(w) for w in words if pd.notna(w) and str(w).strip() != ""]
        return Counter(vals).most_common(n)

    def pairs_to_str(pairs, label_map=None):
        it = []
        for k, v in pairs:
            kk = label_map.get(k, k) if label_map else k
            it.append(f"{kk}({v:,})")
        return ", ".join(it) if it else "N/A"

    font_path = st.session_state.get("kfont_path")

    if genre_list:
        wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                .generate(' '.join(genre_list))
        fig, ax = plt.subplots(); ax.imshow(wc, interpolation='bilinear'); ax.axis('off'); st.pyplot(fig)

    if broadcaster_list:
        wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                .generate(' '.join(broadcaster_list))
        fig, ax = plt.subplots(); ax.imshow(wc, interpolation='bilinear'); ax.axis('off'); st.pyplot(fig)

    if week_list:
        wk = [str(w).strip().lower() for w in week_list if pd.notna(w)]
        wc = WordCloud(width=800, height=400, background_color='white', font_path=font_path)\
                .generate(' '.join(wk))
        fig, ax = plt.subplots(); ax.imshow(wc, interpolation='bilinear'); ax.axis('off'); st.pyplot(fig)

# --- 4.5 ì‹¤ì‹œê°„ í•„í„° ---
with tabs[4]:
    st.header("ì‹¤ì‹œê°„ í•„í„°")
    smin,smax = float(pd.to_numeric(raw_df['score'], errors='coerce').min()), float(pd.to_numeric(raw_df['score'], errors='coerce').max())
    sfilter = st.slider("ìµœì†Œ í‰ì ", smin,smax,smin)
    y_min = int(pd.to_numeric(raw_df['start airing'], errors='coerce').min())
    y_max = int(pd.to_numeric(raw_df['start airing'], errors='coerce').max())
    yfilter = st.slider("ë°©ì˜ë…„ë„ ë²”ìœ„", y_min, y_max, (y_min, y_max))
    filt = raw_df[(pd.to_numeric(raw_df['score'], errors='coerce')>=sfilter) & pd.to_numeric(raw_df['start airing'], errors='coerce').between(*yfilter)]
    st.dataframe(filt.head(20))

# --- 4.6 ì „ì²´ ë¯¸ë¦¬ë³´ê¸° ---
with tabs[5]:
    st.header("ì›ë³¸ ì „ì²´ë³´ê¸°")
    st.dataframe(raw_df, use_container_width=True)

# --- ê³µí†µ ì¤€ë¹„ ---
# íŒŒì´í”„ë¼ì¸ ë¹Œë” (ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ êµ¬ì„±/ìŠ¤í…ëª…)
def make_pipeline(model_name, kind, estimator):
    if kind == "tree":
        return Pipeline([
            ('preprocessor', preprocessor),
            ('model', estimator)
        ])

    if model_name == "SVR":
        return Pipeline([
            ('preprocessor', preprocessor),
            ('scaler', StandardScaler()),
            ('model', estimator)
        ])

    if model_name == "KNN":
        return Pipeline([
            ('preprocessor', preprocessor),
            ('poly', PolynomialFeatures(include_bias=False)),
            ('scaler', StandardScaler()),
            ('knn', estimator)
        ])

    if model_name == "Linear Regression (Poly)":
        return Pipeline([
            ('preprocessor', preprocessor),
            ('poly', PolynomialFeatures(include_bias=False)),
            ('scaler', StandardScaler()),
            ('linreg', estimator)
        ])

    return Pipeline([
        ('preprocessor', preprocessor),
        ('poly', PolynomialFeatures(include_bias=False)),
        ('scaler', StandardScaler()),
        ('model', estimator)
    ])

from sklearn.ensemble import RandomForestRegressor

# --- 4.7 GridSearch íŠœë‹ ---
with tabs[6]:
    st.header("GridSearchCV íŠœë‹")

    # split ë³´ì¥ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼: test_size=0.2, random_state=42, shuffle=True)
    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    scoring = st.selectbox("ìŠ¤ì½”ì–´ë§", ["neg_root_mean_squared_error", "r2"], index=0)
    cv = st.number_input("CV í´ë“œ ìˆ˜", 3, 10, 5, 1)
    cv_shuffle = st.checkbox("CV ì…”í”Œ(shuffle)", value=False)

    def render_param_selector(label, options):
        display_options, to_py = [], {}
        for v in options:
            if v is None:
                s = "(None)"; to_py[s] = None
            else:
                s = str(int(v)) if isinstance(v, float) and v.is_integer() else str(v)
                to_py[s] = v
            display_options.append(s)

        sel = st.multiselect(f"{label}", display_options, default=display_options, key=f"sel_{label}")
        extra = st.text_input(f"{label} ì¶”ê°€ê°’(ì½¤ë§ˆ, ì˜ˆ: 50,75,100 ë˜ëŠ” None)", value="", key=f"extra_{label}")

        chosen = [to_py[s] for s in sel]
        if extra.strip():
            for tok in extra.split(","):
                t = tok.strip()
                if not t:
                    continue
                if t.lower() == "none":
                    val = None
                else:
                    try:
                        val = int(t)
                    except:
                        try:
                            val = float(t)
                        except:
                            val = t
                chosen.append(val)

        uniq = []
        for v in chosen:
            if v not in uniq:
                uniq.append(v)
        return uniq

    model_zoo = {
        "KNN": ("nonsparse", KNeighborsRegressor()),
        "Linear Regression (Poly)": ("nonsparse", LinearRegression()),
        "Ridge": ("nonsparse", Ridge()),
        "Lasso": ("nonsparse", Lasso()),
        "ElasticNet": ("nonsparse", ElasticNet(max_iter=10000)),
        "SGDRegressor": ("nonsparse", SGDRegressor(max_iter=10000, random_state=SEED)),
        "SVR": ("nonsparse", SVR()),
        "Decision Tree": ("tree", DecisionTreeRegressor(random_state=SEED)),
        "Random Forest": ("tree", RandomForestRegressor(random_state=SEED)),
    }
    if 'XGBRegressor' in globals() and XGB_AVAILABLE:
        model_zoo["XGBRegressor"] = ("tree", XGBRegressor(
            random_state=SEED, objective="reg:squarederror", n_jobs=-1, tree_method="hist"
        ))

    # ê·¸ë¦¬ë“œ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼ í‚¤)
    default_param_grids = {
        "KNN": {"poly__degree":[1,2,3], "knn__n_neighbors":[3,4,5,6,7,8,9,10]},
        "Linear Regression (Poly)": {"poly__degree":[1,2,3]},
        "Ridge": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "Lasso": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000]},
        "ElasticNet": {"poly__degree":[1,2,3], "model__alpha":[0.001,0.01,0.1,1,10,100,1000], "model__l1_ratio":[0.1,0.5,0.9]},
        "SGDRegressor": {"poly__degree":[1,2,3], "model__learning_rate":["constant","invscaling","adaptive"]},
        # SVRì€ Poly ì œê±°
        "SVR": {"model__kernel":["poly","rbf","sigmoid"], "model__degree":[1,2,3]},
        "Decision Tree": {"model__max_depth":[10,15,20,25,30], "model__min_samples_split":[5,6,7,8,9,10], "model__min_samples_leaf":[2,3,4,5], "model__max_leaf_nodes":[None,10,20,30]},
        "Random Forest": {"model__n_estimators":[100,200,300], "model__min_samples_split":[5,6,7,8,9,10], "model__max_depth":[5,10,15,20,25,30]},
    }
    if "XGBRegressor" in model_zoo:
        default_param_grids["XGBRegressor"] = {
            "model__n_estimators":[200,400],
            "model__max_depth":[3,5,7],
            "model__learning_rate":[0.03,0.1,0.3],
            "model__subsample":[0.8,1.0],
            "model__colsample_bytree":[0.8,1.0],
        }

    model_name = st.selectbox("íŠœë‹í•  ëª¨ë¸ ì„ íƒ", list(model_zoo.keys()), index=0)
    kind, estimator = model_zoo[model_name]
    pipe = make_pipeline(model_name, kind, estimator)

    st.markdown("**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ**")
    base_grid = default_param_grids.get(model_name, {})
    user_grid = {}
    for param_key, default_vals in base_grid.items():
        user_vals = render_param_selector(param_key, default_vals)
        user_grid[param_key] = user_vals if len(user_vals) > 0 else default_vals

    with st.expander("ì„ íƒí•œ íŒŒë¼ë¯¸í„° í™•ì¸"):
        st.write(user_grid)

    if st.button("GridSearch ì‹¤í–‰"):
        # â˜… ë…¸íŠ¸ë¶ê³¼ ë™ì¼: ê¸°ë³¸ì€ shuffle=False (ì •ìˆ˜ ì „ë‹¬)
        if cv_shuffle:
            cv_obj = KFold(n_splits=int(cv), shuffle=True, random_state=SEED)
        else:
            cv_obj = int(cv)  # ì˜ˆ: 5

        gs = GridSearchCV(
            estimator=pipe,
            param_grid=user_grid,
            cv=cv_obj,
            scoring=scoring,
            n_jobs=-1,
            refit=True,
            return_train_score=True
        )
        with st.spinner("GridSearchCV ì‹¤í–‰ ì¤‘..."):
            gs.fit(X_train, y_train)

        st.subheader("ë² ìŠ¤íŠ¸ ê²°ê³¼")
        st.write("Best Params:", gs.best_params_)
        if scoring == "neg_root_mean_squared_error":
            st.write("Best CV RMSE (ìŒìˆ˜):", gs.best_score_)  # ë¶€í˜¸ ìœ ì§€
        else:
            st.write(f"Best CV {scoring}:", gs.best_score_)

        y_pred_tr = gs.predict(X_train)
        y_pred_te = gs.predict(X_test)
        st.write("Train RMSE:", rmse(y_train, y_pred_tr))
        st.write("Test RMSE:", rmse(y_test, y_pred_te))
        st.write("Train RÂ² Score:", r2_score(y_train, y_pred_tr))
        st.write("Test RÂ² Score:", r2_score(y_test, y_pred_te))

        st.session_state["best_estimator"] = gs.best_estimator_
        st.session_state["best_params"] = gs.best_params_
        st.session_state["best_name"] = model_name
        st.session_state["best_cv_score"] = gs.best_score_
        st.session_state["best_scoring"] = scoring
        st.session_state["best_split_key"] = st.session_state.get("split_key")

        cvres = pd.DataFrame(gs.cv_results_)
        cols = ["rank_test_score","mean_test_score","std_test_score","mean_train_score","std_train_score","params"]
        safe_cols = [c for c in ["rank_test_score","mean_test_score","std_test_score","mean_train_score","std_train_score","params"] if c in cvres.columns]
        sorted_cvres = cvres.loc[:, safe_cols].sort_values("rank_test_score").reset_index(drop=True)
        st.dataframe(sorted_cvres)

    if model_name == "XGBRegressor" and not XGB_AVAILABLE:
        st.warning("xgboostê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. requirements.txtì— `xgboost`ë¥¼ ì¶”ê°€í•˜ê³  ì¬ë°°í¬í•´ ì£¼ì„¸ìš”.")

# --- 4.8 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ---
with tabs[7]:
    st.header("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§")

    if "split_colab" not in st.session_state or st.session_state.get("split_key") != float(test_size):
        X_train, X_test, y_train, y_test = train_test_split(
            X_colab_base, y_all, test_size=test_size, random_state=SEED, shuffle=True
        )
        st.session_state["split_colab"] = (X_train, X_test, y_train, y_test)
        st.session_state["split_key"] = float(test_size)
    X_train, X_test, y_train, y_test = st.session_state["split_colab"]

    if "best_estimator" in st.session_state:
        model = st.session_state["best_estimator"]
        st.caption(f"í˜„ì¬ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© ({st.session_state.get('best_name')})")
        if st.session_state.get("best_split_key") != st.session_state.get("split_key"):
            st.warning("ì£¼ì˜: ë² ìŠ¤íŠ¸ ëª¨ë¸ì€ ì´ì „ ë¶„í• ë¡œ í•™ìŠµë¨. ìƒˆ ë¶„í• ë¡œ ë‹¤ì‹œ íŠœë‹í•´ ì£¼ì„¸ìš”.", icon="âš ï¸")
    else:
        model = Pipeline([('preprocessor', preprocessor),
                          ('model', RandomForestRegressor(random_state=SEED))])
        model.fit(X_train, y_train)
        st.caption("í˜„ì¬ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

    y_pred_tr = model.predict(X_train)
    y_pred_te = model.predict(X_test)
    st.metric("Train RÂ²", f"{r2_score(y_train, y_pred_tr):.3f}")
    st.metric("Test  RÂ²", f"{r2_score(y_test,  y_pred_te):.3f}")
    st.metric("Train RMSE", f"{rmse(y_train, y_pred_tr):.3f}")
    st.metric("Test  RMSE", f"{rmse(y_test,  y_pred_te):.3f}")

    if "best_params" in st.session_state:
        with st.expander("ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³´ê¸°"):
            st.json(st.session_state["best_params"])

# --- 4.9 ì˜ˆì¸¡ ì‹¤í–‰ ---
with tabs[8]:
    st.header("í‰ì  ì˜ˆì¸¡")

    genre_opts   = sorted({g for sub in raw_df['genres'].dropna().apply(clean_cell_colab) for g in sub})
    week_opts    = sorted({d for sub in raw_df['day'].dropna().apply(clean_cell_colab) for d in sub})
    plat_opts    = sorted({p for sub in raw_df['network'].dropna().apply(clean_cell_colab) for p in sub})
    gender_opts  = sorted(raw_df['gender'].dropna().unique())
    role_opts    = sorted(raw_df['role'].dropna().unique())
    quarter_opts = sorted(raw_df['air_q'].dropna().unique())
    married_opts = sorted(raw_df['married'].dropna().unique())

    st.subheader("1) ì…ë ¥")
    col_left, col_right = st.columns(2)

    with col_left:
        st.markdown("**â‘  ì»¨í…ì¸  íŠ¹ì„±**")
        input_age     = st.number_input("ë‚˜ì´", 10, 80, 30)
        input_gender  = st.selectbox("ì„±ë³„", gender_opts) if gender_opts else st.text_input("ì„±ë³„ ì…ë ¥", "")
        input_role    = st.selectbox("ì—­í• ", role_opts) if role_opts else st.text_input("ì—­í•  ì…ë ¥", "")
        input_married = st.selectbox("ê²°í˜¼ì—¬ë¶€", married_opts) if married_opts else st.text_input("ê²°í˜¼ì—¬ë¶€ ì…ë ¥", "")
        input_genre   = st.multiselect("ì¥ë¥´ (ë©€í‹° ì„ íƒ)", genre_opts, default=genre_opts[:1] if genre_opts else [])

        derived_age_group = age_to_age_group(int(input_age))

        n_genre = len(input_genre)
        if n_genre == 0:
            genre_bucket = "ì¥ë¥´ì—†ìŒ"
        elif n_genre <= 2:
            genre_bucket = "1~2ê°œ"
        elif n_genre <= 4:
            genre_bucket = "3~4ê°œ"
        elif n_genre <= 6:
            genre_bucket = "5~6ê°œ"
        else:
            genre_bucket = "7ê°œ ì´ìƒ"

        st.caption(f"ìë™ ì—°ë ¹ëŒ€: **{derived_age_group}**  |  ì¥ë¥´ ê°œìˆ˜: **{genre_bucket}**")

    with col_right:
        st.markdown("**â‘¡ í¸ì„± íŠ¹ì„±**")
        input_quarter = st.selectbox("ë°©ì˜ë¶„ê¸°", quarter_opts) if quarter_opts else st.text_input("ë°©ì˜ë¶„ê¸° ì…ë ¥", "")
        input_week    = st.multiselect("ë°©ì˜ìš”ì¼ (ë©€í‹° ì„ íƒ)", week_opts, default=week_opts[:1] if week_opts else [])
        input_plat    = st.multiselect("í”Œë«í¼ (ë©€í‹° ì„ íƒ)", plat_opts, default=plat_opts[:1] if plat_opts else [])

    predict_btn = st.button("ì˜ˆì¸¡ ì‹¤í–‰")

    if predict_btn:
        if "best_estimator" in st.session_state:
            model_full = clone(st.session_state["best_estimator"])
            st.caption(f"ì˜ˆì¸¡ ëª¨ë¸: GridSearch ë² ìŠ¤íŠ¸ ì¬í•™ìŠµ ì‚¬ìš© ({st.session_state.get('best_name')})")
        else:
            model_full = Pipeline([
                ('preprocessor', preprocessor),
                ('model', RandomForestRegressor(n_estimators=100, random_state=SEED))
            ])
            st.caption("ì˜ˆì¸¡ ëª¨ë¸: ê¸°ë³¸ RandomForest (ë¯¸íŠœë‹)")

        model_full.fit(X_colab_base, y_all)

        user_raw = pd.DataFrame([{
            'age'      : input_age,
            'gender'   : input_gender,
            'role'     : input_role,
            'married'  : input_married,
            'air_q'    : input_quarter,
            'age_group': derived_age_group,
            'genres'   : input_genre,
            'day'      : input_week,
            'network'  : input_plat,
            'ì¥ë¥´êµ¬ë¶„'    : genre_bucket,
        }])

        user_mlb = colab_multilabel_transform(user_raw, cols=('genres','day','network'))

        user_base = pd.concat([X_colab_base.iloc[:0].copy(), user_mlb], ignore_index=True)
        user_base = user_base.drop(columns=[c for c in drop_cols if c in user_base.columns], errors='ignore')
        for c in X_colab_base.columns:
            if c not in user_base.columns:
                user_base[c] = 0
        user_base = user_base[X_colab_base.columns].tail(1)
        user_base = user_base.replace([np.inf, -np.inf], np.nan).fillna(0)

        pred = model_full.predict(user_base)[0]

        pred = model_full.predict(user_base)[0]
        st.success(f"ğŸ’¡ ì˜ˆìƒ í‰ì : {pred:.2f}")
                # =========================
        # ğŸ” Counterfactual What-if
        # =========================
        st.markdown("---")
        st.subheader("ğŸ§ª What-if(ì¹´ìš´í„°íŒ©ì¶”ì–¼) íƒìƒ‰")

        # â”€â”€ ê³µí†µ ìœ í‹¸: user_raw â†’ user_base(feature vector)
        def _build_user_base(df_raw: pd.DataFrame) -> pd.DataFrame:
            _user_mlb = colab_multilabel_transform(df_raw, cols=('genres','day','network'))
            _base = pd.concat([X_colab_base.iloc[:0].copy(), _user_mlb], ignore_index=True)
            _base = _base.drop(columns=[c for c in drop_cols if c in _base.columns], errors='ignore')
            for c in X_colab_base.columns:
                if c not in _base.columns:
                    _base[c] = 0
            _base = _base[X_colab_base.columns].tail(1)
            # í´ë¦¬ë…¸ë¯¸ì–¼/ìŠ¤ì¼€ì¼ëŸ¬ ì•ˆì •í™”: ìˆ«ìí™” + ê²°ì¸¡ 0
            _base = _base.apply(pd.to_numeric, errors="coerce").fillna(0.0)
            return _base

        def _predict_from_raw(df_raw: pd.DataFrame) -> float:
            vb = _build_user_base(df_raw)
            return float(model_full.predict(vb)[0])

        # í˜„ì¬ ì…ë ¥ ì €ì¥(What-ifì˜ ì¶œë°œì )
        st.session_state["cf_user_raw"] = user_raw.copy()
        current_pred = float(pred)

        # â”€â”€ ë³€ê²½ ê°€ëŠ¥í•œ ì•¡ì…˜(í•„ìš”ì‹œ ììœ ë¡­ê²Œ ì¶”ê°€/ìˆ˜ì •)
        # í›„ë³´ ë¼ë²¨ì€ ì‹¤ì œ í•™ìŠµì— ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ì‚¬ìš©(mlb classes ì°¸ì¡°)
        def _classes_safe(key: str):
            return [s for s in (st.session_state.get(f"mlb_classes_{key}", []) or [])]

        genre_classes   = [g for g in _classes_safe("genres") if isinstance(g, str)]
        day_classes     = [d for d in _classes_safe("day") if isinstance(d, str)]
        network_classes = [n for n in _classes_safe("network") if isinstance(n, str)]

        # ìš°ì„ ìˆœìœ„ ì¥ë¥´(ë°ì´í„°ì— ìˆëŠ” ê²ƒë§Œ ë‚¨ê¹€)
        priority_genres = [g for g in ["thriller","hist_war","sf","action","romance","drama","comedy"] if g in genre_classes]
        # ìš”ì¼ í›„ë³´
        saturday_only   = ["saturday"] if "saturday" in day_classes else (day_classes[:1] if day_classes else [])
        friday_only     = ["friday"]   if "friday"   in day_classes else []
        wednesday_only  = ["wednesday"]if "wednesday"in day_classes else []
        # í”Œë«í¼ í›„ë³´
        netflix         = "NETFLIX" if "NETFLIX" in network_classes else (network_classes[0] if network_classes else None)
        tvn             = "TVN" if "TVN" in network_classes else None

        # ì•¡ì…˜ ì •ì˜: (id, ì„¤ëª…, ì ìš©í•¨ìˆ˜)
        # ì ìš©í•¨ìˆ˜ëŠ” user_raw(DataFrame) í•˜ë‚˜ë¥¼ ë°›ì•„ì„œ ë³€ê²½ëœ DataFrameì„ ë°˜í™˜
        def _add_genre(tag: str):
            def _fn(df):
                new = df.copy()
                cur = list(new.at[0, "genres"])
                if tag not in cur:
                    cur = cur + [tag]
                new.at[0, "genres"] = cur
                return new
            return _fn

        def _set_days(days_list: list[str]):
            def _fn(df):
                new = df.copy()
                new.at[0, "day"] = days_list
                return new
            return _fn

        def _ensure_platform(p: str):
            def _fn(df):
                new = df.copy()
                cur = list(new.at[0, "network"])
                if p not in cur:
                    cur = cur + [p]
                new.at[0, "network"] = cur
                return new
            return _fn

        def _set_role(val: str):
            def _fn(df):
                new = df.copy()
                new.at[0, "role"] = val
                return new
            return _fn

        def _set_married(val: str):
            def _fn(df):
                new = df.copy()
                new.at[0, "married"] = val
                return new
            return _fn

        # ë‹¨ì¼ ì•¡ì…˜ í›„ë³´ë“¤
        actions = []
        for g in priority_genres:
            actions.append((f"add_genre_{g}", f"ì¥ë¥´ ì¶”ê°€: {g}", _add_genre(g)))
        if saturday_only:
            actions.append(("set_sat_only", "í¸ì„± ìš”ì¼: í† ìš”ì¼ ë‹¨ì¼", _set_days(saturday_only)))
        if friday_only:
            actions.append(("set_fri_only", "í¸ì„± ìš”ì¼: ê¸ˆìš”ì¼ ë‹¨ì¼", _set_days(friday_only)))
        if wednesday_only:
            actions.append(("set_wed_only", "í¸ì„± ìš”ì¼: ìˆ˜ìš”ì¼ ë‹¨ì¼", _set_days(wednesday_only)))
        if netflix:
            actions.append(("ensure_netflix", "í”Œë«í¼ í¬í•¨: NETFLIX", _ensure_platform(netflix)))
        if tvn:
            actions.append(("ensure_tvn", "í”Œë«í¼ í¬í•¨: TVN", _ensure_platform(tvn)))

        # role / marriedê°€ ë°ì´í„°ì— ìˆë‹¤ë©´ ì•¡ì…˜ ì¶”ê°€
        if "role" in user_raw.columns:
            if str(user_raw.at[0,"role"]) != "ì£¼ì—°":
                actions.append(("set_lead", "ì—­í• : ì£¼ì—°ìœ¼ë¡œ ë³€ê²½", _set_role("ì£¼ì—°")))
        if "married" in user_raw.columns and str(user_raw.at[0,"married"]) != "ë¯¸í˜¼":
            actions.append(("set_single", "ê²°í˜¼ì—¬ë¶€: ë¯¸í˜¼ìœ¼ë¡œ ë³€ê²½", _set_married("ë¯¸í˜¼")))

        # â”€â”€ ë‹¨ì¼ ì•¡ì…˜ í‰ê°€
        rows = []
        for aid, desc, fn in actions:
            cand = fn(user_raw)
            p = _predict_from_raw(cand)
            rows.append({"ì¢…ë¥˜":"ë‹¨ì¼","ì•„ì´ë””":aid,"ì„¤ëª…":desc,"ì˜ˆì¸¡":p,"ë¦¬í”„íŠ¸":p - current_pred,"í¸ì§‘ìˆ˜":1,"ì ìš©":fn})

        # â”€â”€ 2ê°œ ì¡°í•©(ì—°ì‚°ëŸ‰ ì œí•œ: ë‹¨ì¼ ë¦¬í”„íŠ¸ ìƒìœ„ 6ê°œë§Œ ì¡°í•©)
        rows_sorted_single = sorted(rows, key=lambda d: d["ë¦¬í”„íŠ¸"], reverse=True)[:6]
        from itertools import combinations
        for (a1, a2) in combinations(rows_sorted_single, 2):
            fn_combo = lambda df, f1=a1["ì ìš©"], f2=a2["ì ìš©"]: f2(f1(df))
            p = _predict_from_raw(fn_combo(user_raw))
            rows.append({
                "ì¢…ë¥˜":"ì¡°í•©2","ì•„ì´ë””":f'{a1["ì•„ì´ë””"]}+{a2["ì•„ì´ë””"]}',
                "ì„¤ëª…":f'{a1["ì„¤ëª…"]} + {a2["ì„¤ëª…"]}',
                "ì˜ˆì¸¡":p,"ë¦¬í”„íŠ¸":p - current_pred,"í¸ì§‘ìˆ˜":2,"ì ìš©":fn_combo
            })

        # í‘œ ì¶œë ¥
        import math
        import pandas as _pd
        df_cf = _pd.DataFrame(rows)
        if not df_cf.empty:
            df_view = (df_cf
                .sort_values(["ì˜ˆì¸¡","ë¦¬í”„íŠ¸","í¸ì§‘ìˆ˜"], ascending=[False, False, True])
                [["ì¢…ë¥˜","ì„¤ëª…","ì˜ˆì¸¡","ë¦¬í”„íŠ¸","í¸ì§‘ìˆ˜"]]
                .head(12)
                .reset_index(drop=True))
            st.dataframe(df_view.style.format({"ì˜ˆì¸¡":"{:.3f}","ë¦¬í”„íŠ¸":"{:+.3f}"}), use_container_width=True)

        # ëª©í‘œ ì ìˆ˜ ë„ë‹¬ ì¶”ì²œ
        target = st.number_input("ëª©í‘œ ì ìˆ˜", min_value=0.0, max_value=10.0, value=min(8.2, max(7.0, round(current_pred+0.2,2))), step=0.05)
        rec = None
        if not df_cf.empty:
            above = df_cf[df_cf["ì˜ˆì¸¡"] >= float(target)]
            if not above.empty:
                # í¸ì§‘ ìˆ˜ ìµœì†Œ â†’ ë¦¬í”„íŠ¸ ìµœëŒ€ ìˆœ
                rec = (above.sort_values(["í¸ì§‘ìˆ˜","ì˜ˆì¸¡"], ascending=[True, False]).iloc[0]).to_dict()
            else:
                rec = (df_cf.sort_values(["ì˜ˆì¸¡"], ascending=False).iloc[0]).to_dict()

        if rec is not None:
            st.success(f"ì¶”ì²œ ë³€ê²½ì•ˆ â–¶ {rec['ì„¤ëª…']}  |  ì˜ˆìƒ {rec['ì˜ˆì¸¡']:.3f}ì  ({rec['ë¦¬í”„íŠ¸']:+.3f})")

            # ì ìš© ë²„íŠ¼: í˜„ì¬ ì…ë ¥ì— ë°˜ì˜í•´ì„œ ì¦‰ì‹œ ì¬ì˜ˆì¸¡
            if st.button("ì´ ë³€ê²½ì•ˆ ì ìš©í•´ì„œ ë‹¤ì‹œ ì˜ˆì¸¡"):
                applied_raw = rec["ì ìš©"](user_raw)
                # ì¬ì˜ˆì¸¡ ë° í™”ë©´ ê°±ì‹ 
                new_pred = _predict_from_raw(applied_raw)
                st.session_state["cf_user_raw"] = applied_raw
                st.info(f"ì¬ì˜ˆì¸¡ ê²°ê³¼: {new_pred:.2f}  (ê¸°ì¡´ {current_pred:.2f} â†’ {new_pred - current_pred:+.2f})")

